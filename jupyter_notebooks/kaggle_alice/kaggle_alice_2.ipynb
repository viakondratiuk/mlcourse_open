{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41602, 41600, 9088)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_uniq_sites = set(train_df[sites].values.flatten())\n",
    "\n",
    "train_sites = pd.DataFrame(index=train_df.index)\n",
    "test_sites = pd.DataFrame(index=test_df.index)\n",
    "\n",
    "for site in sites:\n",
    "    # transform train sites\n",
    "#     train_sites[site] = train_df[site].map(lambda x: inv_site_dic[x])\n",
    "    train_sites[site] = train_df[site].map(lambda x: inv_site_dic[x].replace(\".\", \"_\").replace(\"-\", \"_\"))\n",
    "    # transform test sites\n",
    "    test_sites[site] = test_df[site].map(lambda x: inv_site_dic[x].replace(\".\", \"_\").replace(\"-\", \"_\") \n",
    "                                         if x in train_uniq_sites else \"unknown_unknown\")\n",
    "    \n",
    "len(train_uniq_sites), len(set(train_sites[sites].values.flatten())), len(set(test_sites[sites].values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../../data/kaggle_alice/\"\n",
    "!PATH_TO_DATA=../../data/kaggle_alice/\n",
    "\n",
    "INP_TRAIN = \"train_sessions.csv\"\n",
    "INP_TEST  = \"test_sessions.csv\"\n",
    "SITE_DIC = \"site_dic.pkl\"\n",
    "SAMPLE_SUBMIT = \"sample_submission.csv\"\n",
    "\n",
    "!INP_TRAIN=train_sessions.csv\n",
    "!INP_TEST=test_sessions.csv\n",
    "!SITE_DIC=site_dic.pkl\n",
    "!SAMPLE_SUBMIT=sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = [\"time%s\" % i for i in range(1, 11)]\n",
    "sites = [\"site%s\" % i for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA + SITE_DIC, \"rb\") as inp_file:\n",
    "    site_dic = pickle.load(inp_file)\n",
    "\n",
    "inv_site_dic = {v: k for k, v in site_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA + INP_TRAIN, \n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times).sort_values(by=\"time1\")\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "test_df = pd.read_csv(PATH_TO_DATA + INP_TEST,\n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times)\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "y_train = train_df[\"target\"]\n",
    "train_df.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_to_text = train_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(train_df[sites]), 1)\n",
    "test_to_text = test_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(test_df[sites]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41592), (82797, 41592))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer())\n",
    "])\n",
    "pipeline.fit(train_to_text.ravel())\n",
    "\n",
    "X_train_sparse = pipeline.transform(train_to_text.ravel())\n",
    "X_test_sparse = pipeline.transform(test_to_text.ravel())\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sites_list = [\" \".join(row.tolist()) for _, row in train_sites.iterrows()]\n",
    "train_sites_list.append(\"unknown_unknown\")\n",
    "test_sites_list = [\" \".join(row.tolist()) for _, row in test_sites.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 10), (82797, 10))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sites.shape, test_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253562, 41600), (82797, 9088))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_sites_list), len(set(train_sites[sites].values.flatten()))), \\\n",
    "(len(test_sites_list), len(set(test_sites[sites].values.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# train_sites_list = train_sites_list[:9] + [train_sites_list[-1]]\n",
    "# test_sites_list = test_sites_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(train_sites_list)\n",
    "X_train_cv = X_train_cv[:-1, :]\n",
    "X_test_cv = cv.transform(test_sites_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41601), (82797, 41601))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape, X_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(train_sites[sites].values.flatten()) - set(cv.vocabulary_) == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X_train_sparse = transformer.fit_transform(X_train_cv)\n",
    "X_test_sparse = transformer.transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41601), (82797, 41601))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== BASELINE ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, ratio=0.9, seed=17):\n",
    "    '''\n",
    "    X, y – выборка\n",
    "    ratio – в каком отношении поделить выборку\n",
    "    C, seed – коэф-т регуляризации и random_state \n",
    "              логистической регрессии\n",
    "    '''\n",
    "    train_len = int(ratio * X.shape[0])\n",
    "    X_train = X[:train_len, :]\n",
    "    X_valid = X[train_len:, :]\n",
    "    y_train = y[:train_len]\n",
    "    y_valid = y[train_len:]\n",
    "    \n",
    "    logit = LogisticRegression(C=C, n_jobs=-1, random_state=seed)\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    valid_pred = logit.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    return round(roc_auc_score(y_valid, valid_pred), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.80008\n",
      "0.00278255940221 0.84407\n",
      "0.00774263682681 0.88516\n",
      "0.0215443469003 0.9061\n",
      "0.0599484250319 0.91304\n",
      "0.16681005372 0.91914\n",
      "0.464158883361 0.925\n",
      "1.29154966501 0.92758\n",
      "3.5938136638 0.92605\n",
      "10.0 0.92211\n"
     ]
    }
   ],
   "source": [
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 20 ms, total: 1.75 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([  1.00000e-03,   2.78256e-03,   7.74264e-03,   2.15443e-02,\n",
       "         5.99484e-02,   1.66810e-01,   4.64159e-01,   1.29155e+00,\n",
       "         3.59381e+00,   1.00000e+01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "param_grid = {\"C\": np.logspace(-3, 1, 10)}\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring=\"roc_auc\")\n",
    "grid.fit(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.77461, std: 0.08813, params: {'C': 0.001},\n",
       " mean: 0.80174, std: 0.08661, params: {'C': 0.0027825594022071257},\n",
       " mean: 0.82517, std: 0.08042, params: {'C': 0.0077426368268112694},\n",
       " mean: 0.83886, std: 0.07485, params: {'C': 0.021544346900318832},\n",
       " mean: 0.84936, std: 0.06795, params: {'C': 0.059948425031894091},\n",
       " mean: 0.85842, std: 0.06070, params: {'C': 0.1668100537200059},\n",
       " mean: 0.86796, std: 0.05349, params: {'C': 0.46415888336127775},\n",
       " mean: 0.87743, std: 0.04696, params: {'C': 1.2915496650148828},\n",
       " mean: 0.88511, std: 0.04066, params: {'C': 3.5938136638046259},\n",
       " mean: 0.88825, std: 0.03610, params: {'C': 10.0}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 944 ms, sys: 4 ms, total: 948 ms\n",
      "Wall time: 955 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91635999999999995"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_auc_lr_valid(X_train_sparse, y_train, C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/simple_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== MY ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_train = pd.DataFrame(index=train_df.index)\n",
    "feat_test = pd.DataFrame(index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**year_month_scaled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "feat_train['year_month'] = train_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "feat_test['year_month'] = test_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feat_train['year_month'].values.reshape(-1, 1))\n",
    "\n",
    "feat_train['year_month_scaled'] = scaler.transform(feat_train['year_month'].values.reshape(-1, 1))\n",
    "feat_test['year_month_scaled'] = scaler.transform(feat_test['year_month'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse, \n",
    "                                        feat_train['year_month_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse, \n",
    "                                       feat_test['year_month_scaled'].values.reshape(-1, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 0 ns, total: 1.83 s\n",
      "Wall time: 1.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92730999999999997"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_auc_lr_valid(X_train_sparse_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 s, sys: 48 ms, total: 2.35 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_new, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_new)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/simple_tfidf_yms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_hour_scaled, weekday_scaled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96496\n",
      "CPU times: user 5.04 s, sys: 128 ms, total: 5.17 s\n",
      "Wall time: 5.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_train['start_hour'] = train_df['time1'].apply(lambda ts: ts.hour)\n",
    "feat_test['start_hour'] = test_df['time1'].apply(lambda ts: ts.hour)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feat_train['start_hour'].values.reshape(-1, 1))\n",
    "\n",
    "feat_train['start_hour_scaled'] = scaler.transform(feat_train['start_hour'].values.reshape(-1, 1))\n",
    "feat_test['start_hour_scaled'] = scaler.transform(feat_test['start_hour'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train['start_hour_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['start_hour_scaled'].values.reshape(-1, 1)]))\n",
    "\n",
    "print(get_auc_lr_valid(X_train_sparse_new, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97688\n",
      "CPU times: user 5.28 s, sys: 368 ms, total: 5.65 s\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_train['weekday'] = train_df['time1'].apply(lambda ts: ts.dayofweek)\n",
    "feat_test['weekday'] = test_df['time1'].apply(lambda ts: ts.dayofweek)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feat_train['weekday'].values.reshape(-1, 1))\n",
    "\n",
    "feat_train['weekday_scaled'] = scaler.transform(feat_train['weekday'].values.reshape(-1, 1))\n",
    "feat_test['weekday_scaled'] = scaler.transform(feat_test['weekday'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train['weekday_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['weekday_scaled'].values.reshape(-1, 1)]))\n",
    "\n",
    "print(get_auc_lr_valid(X_train_sparse_new, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([  1.00000e-03,   2.78256e-03,   7.74264e-03,   2.15443e-02,\n",
       "         5.99484e-02,   1.66810e-01,   4.64159e-01,   1.29155e+00,\n",
       "         3.59381e+00,   1.00000e+01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "param_grid = {\"C\": np.logspace(-3, 1, 10)}\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring=\"roc_auc\")\n",
    "grid.fit(X_train_sparse_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.82565, std: 0.10471, params: {'C': 0.001},\n",
       " mean: 0.82962, std: 0.11273, params: {'C': 0.0027825594022071257},\n",
       " mean: 0.83901, std: 0.11507, params: {'C': 0.0077426368268112694},\n",
       " mean: 0.85244, std: 0.11170, params: {'C': 0.021544346900318832},\n",
       " mean: 0.86887, std: 0.09993, params: {'C': 0.059948425031894091},\n",
       " mean: 0.88458, std: 0.08507, params: {'C': 0.1668100537200059},\n",
       " mean: 0.89793, std: 0.07335, params: {'C': 0.46415888336127775},\n",
       " mean: 0.90825, std: 0.06360, params: {'C': 1.2915496650148828},\n",
       " mean: 0.91585, std: 0.05516, params: {'C': 3.5938136638046259},\n",
       " mean: 0.92051, std: 0.04787, params: {'C': 10.0}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.96548\n",
      "0.00278255940221 0.96817\n",
      "0.00774263682681 0.96993\n",
      "0.0215443469003 0.97157\n",
      "0.0599484250319 0.97424\n",
      "0.16681005372 0.97632\n",
      "0.464158883361 0.97723\n",
      "1.29154966501 0.97662\n",
      "3.5938136638 0.97507\n",
      "10.0 0.97285\n"
     ]
    }
   ],
   "source": [
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 16 ms, total: 2.17 s\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_new, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_new)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-891654a9eb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_youtube'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"youtube\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ytimg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_youtube'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"youtube\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ytimg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n\u001b[1;32m      5\u001b[0m                                         feat_train['is_youtube'].values.reshape(-1, 1)]))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-891654a9eb51>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_youtube'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"youtube\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ytimg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_youtube'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"youtube\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ytimg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n\u001b[1;32m      5\u001b[0m                                         feat_train['is_youtube'].values.reshape(-1, 1)]))\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "feat_train['is_youtube'] = train_df['site1'].apply(lambda s: 1 if (\"youtube\" in s) or (\"ytimg\" in s) else 0)\n",
    "feat_test['is_youtube'] = test_df['site1'].apply(lambda s: 1 if (\"youtube\" in s) or (\"ytimg\" in s) else 0)\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n",
    "                                        feat_train['is_youtube'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['is_youtube'].values.reshape(-1, 1)]))\n",
    "print(get_auc_lr_valid(X_train_sparse_new, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-3c580b65eb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_social'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"vk_\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_social'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"vk_\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n\u001b[1;32m      5\u001b[0m                                         feat_train['is_social'].values.reshape(-1, 1)]))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-3c580b65eb53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_social'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"vk_\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_social'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"vk_\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n\u001b[1;32m      5\u001b[0m                                         feat_train['is_social'].values.reshape(-1, 1)]))\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "feat_train['is_social'] = train_df['site1'].apply(lambda s: 1 if (\"facebook\" in s) or (\"vk_\" in s) else 0)\n",
    "feat_test['is_social'] = test_df['site1'].apply(lambda s: 1 if (\"facebook\" in s) or (\"vk_\" in s) else 0)\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n",
    "                                        feat_train['is_social'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['is_social'].values.reshape(-1, 1)]))\n",
    "print(get_auc_lr_valid(X_train_sparse_new, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9768\n"
     ]
    }
   ],
   "source": [
    "feat_train['is_google'] = train_df['site1'].apply(lambda s: 1 if \"google\" in s else 0)\n",
    "feat_test['is_google'] = test_df['site1'].apply(lambda s: 1 if \"google\" in s else 0)\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n",
    "                                        feat_train['is_google'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['is_google'].values.reshape(-1, 1)]))\n",
    "print(get_auc_lr_valid(X_train_sparse_new, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 964 ms, sys: 200 ms, total: 1.16 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegressionCV(n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_new, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_new)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws_first_sites.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
