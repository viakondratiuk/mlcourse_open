{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../../data/kaggle_alice/\"\n",
    "!PATH_TO_DATA=../../data/kaggle_alice/\n",
    "\n",
    "INP_TRAIN = \"train_sessions.csv\"\n",
    "INP_TEST  = \"test_sessions.csv\"\n",
    "SITE_DIC = \"site_dic.pkl\"\n",
    "SAMPLE_SUBMIT = \"sample_submission.csv\"\n",
    "\n",
    "!INP_TRAIN=train_sessions.csv\n",
    "!INP_TEST=test_sessions.csv\n",
    "!SITE_DIC=site_dic.pkl\n",
    "!SAMPLE_SUBMIT=sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, ratio=0.9, seed=17):\n",
    "    '''\n",
    "    X, y – выборка\n",
    "    ratio – в каком отношении поделить выборку\n",
    "    C, seed – коэф-т регуляризации и random_state \n",
    "              логистической регрессии\n",
    "    '''\n",
    "    train_len = int(ratio * X.shape[0])\n",
    "    X_train = X[:train_len, :]\n",
    "    X_valid = X[train_len:, :]\n",
    "    y_train = y[:train_len]\n",
    "    y_valid = y[train_len:]\n",
    "    \n",
    "    logit = LogisticRegression(C=C, n_jobs=-1, random_state=seed)\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    valid_pred = logit.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    return round(roc_auc_score(y_valid, valid_pred), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = [\"time%s\" % i for i in range(1, 11)]\n",
    "sites = [\"site%s\" % i for i in range(1, 11)]\n",
    "\n",
    "with open(PATH_TO_DATA + SITE_DIC, \"rb\") as inp_file:\n",
    "    site_dic = pickle.load(inp_file)\n",
    "\n",
    "inv_site_dic = {v: k for k, v in site_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA + INP_TRAIN, \n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times).sort_values(by=\"time1\")\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "test_df = pd.read_csv(PATH_TO_DATA + INP_TEST,\n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times)\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "y_train = train_df[\"target\"]\n",
    "train_df.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_to_text = train_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(train_df[sites]), 1)\n",
    "test_to_text = test_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(test_df[sites]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41592), (82797, 41592))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer())\n",
    "])\n",
    "pipeline.fit(train_to_text.ravel())\n",
    "\n",
    "X_train_sparse = pipeline.transform(train_to_text.ravel())\n",
    "X_test_sparse = pipeline.transform(test_to_text.ravel())\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.77996\n",
      "0.00278255940221 0.83187\n",
      "0.00774263682681 0.8792\n",
      "0.0215443469003 0.90236\n",
      "0.0599484250319 0.91056\n",
      "0.16681005372 0.91727\n",
      "0.464158883361 0.92185\n",
      "1.29154966501 0.92373\n",
      "3.5938136638 0.92341\n",
      "10.0 0.92245\n"
     ]
    }
   ],
   "source": [
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_train = pd.DataFrame(index=train_df.index)\n",
    "feat_test = pd.DataFrame(index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "feat_train['year_month'] = train_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "feat_test['year_month'] = test_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['year_month_scaled'] = scaler.fit_transform(feat_train['year_month'].values.reshape(-1, 1))\n",
    "feat_test['year_month_scaled'] = scaler.transform(feat_test['year_month'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse, \n",
    "                                        feat_train['year_month_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse, \n",
    "                                       feat_test['year_month_scaled'].values.reshape(-1, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.77932\n",
      "0.00278255940221 0.83217\n",
      "0.00774263682681 0.87618\n",
      "0.0215443469003 0.89742\n",
      "0.0599484250319 0.9083\n",
      "0.16681005372 0.91647\n",
      "0.464158883361 0.92163\n",
      "1.29154966501 0.92341\n",
      "3.5938136638 0.92277\n",
      "10.0 0.92191\n"
     ]
    }
   ],
   "source": [
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.9193\n",
      "0.00278255940221 0.92766\n",
      "0.00774263682681 0.93361\n",
      "0.0215443469003 0.94103\n",
      "0.0599484250319 0.95025\n",
      "0.16681005372 0.95743\n",
      "0.464158883361 0.96158\n",
      "1.29154966501 0.963\n",
      "3.5938136638 0.96259\n",
      "10.0 0.96126\n",
      "CPU times: user 16.5 s, sys: 72 ms, total: 16.6 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_train['start_hour'] = train_df['time1'].apply(lambda ts: ts.hour)\n",
    "feat_test['start_hour'] = test_df['time1'].apply(lambda ts: ts.hour)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['start_hour_scaled'] = scaler.fit_transform(feat_train['start_hour'].values.reshape(-1, 1))\n",
    "feat_test['start_hour_scaled'] = scaler.transform(feat_test['start_hour'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train['start_hour_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['start_hour_scaled'].values.reshape(-1, 1)]))\n",
    "\n",
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.96173\n",
      "0.00278255940221 0.96539\n",
      "0.00774263682681 0.96766\n",
      "0.0215443469003 0.96969\n",
      "0.0599484250319 0.9729\n",
      "0.16681005372 0.9751\n",
      "0.464158883361 0.97592\n",
      "1.29154966501 0.97518\n",
      "3.5938136638 0.97351\n",
      "10.0 0.97139\n",
      "CPU times: user 18.5 s, sys: 64 ms, total: 18.6 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_train['weekday'] = train_df['time1'].apply(lambda ts: ts.dayofweek)\n",
    "feat_test['weekday'] = test_df['time1'].apply(lambda ts: ts.dayofweek)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['weekday_scaled'] = scaler.fit_transform(feat_train['weekday'].values.reshape(-1, 1))\n",
    "feat_test['weekday_scaled'] = scaler.transform(feat_test['weekday'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train['weekday_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['weekday_scaled'].values.reshape(-1, 1)]))\n",
    "\n",
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.93055\n",
      "0.00278255940221 0.96539\n",
      "0.00774263682681 0.96887\n",
      "0.0215443469003 0.9704\n",
      "0.0599484250319 0.97295\n",
      "0.16681005372 0.97481\n",
      "0.464158883361 0.97566\n",
      "1.29154966501 0.97536\n",
      "3.5938136638 0.9742\n",
      "10.0 0.97266\n"
     ]
    }
   ],
   "source": [
    "feat_train['morning'] = train_df['time1'].apply(lambda ts: int(ts.hour > 4 and ts.hour <= 11))\n",
    "feat_test['morning'] = test_df['time1'].apply(lambda ts: int(ts.hour > 4 and ts.hour <= 11))\n",
    "\n",
    "X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new,  \n",
    "                                        feat_train['morning'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['morning'].values.reshape(-1, 1)]))\n",
    "\n",
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 8 ms, total: 2.21 s\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(C=0.5, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_new, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_new)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws_m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.82335\n",
      "0.00278255940221 0.9255\n",
      "0.00774263682681 0.9612\n",
      "0.0215443469003 0.96871\n",
      "0.0599484250319 0.97233\n",
      "0.16681005372 0.97461\n",
      "0.464158883361 0.97552\n",
      "1.29154966501 0.97526\n",
      "3.5938136638 0.97408\n",
      "10.0 0.97251\n"
     ]
    }
   ],
   "source": [
    "feat_train['is_youtube'] = train_df['site1'].apply(lambda s: 1 if (\"youtube\" in inv_site_dic[s]) or (\"ytimg\" in inv_site_dic[s]) else 0)\n",
    "feat_test['is_youtube'] = test_df['site1'].apply(lambda s: 1 if (\"youtube\" in inv_site_dic[s]) or (\"ytimg\" in inv_site_dic[s]) else 0)\n",
    "\n",
    "X_train_sparse_site = csr_matrix(hstack([X_train_sparse_new,  \n",
    "                                        feat_train['is_youtube'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_site = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['is_youtube'].values.reshape(-1, 1)]))\n",
    "\n",
    "feat_train['is_social'] = train_df['site1'].apply(lambda s: 1 if (\"facebook\" in inv_site_dic[s]) or (\"vk_\" in inv_site_dic[s]) else 0)\n",
    "feat_test['is_social'] = test_df['site1'].apply(lambda s: 1 if (\"facebook\" in inv_site_dic[s]) or (\"vk_\" in inv_site_dic[s]) else 0)\n",
    "\n",
    "X_train_sparse_site = csr_matrix(hstack([X_train_sparse_site,  \n",
    "                                        feat_train['is_social'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_site = csr_matrix(hstack([X_test_sparse_site, \n",
    "                                       feat_test['is_social'].values.reshape(-1, 1)]))\n",
    "\n",
    "feat_train['is_google'] = train_df['site1'].apply(lambda s: 1 if \"google\" in inv_site_dic[s] else 0)\n",
    "feat_test['is_google'] = test_df['site1'].apply(lambda s: 1 if \"google\" in inv_site_dic[s] else 0)\n",
    "\n",
    "X_train_sparse_site = csr_matrix(hstack([X_train_sparse_site,  \n",
    "                                        feat_train['is_google'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_site = csr_matrix(hstack([X_test_sparse_site, \n",
    "                                       feat_test['is_google'].values.reshape(-1, 1)]))\n",
    "\n",
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_site, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 8 ms, total: 2.57 s\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(C=0.5, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_site, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_site)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws_m_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251175</th>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>7832</td>\n",
       "      <td>21</td>\n",
       "      <td>7832</td>\n",
       "      <td>30</td>\n",
       "      <td>7832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196388</th>\n",
       "      <td>29</td>\n",
       "      <td>7832</td>\n",
       "      <td>37</td>\n",
       "      <td>7832</td>\n",
       "      <td>7832</td>\n",
       "      <td>29</td>\n",
       "      <td>7832</td>\n",
       "      <td>29</td>\n",
       "      <td>7832</td>\n",
       "      <td>7832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172448</th>\n",
       "      <td>29</td>\n",
       "      <td>7832</td>\n",
       "      <td>7832</td>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>7832</td>\n",
       "      <td>29</td>\n",
       "      <td>7832</td>\n",
       "      <td>29</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70129</th>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>1515</td>\n",
       "      <td>167</td>\n",
       "      <td>37</td>\n",
       "      <td>1514</td>\n",
       "      <td>855</td>\n",
       "      <td>1515</td>\n",
       "      <td>855</td>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206254</th>\n",
       "      <td>1520</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1515</td>\n",
       "      <td>1515</td>\n",
       "      <td>1524</td>\n",
       "      <td>1514</td>\n",
       "      <td>1515</td>\n",
       "      <td>1520</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "251175        270    270    270     21     21   7832     21   7832     30   \n",
       "196388         29   7832     37   7832   7832     29   7832     29   7832   \n",
       "172448         29   7832   7832     29     37   7832     29   7832     29   \n",
       "70129         167    167   1515    167     37   1514    855   1515    855   \n",
       "206254       1520   1522   1522   1515   1515   1524   1514   1515   1520   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "251175        7832  \n",
       "196388        7832  \n",
       "172448         270  \n",
       "70129         1514  \n",
       "206254        1521  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[y_train == 1][sites].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77       1382\n",
       "80       1354\n",
       "76       1307\n",
       "29        897\n",
       "21        857\n",
       "81        609\n",
       "879       522\n",
       "22        522\n",
       "75        451\n",
       "82        447\n",
       "23        437\n",
       "35        381\n",
       "881       371\n",
       "37        293\n",
       "33        291\n",
       "3000      286\n",
       "733       274\n",
       "30        272\n",
       "78        236\n",
       "941       215\n",
       "7832      209\n",
       "52        206\n",
       "704       204\n",
       "0         201\n",
       "2078      188\n",
       "617       159\n",
       "1057      155\n",
       "942       153\n",
       "270       151\n",
       "335       150\n",
       "         ... \n",
       "24480       1\n",
       "27294       1\n",
       "21584       1\n",
       "8931        1\n",
       "27370       1\n",
       "27240       1\n",
       "27182       1\n",
       "27386       1\n",
       "970         1\n",
       "13934       1\n",
       "27272       1\n",
       "188         1\n",
       "3003        1\n",
       "12613       1\n",
       "2590        1\n",
       "27288       1\n",
       "2381        1\n",
       "6400        1\n",
       "508         1\n",
       "540         1\n",
       "27169       1\n",
       "19190       1\n",
       "14244       1\n",
       "27185       1\n",
       "5061        1\n",
       "698         1\n",
       "27352       1\n",
       "1570        1\n",
       "5648        1\n",
       "3159        1\n",
       "Length: 1054, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[y_train == 1][sites].stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1.ytimg.com\n",
      "s.youtube.com\n",
      "www.youtube.com\n",
      "www.facebook.com\n",
      "www.google.fr\n",
      "r4---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "r1---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "apis.google.com\n",
      "s.ytimg.com\n",
      "r2---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "www.google.com\n",
      "s-static.ak.facebook.com\n",
      "r3---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "twitter.com\n",
      "static.ak.facebook.com\n",
      "vk.com\n",
      "translate.google.fr\n",
      "platform.twitter.com\n",
      "yt3.ggpht.com\n",
      "mts0.google.com\n",
      "www.info-jeunes.net\n",
      "clients1.google.com\n",
      "www.audienceinsights.net\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3c8bb9ec38c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_site_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "a = train_df[y_train == 1][sites].stack().value_counts().nlargest(30).index\n",
    "for s in a:\n",
    "    print(inv_site_dic[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.93522\n",
      "0.00278255940221 0.96685\n",
      "0.00774263682681 0.96806\n",
      "0.0215443469003 0.96884\n",
      "0.0599484250319 0.97126\n",
      "0.16681005372 0.97307\n",
      "0.464158883361 0.97408\n",
      "1.29154966501 0.97436\n",
      "3.5938136638 0.97388\n",
      "10.0 0.97266\n",
      "CPU times: user 25 s, sys: 32 ms, total: 25 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_train['uniq_sites'] = train_df[sites].apply(lambda x: len(set(a for a in x.values if a != 0)), axis=1)\\\n",
    "               .values.reshape(len(train_df[sites]), 1)\n",
    "feat_test['uniq_sites'] = test_df[sites].apply(lambda x: len(set(a for a in x.values if a != 0)), axis=1)\\\n",
    "               .values.reshape(len(test_df[sites]), 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['uniq_sites_scaled'] = scaler.fit_transform(feat_train['uniq_sites'].values.reshape(-1, 1))\n",
    "feat_test['uniq_sites_scaled'] = scaler.transform(feat_test['uniq_sites'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_sparse_uniq = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train['uniq_sites_scaled'].values.reshape(-1, 1)]))\n",
    "X_test_sparse_uniq = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                       feat_test['uniq_sites_scaled'].values.reshape(-1, 1)]))\n",
    "\n",
    "for C in np.logspace(-3, 1, 10):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_uniq, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 8 ms, total: 3.4 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(C=1.2, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_site, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_site)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws_m_uniq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([  1.00000e-03,   2.78256e-03,   7.74264e-03,   2.15443e-02,\n",
       "         5.99484e-02,   1.66810e-01,   4.64159e-01,   1.29155e+00,\n",
       "         3.59381e+00,   1.00000e+01]),\n",
       "           class_weight='balanced', cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=17,\n",
       "           refit=True, scoring='roc_auc', solver='lbfgs', tol=0.0001,\n",
       "           verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_params={'scoring':'roc_auc','class_weight':'balanced',\n",
    "             'Cs':np.logspace(-3, 1, 10),'n_jobs':-1, 'random_state':17}\n",
    "logit = LogisticRegressionCV(**logit_params)\n",
    "logit.fit(X_train_sparse_site, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[ 0.82549158,  0.84968286,  0.87119884,  0.88443907,  0.89032856,\n",
       "          0.89230763,  0.89352081,  0.89537402,  0.89789291,  0.90035543],\n",
       "        [ 0.8597251 ,  0.88779918,  0.90753221,  0.91753303,  0.92073008,\n",
       "          0.91990324,  0.91679022,  0.91270905,  0.90708727,  0.90108702],\n",
       "        [ 0.94136112,  0.95111974,  0.95242123,  0.95089276,  0.94879406,\n",
       "          0.94625014,  0.94372691,  0.94133113,  0.93943402,  0.93706618]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.scores_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
