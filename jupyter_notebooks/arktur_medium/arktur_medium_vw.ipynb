{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperopt_objective(params):\n",
    "    !vw -d data/habr_train.vw \\\n",
    "    -l {params['l']} \\\n",
    "    --quiet\n",
    "    \n",
    "    !vw -i habr_model.vw -t \\\n",
    "    -d data/habr_valid.vw \\\n",
    "    -p habr_valid_pred.txt \\\n",
    "    --quiet\n",
    "    \n",
    "    with open('habr_valid_pred.txt') as pred_file:\n",
    "        valid_prediction = [float(popularity) for popularity in pred_file.readlines()]\n",
    "    score = mean_absolute_error(valid_vals, valid_prediction)\n",
    "    return score\n",
    "\n",
    "\n",
    "params_space = {\n",
    "    'l': hp.uniform('l', 0.2, 0.8), # default = 0.5\n",
    "}\n",
    "\n",
    "\n",
    "%%time\n",
    "trials = Trials()\n",
    "\n",
    "best_params = fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=200,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Как завести vw-hyperopt.py*\n",
    "1. Установите нужные либы\n",
    "!pip install hyperopt\n",
    "!pip install networkx==1.11 - откат одной из связанных библиотек для работы скрипта\n",
    "\n",
    "2. Найдите, куда установилось\n",
    "!find / -name “vowpalwabbit” 2>/dev/null\n",
    "!find / -name “vw-hyperopt.py” 2>/dev/null\n",
    "\n",
    "3. Подготовьте vw-hyperopt.py\n",
    "Необходимо добавить encoding=“utf-8” во все функции open() для правильной работы с utf-8 файлами\n",
    "\n",
    "*Ну и теперь можно удачно запустить.*\n",
    "Пример строки запуска(параметры изменил)\n",
    "%run /vowpal_wabbit/utl/vw-hyperopt.py \\ --train ../../data/vowpal/habr_traink.vw \\ --holdout ../../data/vowpal/habr_tholdout.vw \\ --max_evals 200 \\ --outer_loss_function squared \\ --vw_space ‘--algorithms=sgd -l=0.01..1~L --passes=1..10~I --loss_function=squared ’ --plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../../data/arktur_medium/'\n",
    "!PATH_TO_DATA=../../data/arktur_medium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -1 $PATH_TO_DATA/train.json > $PATH_TO_DATA/train1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, \"train1.json\")) as inp_json:\n",
    "    first_json = json.load(inp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['domain', '_timestamp', 'published', 'tags', 'url', 'content', 'quality', 'meta_tags', 'image_url', 'author', 'link_tags', 'title', '_spider', '_id'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium.com'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_json[\"domain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_TRAIN = \"train.json\"\n",
    "INP_TEST  = \"test.json\"\n",
    "INP_TRAIN_PARSED = \"train_parsed.csv\"\n",
    "INP_TEST_PARSED = \"test_parsed.csv\"\n",
    "!INP_TRAIN=train.json\n",
    "!INP_TEST=test.json\n",
    "\n",
    "OUT_TRAIN = \"train.vw\"\n",
    "OUT_PART_TRAIN = \"part_train.vw\"\n",
    "OUT_PART_VALID = \"part_valid.vw\"\n",
    "OUT_TEST = \"test.vw\"\n",
    "!OUT_TRAIN=train.vw\n",
    "!OUT_PART_TRAIN=part_train.vw\n",
    "!OUT_PART_VALID=part_valid.vw\n",
    "!OUT_TEST=test.vw\n",
    "\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                        'train_log1p_recommends.csv'), \n",
    "                           index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform data to vw format\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stop = set(stopwords.words(\"english\") + list(string.punctuation) + [\"’\", \"“\", \"”\"])\n",
    "tokenize = lambda s: \" \".join(lmtzr.lemmatize(i) for i in word_tokenize(s.lower()) if i not in stop)\n",
    "\n",
    "\n",
    "def to_vw(data, label):\n",
    "    exclude = set(string.punctuation)\n",
    "       \n",
    "    content = strip_tags(data[\"content\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    content = tokenize(content)\n",
    "    \n",
    "    title = strip_tags(data[\"title\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    title = tokenize(title)\n",
    "    \n",
    "    author = strip_tags(data[\"meta_tags\"][\"author\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    author = \"\".join(ch for ch in author if ch not in exclude).replace(\" \", \"_\").lower()\n",
    "    \n",
    "    domain = data[\"domain\"].replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\").lower()\n",
    "    \n",
    "    # date time\n",
    "    dt = datetime.strptime(data[\"published\"][\"$date\"][:-1], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    month = dt.month\n",
    "    weekday = dt.weekday()\n",
    "    hour = dt.hour \n",
    "    read_time = data[\"meta_tags\"][\"twitter:data1\"].split(\" \")[0]\n",
    "    read_time = read_time if read_time else 0.1\n",
    "    \n",
    "    # binary\n",
    "    is_weekend = 1 if weekday in (5, 6) else 0\n",
    "    is_night = 1 if hour in range(0, 7) else 0\n",
    "    is_morning = 1 if hour in range(7, 12) else 0\n",
    "    is_noon = 1 if hour in range(12, 19) else 0\n",
    "    is_eve = 1 if hour in range(19, 24) else 0\n",
    "    is_image = 1 if data[\"image_url\"] else 0\n",
    "\n",
    "\n",
    "    out = [\n",
    "        str(np.log(label)),\n",
    "        \"a_content %s\" % content,\n",
    "        \"b_author %s\" % author,\n",
    "        \"c_title %s\" % title,\n",
    "        \"d_domain %s\" % domain,\n",
    "        \"e_num 10:%s 11:%s 12:%s 13:%s\" % (month, weekday + 1, hour + 1, read_time),\n",
    "        \"f_binary 0:%s 1:%s 2:%s 3:%s 4:%s 5:%s\" % \n",
    "        (is_weekend, is_night, is_morning, is_noon, is_eve, is_image)\n",
    "    ]\n",
    "    \n",
    "    return str(\" |\".join(out).encode('ascii', 'ignore').strip())[2:-1] + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def to_vw1(label, content, tags, data):\n",
    "    author = strip_tags(data[\"meta_tags\"][\"author\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    author = \"\".join(ch for ch in author if ch not in exclude).replace(\" \", \"_\").lower()\n",
    "    \n",
    "    domain = data[\"domain\"].replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\").lower()\n",
    "    \n",
    "    # date time\n",
    "    dt = datetime.strptime(data[\"published\"][\"$date\"][:-1], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    month = dt.month\n",
    "    weekday = dt.weekday()\n",
    "    hour = dt.hour \n",
    "    read_time = data[\"meta_tags\"][\"twitter:data1\"].split(\" \")[0]\n",
    "    read_time = read_time if read_time else 0.1\n",
    "    \n",
    "    # binary\n",
    "    is_weekend = 1 if weekday in (5, 6) else 0\n",
    "    is_night = 1 if hour in range(0, 7) else 0\n",
    "    is_morning = 1 if hour in range(7, 12) else 0\n",
    "    is_noon = 1 if hour in range(12, 19) else 0\n",
    "    is_eve = 1 if hour in range(19, 24) else 0\n",
    "    is_image = 1 if data[\"image_url\"] else 0\n",
    "    \n",
    "    out = [\n",
    "        str(np.log(label)) if LOG else str(label),\n",
    "        \"a_content %s\" % content,\n",
    "        \"b_tags %s\" % (tags if tags else \"none\",),\n",
    "        \"d_domain %s\" % domain,\n",
    "        \"e_num 10:%s 11:%s 12:%s 13:%s\" % (month, weekday + 1, hour + 1, read_time),\n",
    "        \"f_binary 0:%s 1:%s 2:%s 3:%s 4:%s 5:%s\" % \n",
    "        (is_weekend, is_night, is_morning, is_noon, is_eve, is_image),\n",
    "        \"g_author %s\" % author,\n",
    "    ]\n",
    "    \n",
    "    return \" |\".join(out) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(label, data):\n",
    "    # date time\n",
    "    dt = datetime.strptime(data[\"published\"][\"$date\"][:-1], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "#     publish_ts = pd.to_datetime(data['published']['$date'].replace('T', ' '))\n",
    "    \n",
    "    read_time = data[\"meta_tags\"][\"twitter:data1\"].split(\" \")[0]\n",
    "    read_time = read_time if read_time else \"0\"\n",
    "        \n",
    "    out = [\n",
    "        data[\"published\"][\"$date\"][:-1],\n",
    "        str(label),\n",
    "    ]\n",
    "    \n",
    "    return \",\".join(out) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db6fbdd4dd540d0b81d30af2dcbd024"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(os.path.join(PATH_TO_DATA, INP_TRAIN), encoding=\"utf-8\") as inp_json, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, INP_TRAIN_PARSED), encoding=\"utf-8\") as inp_parsed, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, \"train.csv\"), 'w', encoding=\"utf-8\") as out_train:\n",
    "    N = 52699\n",
    "#     N = 100        \n",
    "    header = False\n",
    "    \n",
    "    for n, (json_, parsed) in enumerate(tqdm_notebook(zip(inp_json, inp_parsed), total=N)):\n",
    "        if n == N: break\n",
    "#         if not header:\n",
    "#             out_train.write(\"Time,Label\\n\")\n",
    "#             header = True\n",
    "\n",
    "        json_data = json.loads(json_)\n",
    "        url, content, tags = parsed.split(\",\")\n",
    "        tags = tags.strip()\n",
    "        \n",
    "        label = train_target.iloc[n][\"log_recommends\"]\n",
    "#         out = to_vw(json_data, label)\n",
    "        out = to_vw1(label, content, tags, json_data)\n",
    "#         out = to_csv(label, json_data)\n",
    "        out_train.write(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import graph_objs as go\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "def plotly_df(df, title = ''):\n",
    "    data = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x = df.index,\n",
    "            y = df[column],\n",
    "            mode = 'lines',\n",
    "            name = column\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = dict(title = title)\n",
    "    fig = dict(data = data, layout = layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "df = pd.read_csv(os.path.join(PATH_TO_DATA, \"test.csv\"), parse_dates=['Time'])\n",
    "# df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-23 08:19:34.211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-23 14:27:27.083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-23 12:36:34.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06-23 15:11:42.425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-06-23 15:21:54.977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time  Label\n",
       "0 2017-06-23 08:19:34.211      1\n",
       "1 2017-06-23 14:27:27.083      1\n",
       "2 2017-06-23 12:36:34.000      1\n",
       "3 2017-06-23 15:11:42.425      1\n",
       "4 2017-06-23 15:21:54.977      1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height has been deprecated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">2017</th>\n",
       "      <th>1</th>\n",
       "      <td>5046</td>\n",
       "      <td>5046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5025</td>\n",
       "      <td>5025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6103</td>\n",
       "      <td>6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6604</td>\n",
       "      <td>6604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8577</td>\n",
       "      <td>8577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8137</td>\n",
       "      <td>8137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time Label\n",
       "          count count\n",
       "Time Time            \n",
       "2017 1     5046  5046\n",
       "     2     5025  5025\n",
       "     3     6103  6103\n",
       "     4     6604  6604\n",
       "     5     8577  8577\n",
       "     6     8137  8137"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([df.Time.dt.year, df.Time.dt.month]).agg({\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "Label",
         "type": "scatter",
         "x": [
          "2016-12-23 15:53:30.718000",
          "2016-12-23 16:01:01.565000",
          "2016-12-23 16:22:24.813000",
          "2016-12-23 16:46:18.982000",
          "2016-12-23 16:49:19.775000",
          "2016-12-23 16:52:58.407000",
          "2016-12-23 16:58:18.601000",
          "2016-12-23 17:01:10.853000",
          "2016-12-23 17:28:20.805000",
          "2016-12-23 17:37:43.643000",
          "2016-12-23 17:53:38.413000",
          "2016-12-23 17:56:26.242000",
          "2016-12-23 18:17:59.181000",
          "2016-12-23 18:19:16.464000",
          "2016-12-23 18:22:39.945000",
          "2016-12-23 18:26:08",
          "2016-12-23 18:38:13.888000",
          "2016-12-23 18:40:45.846000",
          "2016-12-23 19:10:46.357000",
          "2016-12-23 19:23:45.172000",
          "2016-12-23 19:29:01.670000",
          "2016-12-23 19:39:33.457000",
          "2016-12-23 19:40:53.997000",
          "2016-12-23 19:58:01.981000",
          "2016-12-23 20:04:00.106000",
          "2016-12-23 20:23:28.587000",
          "2016-12-23 20:39:51.860000",
          "2016-12-23 21:10:09.295000",
          "2016-12-23 21:25:25.159000",
          "2016-12-23 21:30:11.194000",
          "2016-12-23 21:35:36.930000",
          "2016-12-23 21:38:17.414000",
          "2016-12-23 21:54:19.476000",
          "2016-12-23 21:57:12.602000",
          "2016-12-23 22:03:51.205000",
          "2016-12-23 22:05:17.490000",
          "2016-12-23 22:11:43.077000",
          "2016-12-23 22:35:12.931000",
          "2016-12-23 22:39:40.040000",
          "2016-12-23 23:38:05.726000",
          "2016-12-23 23:38:06.062000",
          "2016-12-23 23:47:34.435000",
          "2016-12-23 23:57:53.877000",
          "2016-12-24 00:01:07.669000",
          "2016-12-24 00:05:26.151000",
          "2016-12-24 00:08:49.419000",
          "2016-12-24 00:15:53.132000",
          "2016-12-24 00:19:06.082000",
          "2016-12-24 00:26:33.146000",
          "2016-12-24 00:38:54.359000",
          "2016-12-24 00:53:51.615000",
          "2016-12-24 00:58:10.609000",
          "2016-12-24 02:06:13.271000",
          "2016-12-24 02:13:48.916000",
          "2016-12-24 02:47:03.725000",
          "2016-12-24 03:22:46.851000",
          "2016-12-24 03:36:45.364000",
          "2016-12-24 03:37:00.894000",
          "2016-12-24 03:50:27.996000",
          "2016-12-24 03:56:01.347000",
          "2016-12-24 04:07:44.752000",
          "2016-12-24 04:13:29.771000",
          "2016-12-24 04:39:27.901000",
          "2016-12-24 05:12:09.540000",
          "2016-12-24 05:17:10.088000",
          "2016-12-24 06:04:55.942000",
          "2016-12-24 06:17:10.332000",
          "2016-12-24 06:17:19.029000",
          "2016-12-24 07:29:21.748000",
          "2016-12-24 08:05:38.002000",
          "2016-12-24 08:11:01.367000",
          "2016-12-24 08:40:16.309000",
          "2016-12-24 08:40:39.604000",
          "2016-12-24 09:26:45.069000",
          "2016-12-24 10:35:53.938000",
          "2016-12-24 10:59:27.579000",
          "2016-12-24 11:21:26.862000",
          "2016-12-24 11:45:31.389000",
          "2016-12-24 12:47:47.786000",
          "2016-12-24 13:01:37.613000",
          "2016-12-24 13:03:42.031000",
          "2016-12-24 13:04:31.551000",
          "2016-12-24 13:05:46.233000",
          "2016-12-24 13:07:10.670000",
          "2016-12-24 13:20:12.217000",
          "2016-12-24 13:22:55.304000",
          "2016-12-24 13:29:57.720000",
          "2016-12-24 13:43:59.174000",
          "2016-12-24 13:47:08.209000",
          "2016-12-24 14:11:46.525000",
          "2016-12-24 14:34:48.982000",
          "2016-12-24 14:56:09.018000",
          "2016-12-24 15:03:03.805000",
          "2016-12-24 15:31:40.185000",
          "2016-12-24 15:57:51.214000",
          "2016-12-24 15:58:50.199000",
          "2016-12-24 16:01:01.540000",
          "2016-12-24 16:02:20.287000",
          "2016-12-24 16:03:45.710000",
          "2016-12-24 16:10:45.552000",
          "2016-12-24 16:28:22.363000",
          "2016-12-24 16:33:31.462000",
          "2016-12-24 16:39:54.555000",
          "2016-12-24 16:42:38.858000",
          "2016-12-24 17:35:47.354000",
          "2016-12-24 17:59:05.886000",
          "2016-12-24 18:21:13.427000",
          "2016-12-24 18:35:04.966000",
          "2016-12-24 19:01:01.737000",
          "2016-12-24 19:26:08.771000",
          "2016-12-24 20:03:32.943000",
          "2016-12-24 22:30:37.611000",
          "2016-12-25 00:54:39.256000",
          "2016-12-25 01:10:33.156000",
          "2016-12-25 01:29:55.583000",
          "2016-12-25 02:00:35",
          "2016-12-25 02:01:01.281000",
          "2016-12-25 02:36:47.459000",
          "2016-12-25 03:07:57.271000",
          "2016-12-25 03:45:35.359000",
          "2016-12-25 04:28:28.426000",
          "2016-12-25 04:45:32.030000",
          "2016-12-25 04:59:24.268000",
          "2016-12-25 05:53:44.534000",
          "2016-12-25 07:06:42.985000",
          "2016-12-25 07:43:27.635000",
          "2016-12-25 08:17:17.006000",
          "2016-12-25 08:40:14.899000",
          "2016-12-25 08:58:58.175000",
          "2016-12-25 10:47:01.541000",
          "2016-12-25 10:48:53.564000",
          "2016-12-25 10:51:14.297000",
          "2016-12-25 11:04:11.885000",
          "2016-12-25 11:22:01.003000",
          "2016-12-25 11:23:06.895000",
          "2016-12-25 13:06:35.738000",
          "2016-12-25 13:20:00.450000",
          "2016-12-25 13:20:39.624000",
          "2016-12-25 14:07:47.185000",
          "2016-12-25 14:28:02.507000",
          "2016-12-25 14:59:48.358000",
          "2016-12-25 15:30:46.848000",
          "2016-12-25 15:50:32.652000",
          "2016-12-25 16:33:57.347000",
          "2016-12-25 16:39:25.081000",
          "2016-12-25 16:44:39.622000",
          "2016-12-25 16:46:35",
          "2016-12-25 17:00:28.900000",
          "2016-12-25 17:14:27.688000",
          "2016-12-25 17:18:53.969000",
          "2016-12-25 17:25:10.509000",
          "2016-12-25 17:25:12.173000",
          "2016-12-25 18:55:42.621000",
          "2016-12-25 19:31:42.824000",
          "2016-12-25 20:25:31.956000",
          "2016-12-25 20:48:29.348000",
          "2016-12-25 20:57:53.312000",
          "2016-12-25 21:02:08.670000",
          "2016-12-25 21:05:35.551000",
          "2016-12-25 21:25:15.101000",
          "2016-12-25 21:35:09.018000",
          "2016-12-25 21:35:25.562000",
          "2016-12-25 23:31:22.500000",
          "2016-12-25 23:49:42.495000",
          "2016-12-26",
          "2016-12-26 00:21:31.175000",
          "2016-12-26 00:38:00.047000",
          "2016-12-26 01:00:05.202000",
          "2016-12-26 01:05:25.473000",
          "2016-12-26 01:48:51.935000",
          "2016-12-26 01:57:41.679000",
          "2016-12-26 02:37:29.819000",
          "2016-12-26 02:48:07.067000",
          "2016-12-26 02:57:07.649000",
          "2016-12-26 02:57:20.146000",
          "2016-12-26 02:59:58.497000",
          "2016-12-26 03:39:15.024000",
          "2016-12-26 04:35:04.722000",
          "2016-12-26 04:54:40.497000",
          "2016-12-26 04:56:17.856000",
          "2016-12-26 05:31:08.076000",
          "2016-12-26 06:09:37.064000",
          "2016-12-26 06:23:23.349000",
          "2016-12-26 06:28:45.144000",
          "2016-12-26 06:37:50.493000",
          "2016-12-26 06:55:50.031000",
          "2016-12-26 07:17:44.230000",
          "2016-12-26 07:19:55.989000",
          "2016-12-26 07:22:04.687000",
          "2016-12-26 07:27:59.865000",
          "2016-12-26 07:55:02.563000",
          "2016-12-26 08:00:22.561000",
          "2016-12-26 08:03:50.527000",
          "2016-12-26 08:22:16.742000",
          "2016-12-26 08:42:57.641000",
          "2016-12-26 08:44:53.593000",
          "2016-12-26 09:09:19.620000",
          "2016-12-26 10:16:52.874000",
          "2016-12-26 10:18:48.546000",
          "2016-12-26 11:18:19.097000",
          "2016-12-26 11:33:23.325000",
          "2016-12-26 11:33:26.244000",
          "2016-12-26 12:19:02.234000",
          "2016-12-26 12:29:18.035000",
          "2016-12-26 12:39:36.314000",
          "2016-12-26 13:01:01.557000",
          "2016-12-26 13:02:31.711000",
          "2016-12-26 13:21:03.790000",
          "2016-12-26 13:21:04.587000",
          "2016-12-26 13:37:31.998000",
          "2016-12-26 13:53:50.173000",
          "2016-12-26 13:57:17.487000",
          "2016-12-26 14:01:01.490000",
          "2016-12-26 14:07:42.244000",
          "2016-12-26 14:27:23.319000",
          "2016-12-26 14:45:28.681000",
          "2016-12-26 15:01:01.665000",
          "2016-12-26 15:01:43.347000",
          "2016-12-26 15:08:39.037000",
          "2016-12-26 15:12:55.687000",
          "2016-12-26 15:34:58.118000",
          "2016-12-26 15:38:05.408000",
          "2016-12-26 16:01:56.605000",
          "2016-12-26 16:14:14.702000",
          "2016-12-26 16:26:16.277000",
          "2016-12-26 16:32:50.694000",
          "2016-12-26 16:34:14.270000",
          "2016-12-26 16:38:13.877000",
          "2016-12-26 16:38:22.330000",
          "2016-12-26 16:51:09.117000",
          "2016-12-26 16:59:05.550000",
          "2016-12-26 17:00:33.343000",
          "2016-12-26 17:09:39.318000",
          "2016-12-26 17:16:15.874000",
          "2016-12-26 17:23:48.318000",
          "2016-12-26 17:32:19.061000",
          "2016-12-26 17:36:01.287000",
          "2016-12-26 17:47:45.283000",
          "2016-12-26 17:50:59.828000",
          "2016-12-26 17:52:45.569000",
          "2016-12-26 17:52:54.504000",
          "2016-12-26 17:54:09.147000",
          "2016-12-26 18:22:18.642000",
          "2016-12-26 18:30:55.693000",
          "2016-12-26 18:39:26.847000",
          "2016-12-26 18:53:12.883000",
          "2016-12-26 18:59:43.557000",
          "2016-12-26 19:02:36.546000",
          "2016-12-26 19:10:08.267000",
          "2016-12-26 19:14:27.728000",
          "2016-12-26 19:25:56.098000",
          "2016-12-26 19:31:09.234000",
          "2016-12-26 20:03:05.951000",
          "2016-12-26 20:19:06.019000",
          "2016-12-26 20:20:43.254000",
          "2016-12-26 20:38:58.641000",
          "2016-12-26 20:40:22.647000",
          "2016-12-26 20:42:24.307000",
          "2016-12-26 20:52:32.405000",
          "2016-12-26 21:09:07.047000",
          "2016-12-26 21:16:07.582000",
          "2016-12-26 21:38:00.223000",
          "2016-12-26 21:52:40.449000",
          "2016-12-26 22:09:05.074000",
          "2016-12-26 22:32:58.110000",
          "2016-12-26 22:34:42.467000",
          "2016-12-26 22:39:24.100000",
          "2016-12-26 22:51:08.174000",
          "2016-12-26 23:30:45.086000",
          "2016-12-26 23:31:48.388000",
          "2016-12-26 23:45:32.558000",
          "2016-12-26 23:48:55.415000",
          "2016-12-26 23:50:58.725000",
          "2016-12-27 00:10:56.507000",
          "2016-12-27 00:12:25.769000",
          "2016-12-27 00:21:07.144000",
          "2016-12-27 00:24:03.622000",
          "2016-12-27 00:26:59.103000",
          "2016-12-27 00:31:56.572000",
          "2016-12-27 00:56:50.919000",
          "2016-12-27 00:57:43.988000",
          "2016-12-27 00:59:15.112000",
          "2016-12-27 01:02:12.447000",
          "2016-12-27 01:53:35.636000",
          "2016-12-27 01:56:28.693000",
          "2016-12-27 02:10:37.541000",
          "2016-12-27 02:51:05.955000",
          "2016-12-27 03:02:21.454000",
          "2016-12-27 03:13:36.603000",
          "2016-12-27 03:41:52.699000",
          "2016-12-27 04:21:52.996000",
          "2016-12-27 05:22:46.005000",
          "2016-12-27 05:31:44.983000",
          "2016-12-27 05:34:07.339000",
          "2016-12-27 05:34:19.172000",
          "2016-12-27 05:36:39.768000",
          "2016-12-27 05:57:18.494000",
          "2016-12-27 06:20:14.361000",
          "2016-12-27 06:45:38.565000",
          "2016-12-27 07:32:05.729000",
          "2016-12-27 07:45:17.220000",
          "2016-12-27 07:50:35.605000",
          "2016-12-27 07:52:17.448000",
          "2016-12-27 08:09:50.698000",
          "2016-12-27 08:15:06.870000",
          "2016-12-27 08:17:00.248000",
          "2016-12-27 08:40:03.420000",
          "2016-12-27 08:58:18.968000",
          "2016-12-27 08:59:57.428000",
          "2016-12-27 09:11:57.488000",
          "2016-12-27 09:17:50.462000",
          "2016-12-27 09:19:52.271000",
          "2016-12-27 09:22:50.150000",
          "2016-12-27 09:24:04.851000",
          "2016-12-27 09:31:01.492000",
          "2016-12-27 09:41:00.220000",
          "2016-12-27 10:01:00.810000",
          "2016-12-27 10:16:11.886000",
          "2016-12-27 10:17:12.984000",
          "2016-12-27 10:30:09.817000",
          "2016-12-27 10:31:31.191000",
          "2016-12-27 11:10:39.581000",
          "2016-12-27 11:17:56.221000",
          "2016-12-27 11:36:32.030000",
          "2016-12-27 11:37:20.562000",
          "2016-12-27 11:41:06.393000",
          "2016-12-27 11:53:18.178000",
          "2016-12-27 12:15:29.080000",
          "2016-12-27 12:24:01.005000",
          "2016-12-27 12:38:36.671000",
          "2016-12-27 12:44:37.706000",
          "2016-12-27 12:49:48.705000",
          "2016-12-27 12:49:49.043000",
          "2016-12-27 13:00:07.343000",
          "2016-12-27 13:06:20.535000",
          "2016-12-27 13:36:19.857000",
          "2016-12-27 13:39:57.766000",
          "2016-12-27 13:45:27.238000",
          "2016-12-27 14:34:20.029000",
          "2016-12-27 14:36:23.842000",
          "2016-12-27 14:44:34.554000",
          "2016-12-27 14:50:40.296000",
          "2016-12-27 15:00:45.144000",
          "2016-12-27 15:01:01.667000",
          "2016-12-27 15:02:16.286000",
          "2016-12-27 15:13:18.097000",
          "2016-12-27 15:19:34.577000",
          "2016-12-27 15:28:47.733000",
          "2016-12-27 15:29:15.708000",
          "2016-12-27 15:44:58.981000",
          "2016-12-27 15:57:51",
          "2016-12-27 15:58:16.265000",
          "2016-12-27 16:00:36.265000",
          "2016-12-27 16:03:16.628000",
          "2016-12-27 16:05:03.228000",
          "2016-12-27 16:11:01.549000",
          "2016-12-27 16:13:38.863000",
          "2016-12-27 16:18:41.925000",
          "2016-12-27 16:22:59.453000",
          "2016-12-27 16:26:14.625000",
          "2016-12-27 16:32:39.773000",
          "2016-12-27 16:32:54.447000",
          "2016-12-27 16:51:06.965000",
          "2016-12-27 17:06:01.247000",
          "2016-12-27 17:13:11.955000",
          "2016-12-27 17:17:46.117000",
          "2016-12-27 17:21:35.418000",
          "2016-12-27 17:26:57.681000",
          "2016-12-27 17:31:29.663000",
          "2016-12-27 17:33:31.823000",
          "2016-12-27 17:39:55.090000",
          "2016-12-27 17:49:30.095000",
          "2016-12-27 17:50:03.615000",
          "2016-12-27 17:57:26.524000",
          "2016-12-27 18:00:32.782000",
          "2016-12-27 18:21:21.221000",
          "2016-12-27 18:24:39.637000",
          "2016-12-27 18:26:11.219000",
          "2016-12-27 18:57:55.257000",
          "2016-12-27 19:01:01.933000",
          "2016-12-27 19:06:01.367000",
          "2016-12-27 19:09:17.345000",
          "2016-12-27 19:16:38.349000",
          "2016-12-27 19:27:03.422000",
          "2016-12-27 19:45:18.662000",
          "2016-12-27 20:02:02.480000",
          "2016-12-27 20:06:09.511000",
          "2016-12-27 20:14:24.124000",
          "2016-12-27 20:18:20.553000",
          "2016-12-27 20:19:46.777000",
          "2016-12-27 20:34:33.344000",
          "2016-12-27 21:11:39.893000",
          "2016-12-27 21:13:19.574000",
          "2016-12-27 21:28:09.806000",
          "2016-12-27 21:50:04.874000",
          "2016-12-27 21:50:09.009000",
          "2016-12-27 21:56:47.213000",
          "2016-12-27 21:57:50.674000",
          "2016-12-27 22:06:10.192000",
          "2016-12-27 22:11:47.947000",
          "2016-12-27 22:15:27.411000",
          "2016-12-27 22:20:19.100000",
          "2016-12-27 22:35:43.612000",
          "2016-12-27 22:35:46.495000",
          "2016-12-27 22:43:36.707000",
          "2016-12-27 23:01:01.395000",
          "2016-12-27 23:07:05.130000",
          "2016-12-27 23:09:28.107000",
          "2016-12-27 23:09:59.658000",
          "2016-12-27 23:10:32.779000",
          "2016-12-27 23:19:26.314000",
          "2016-12-27 23:32:13.220000",
          "2016-12-27 23:51:47.548000",
          "2016-12-28 00:13:24.219000",
          "2016-12-28 00:29:32.608000",
          "2016-12-28 00:37:13.109000",
          "2016-12-28 00:41:56.860000",
          "2016-12-28 01:01:01.660000",
          "2016-12-28 01:23:59.111000",
          "2016-12-28 01:27:03.530000",
          "2016-12-28 01:29:03.860000",
          "2016-12-28 01:39:43.215000",
          "2016-12-28 01:48:38.683000",
          "2016-12-28 01:51:00",
          "2016-12-28 01:55:14.359000",
          "2016-12-28 02:23:48.555000",
          "2016-12-28 02:56:07.156000",
          "2016-12-28 03:01:01.669000",
          "2016-12-28 03:26:04.344000",
          "2016-12-28 03:29:18.178000",
          "2016-12-28 03:37:43.371000",
          "2016-12-28 03:44:00.822000",
          "2016-12-28 03:44:25.645000",
          "2016-12-28 03:49:58.404000",
          "2016-12-28 03:53:06.033000",
          "2016-12-28 04:19:10.608000",
          "2016-12-28 04:21:08.021000",
          "2016-12-28 04:33:59.286000",
          "2016-12-28 04:46:06.926000",
          "2016-12-28 04:57:24.595000",
          "2016-12-28 05:15:34.533000",
          "2016-12-28 05:34:45.101000",
          "2016-12-28 05:40:29.287000",
          "2016-12-28 05:50:47.216000",
          "2016-12-28 05:51:15.388000",
          "2016-12-28 06:05:16.719000",
          "2016-12-28 06:06:34.389000",
          "2016-12-28 06:21:01.645000",
          "2016-12-28 06:29:20.191000",
          "2016-12-28 06:33:14",
          "2016-12-28 06:41:05.636000",
          "2016-12-28 06:41:36.878000",
          "2016-12-28 07:00:44.698000",
          "2016-12-28 07:04:57.531000",
          "2016-12-28 07:12:01.850000",
          "2016-12-28 07:31:43.567000",
          "2016-12-28 07:33:53.575000",
          "2016-12-28 07:35:43.120000",
          "2016-12-28 07:52:18.878000",
          "2016-12-28 08:03:39.732000",
          "2016-12-28 08:17:18.171000",
          "2016-12-28 08:20:21.759000",
          "2016-12-28 08:25:42.916000",
          "2016-12-28 08:34:46.026000",
          "2016-12-28 08:38:54.337000",
          "2016-12-28 08:44:01.827000",
          "2016-12-28 09:04:56.095000",
          "2016-12-28 09:31:15.812000",
          "2016-12-28 09:35:36.281000",
          "2016-12-28 09:57:24.762000",
          "2016-12-28 10:06:16.031000",
          "2016-12-28 10:10:33.506000",
          "2016-12-28 10:12:46.334000",
          "2016-12-28 10:16:01.149000",
          "2016-12-28 10:39:04.865000",
          "2016-12-28 11:08:32.163000",
          "2016-12-28 11:17:05.106000",
          "2016-12-28 11:25:44.148000",
          "2016-12-28 11:36:59.576000",
          "2016-12-28 11:38:55.244000",
          "2016-12-28 11:43:02.230000",
          "2016-12-28 12:04:37.801000",
          "2016-12-28 12:04:49.895000",
          "2016-12-28 12:29:11.151000",
          "2016-12-28 12:29:35.442000",
          "2016-12-28 12:34:09.930000",
          "2016-12-28 12:46:07.547000",
          "2016-12-28 12:50:07.671000",
          "2016-12-28 12:51:01.572000",
          "2016-12-28 13:07:31.291000",
          "2016-12-28 13:22:06.465000",
          "2016-12-28 13:23:05.109000",
          "2016-12-28 13:27:48.878000",
          "2016-12-28 13:30:26.480000",
          "2016-12-28 13:32:24.273000",
          "2016-12-28 13:46:23.063000",
          "2016-12-28 13:52:34.395000",
          "2016-12-28 14:05:22.217000",
          "2016-12-28 14:08:48.218000",
          "2016-12-28 14:10:17.193000",
          "2016-12-28 14:14:43.629000",
          "2016-12-28 14:16:42.628000",
          "2016-12-28 14:22:08.088000",
          "2016-12-28 14:28:57.481000",
          "2016-12-28 14:32:49.656000",
          "2016-12-28 14:41:18.505000",
          "2016-12-28 14:47:42.050000",
          "2016-12-28 15:24:51.100000",
          "2016-12-28 15:26:10.273000",
          "2016-12-28 15:33:42.176000",
          "2016-12-28 15:39:20.180000",
          "2016-12-28 15:41:18.227000",
          "2016-12-28 15:43:51.482000",
          "2016-12-28 15:48:35.941000",
          "2016-12-28 15:49:18.688000",
          "2016-12-28 15:51:39.588000",
          "2016-12-28 15:52:46.486000",
          "2016-12-28 16:04:08.196000",
          "2016-12-28 16:07:36.760000",
          "2016-12-28 16:13:41.686000",
          "2016-12-28 16:22:29.472000",
          "2016-12-28 16:28:15.579000",
          "2016-12-28 16:43:07.160000",
          "2016-12-28 17:09:15.707000",
          "2016-12-28 17:10:14.131000",
          "2016-12-28 17:33:07.680000",
          "2016-12-28 17:34:39.247000",
          "2016-12-28 17:35:17.852000",
          "2016-12-28 17:41:41.184000",
          "2016-12-28 18:04:39.292000",
          "2016-12-28 18:05:39.765000",
          "2016-12-28 18:16:41.424000",
          "2016-12-28 18:23:05.465000",
          "2016-12-28 18:30:43.179000",
          "2016-12-28 18:31:04.625000",
          "2016-12-28 18:35:13.076000",
          "2016-12-28 18:44:14.579000",
          "2016-12-28 18:47:00.188000",
          "2016-12-28 19:01:01.476000",
          "2016-12-28 19:06:46.216000",
          "2016-12-28 19:10:19.755000",
          "2016-12-28 19:12:58.363000",
          "2016-12-28 19:19:50.705000",
          "2016-12-28 19:32:36.585000",
          "2016-12-28 19:33:57.863000",
          "2016-12-28 19:41:21.141000",
          "2016-12-28 19:59:18.840000",
          "2016-12-28 20:07:53.242000",
          "2016-12-28 20:09:52.559000",
          "2016-12-28 20:15:21.649000",
          "2016-12-28 20:25:57.082000",
          "2016-12-28 20:26:35.300000",
          "2016-12-28 20:42:07.314000",
          "2016-12-28 20:50:46.968000",
          "2016-12-28 21:07:16.547000",
          "2016-12-28 21:08:50.776000",
          "2016-12-28 21:27:25",
          "2016-12-28 21:28:40.897000",
          "2016-12-28 21:33:50.344000",
          "2016-12-28 21:36:51.324000",
          "2016-12-28 21:40:17.142000",
          "2016-12-28 21:44:56",
          "2016-12-28 21:55:39.135000",
          "2016-12-28 22:00:50.630000",
          "2016-12-28 22:05:17.323000",
          "2016-12-28 22:12:07.710000",
          "2016-12-28 22:15:27.638000",
          "2016-12-28 22:16:13.779000",
          "2016-12-28 22:21:37.827000",
          "2016-12-28 22:34:25.333000",
          "2016-12-28 22:45:18.090000",
          "2016-12-28 22:46:05.596000",
          "2016-12-28 22:51:01.400000",
          "2016-12-28 22:54:45.823000",
          "2016-12-28 22:55:33.931000",
          "2016-12-28 23:03:21.456000",
          "2016-12-28 23:32:41.720000",
          "2016-12-28 23:41:42.869000",
          "2016-12-28 23:59:46.258000",
          "2016-12-29",
          "2016-12-29 00:00:31.579000",
          "2016-12-29 00:34:18.429000",
          "2016-12-29 00:36:56.428000",
          "2016-12-29 00:49:18.921000",
          "2016-12-29 01:07:44.100000",
          "2016-12-29 01:42:15.520000",
          "2016-12-29 02:01:01.460000",
          "2016-12-29 02:44:21.762000",
          "2016-12-29 03:08:41.077000",
          "2016-12-29 03:12:27.413000",
          "2016-12-29 03:21:48.894000",
          "2016-12-29 03:30:42.819000",
          "2016-12-29 03:46:13.334000",
          "2016-12-29 04:02:06.870000",
          "2016-12-29 04:07:09.509000",
          "2016-12-29 04:21:51.261000",
          "2016-12-29 04:27:15.836000",
          "2016-12-29 04:30:22.597000",
          "2016-12-29 04:31:25.017000",
          "2016-12-29 04:34:51.464000",
          "2016-12-29 04:58:48.920000",
          "2016-12-29 05:07:10",
          "2016-12-29 05:23:50.483000",
          "2016-12-29 05:25:28.416000",
          "2016-12-29 06:02:15.988000",
          "2016-12-29 06:04:47.114000",
          "2016-12-29 06:22:13.709000",
          "2016-12-29 07:14:38.454000",
          "2016-12-29 07:16:39.292000",
          "2016-12-29 07:29:05.287000",
          "2016-12-29 07:29:20.709000",
          "2016-12-29 09:11:19.132000",
          "2016-12-29 09:18:29.526000",
          "2016-12-29 09:29:41.795000",
          "2016-12-29 09:55:33.418000",
          "2016-12-29 10:14:43.116000",
          "2016-12-29 10:25:58.814000",
          "2016-12-29 10:36:27.125000",
          "2016-12-29 10:45:18.841000",
          "2016-12-29 11:02:36.465000",
          "2016-12-29 11:04:47.104000",
          "2016-12-29 11:12:18.160000",
          "2016-12-29 11:14:40.721000",
          "2016-12-29 11:43:39.203000",
          "2016-12-29 11:47:50.820000",
          "2016-12-29 11:52:19.828000",
          "2016-12-29 12:05:26.038000",
          "2016-12-29 12:08:10.174000",
          "2016-12-29 12:25:58.606000",
          "2016-12-29 12:31:00",
          "2016-12-29 12:36:49.894000",
          "2016-12-29 12:54:24.328000",
          "2016-12-29 13:00:00",
          "2016-12-29 13:19:07.957000",
          "2016-12-29 13:23:10",
          "2016-12-29 13:25:47.217000",
          "2016-12-29 13:47:27.267000",
          "2016-12-29 13:47:45.003000",
          "2016-12-29 14:01:01.710000",
          "2016-12-29 14:04:22.212000",
          "2016-12-29 14:08:58.140000",
          "2016-12-29 14:11:42.766000",
          "2016-12-29 14:26:01.423000",
          "2016-12-29 14:31:07.959000",
          "2016-12-29 14:31:55.394000",
          "2016-12-29 14:35:06.301000",
          "2016-12-29 14:37:12.173000",
          "2016-12-29 14:46:47.128000",
          "2016-12-29 14:48:41.731000",
          "2016-12-29 14:55:43.134000",
          "2016-12-29 15:00:10",
          "2016-12-29 15:00:28.101000",
          "2016-12-29 15:01:01.595000",
          "2016-12-29 15:01:01.605000",
          "2016-12-29 15:01:40.762000",
          "2016-12-29 15:05:39.080000",
          "2016-12-29 15:08:42.843000",
          "2016-12-29 15:16:40.770000",
          "2016-12-29 15:35:50.221000",
          "2016-12-29 15:42:53.174000",
          "2016-12-29 15:45:07.824000",
          "2016-12-29 16:06:01.985000",
          "2016-12-29 16:15:59.635000",
          "2016-12-29 16:23:25.839000",
          "2016-12-29 16:25:05",
          "2016-12-29 16:30:43.257000",
          "2016-12-29 16:33:07.765000",
          "2016-12-29 16:34:30.986000",
          "2016-12-29 16:44:40.105000",
          "2016-12-29 16:47:44.744000",
          "2016-12-29 16:48:18.333000",
          "2016-12-29 16:49:02.612000",
          "2016-12-29 16:52:03.192000",
          "2016-12-29 16:56:20.935000",
          "2016-12-29 17:01:01.377000",
          "2016-12-29 17:18:16.308000",
          "2016-12-29 17:23:32.461000",
          "2016-12-29 17:25:00.830000",
          "2016-12-29 17:34:31.438000",
          "2016-12-29 17:40:59.606000",
          "2016-12-29 17:41:01.496000",
          "2016-12-29 17:46:54.338000",
          "2016-12-29 17:58:54.092000",
          "2016-12-29 18:08:04.788000",
          "2016-12-29 18:10:08.172000",
          "2016-12-29 18:12:15.791000",
          "2016-12-29 18:15:04.697000",
          "2016-12-29 18:16:06.138000",
          "2016-12-29 18:16:24.860000",
          "2016-12-29 18:30:00.471000",
          "2016-12-29 18:32:13.788000",
          "2016-12-29 18:36:34.542000",
          "2016-12-29 18:51:15.579000",
          "2016-12-29 18:55:52.317000",
          "2016-12-29 19:02:32.474000",
          "2016-12-29 19:02:43.051000",
          "2016-12-29 19:10:18.184000",
          "2016-12-29 19:11:33.446000",
          "2016-12-29 19:27:36.343000",
          "2016-12-29 19:42:01.659000",
          "2016-12-29 19:42:33.005000",
          "2016-12-29 19:46:43.945000",
          "2016-12-29 19:48:27.965000",
          "2016-12-29 19:50:34.237000",
          "2016-12-29 19:53:29.281000",
          "2016-12-29 19:55:27.819000",
          "2016-12-29 19:58:36.809000",
          "2016-12-29 20:21:26.149000",
          "2016-12-29 20:29:26.506000",
          "2016-12-29 20:40:27.834000",
          "2016-12-29 20:46:18.379000",
          "2016-12-29 20:46:25.053000",
          "2016-12-29 20:56:50.666000",
          "2016-12-29 21:23:00.562000",
          "2016-12-29 21:24:36.111000",
          "2016-12-29 21:27:42.537000",
          "2016-12-29 21:37:30.820000",
          "2016-12-29 22:02:28.956000",
          "2016-12-29 22:26:02.330000",
          "2016-12-29 22:38:59.486000",
          "2016-12-29 22:49:26.276000",
          "2016-12-29 22:55:20.911000",
          "2016-12-29 23:09:28.832000",
          "2016-12-29 23:13:54.502000",
          "2016-12-29 23:28:26.912000",
          "2016-12-29 23:33:04.678000",
          "2016-12-29 23:35:54.027000",
          "2016-12-29 23:43:52.232000",
          "2016-12-30",
          "2016-12-30 00:07:45.976000",
          "2016-12-30 00:15:44.080000",
          "2016-12-30 00:35:40.782000",
          "2016-12-30 00:36:03.961000",
          "2016-12-30 00:51:43.870000",
          "2016-12-30 01:04:17.799000",
          "2016-12-30 01:16:29.549000",
          "2016-12-30 01:16:34.417000",
          "2016-12-30 01:25:48.749000",
          "2016-12-30 01:48:48.459000",
          "2016-12-30 02:03:48.380000",
          "2016-12-30 02:20:54.168000",
          "2016-12-30 02:35:12.742000",
          "2016-12-30 02:35:21.907000",
          "2016-12-30 02:35:23.380000",
          "2016-12-30 02:41:40.048000",
          "2016-12-30 03:12:26.662000",
          "2016-12-30 04:59:09.866000",
          "2016-12-30 05:08:49.714000",
          "2016-12-30 05:09:44.937000",
          "2016-12-30 05:17:48.548000",
          "2016-12-30 05:38:03.834000",
          "2016-12-30 05:39:45.764000",
          "2016-12-30 05:55:24.223000",
          "2016-12-30 06:05:55.939000",
          "2016-12-30 06:09:35.775000",
          "2016-12-30 06:23:00.922000",
          "2016-12-30 06:41:22.485000",
          "2016-12-30 06:51:29",
          "2016-12-30 06:58:55.588000",
          "2016-12-30 07:17:55.365000",
          "2016-12-30 07:19:58.934000",
          "2016-12-30 07:27:23.653000",
          "2016-12-30 07:34:27.879000",
          "2016-12-30 07:48:41.792000",
          "2016-12-30 08:06:27.929000",
          "2016-12-30 08:16:24.143000",
          "2016-12-30 08:25:29.348000",
          "2016-12-30 09:16:03.929000",
          "2016-12-30 09:34:38.852000",
          "2016-12-30 09:36:02.740000",
          "2016-12-30 09:47:33.987000",
          "2016-12-30 09:56:07",
          "2016-12-30 09:57:30.992000",
          "2016-12-30 10:07:12.573000",
          "2016-12-30 10:41:28.397000",
          "2016-12-30 11:06:08.895000",
          "2016-12-30 11:22:45.655000",
          "2016-12-30 11:26:51.213000",
          "2016-12-30 11:35:48.575000",
          "2016-12-30 11:39:03.484000",
          "2016-12-30 11:49:03.072000",
          "2016-12-30 11:52:03.382000",
          "2016-12-30 12:01:35.448000",
          "2016-12-30 12:44:18.534000",
          "2016-12-30 12:46:23",
          "2016-12-30 12:59:20.144000",
          "2016-12-30 13:07:27.965000",
          "2016-12-30 13:10:36.360000",
          "2016-12-30 13:13:44.283000",
          "2016-12-30 13:18:30.169000",
          "2016-12-30 13:18:48.104000",
          "2016-12-30 13:26:10.240000",
          "2016-12-30 13:27:43.890000",
          "2016-12-30 13:29:07.907000",
          "2016-12-30 13:29:45.175000",
          "2016-12-30 13:31:01.619000",
          "2016-12-30 13:31:02.351000",
          "2016-12-30 13:44:40.740000",
          "2016-12-30 13:53:00.132000",
          "2016-12-30 13:59:40.624000",
          "2016-12-30 14:00:35.175000",
          "2016-12-30 14:31:01.720000",
          "2016-12-30 14:35:53.690000",
          "2016-12-30 14:36:20.589000",
          "2016-12-30 14:52:59.871000",
          "2016-12-30 15:01:02.380000",
          "2016-12-30 15:05:07.509000",
          "2016-12-30 15:14:34.823000",
          "2016-12-30 15:17:28.727000",
          "2016-12-30 15:25:42.789000",
          "2016-12-30 15:34:45.005000",
          "2016-12-30 15:45:42.006000",
          "2016-12-30 15:48:04.684000",
          "2016-12-30 15:52:38.360000",
          "2016-12-30 16:07:04.628000",
          "2016-12-30 16:07:37.269000",
          "2016-12-30 16:12:07.412000",
          "2016-12-30 16:15:04.657000",
          "2016-12-30 16:18:43.101000",
          "2016-12-30 16:36:52.008000",
          "2016-12-30 16:38:11.413000",
          "2016-12-30 16:39:15.346000",
          "2016-12-30 17:05:45.721000",
          "2016-12-30 17:10:38.888000",
          "2016-12-30 17:21:30.774000",
          "2016-12-30 17:22:30.625000",
          "2016-12-30 17:30:11.934000",
          "2016-12-30 17:43:42",
          "2016-12-30 17:44:00.028000",
          "2016-12-30 17:50:00.505000",
          "2016-12-30 17:51:57.233000",
          "2016-12-30 17:54:32.217000",
          "2016-12-30 18:02:16.846000",
          "2016-12-30 18:04:09.465000",
          "2016-12-30 18:11:24.969000",
          "2016-12-30 18:14:17.342000",
          "2016-12-30 18:22:02.476000",
          "2016-12-30 18:30:19.634000",
          "2016-12-30 18:33:18.083000",
          "2016-12-30 18:36:47.455000",
          "2016-12-30 18:36:54.008000",
          "2016-12-30 18:39:34.862000",
          "2016-12-30 18:44:16.084000",
          "2016-12-30 18:54:40.834000",
          "2016-12-30 19:11:17.762000",
          "2016-12-30 19:15:38.041000",
          "2016-12-30 19:24:48.902000",
          "2016-12-30 19:24:51.518000",
          "2016-12-30 19:25:54.446000",
          "2016-12-30 19:25:59.206000",
          "2016-12-30 19:31:01.315000",
          "2016-12-30 19:39:21.884000",
          "2016-12-30 20:29:25.392000",
          "2016-12-30 20:38:07.759000",
          "2016-12-30 20:39:07.243000",
          "2016-12-30 20:49:04.016000",
          "2016-12-30 20:53:25.981000",
          "2016-12-30 21:02:52.260000",
          "2016-12-30 21:10:19.869000",
          "2016-12-30 21:10:22.047000",
          "2016-12-30 21:35:46.214000",
          "2016-12-30 21:43:44.431000",
          "2016-12-30 21:48:51.285000",
          "2016-12-30 22:08:21.619000",
          "2016-12-30 22:21:27.664000",
          "2016-12-30 22:46:52.845000",
          "2016-12-30 22:53:06.466000",
          "2016-12-30 22:53:11.166000",
          "2016-12-30 23:02:42",
          "2016-12-30 23:07:33.314000",
          "2016-12-30 23:18:24.581000",
          "2016-12-30 23:18:53.055000",
          "2016-12-30 23:30:36.206000",
          "2016-12-30 23:30:44.951000",
          "2016-12-30 23:32:03.275000",
          "2016-12-30 23:37:44.312000",
          "2016-12-31",
          "2016-12-31 00:16:00.449000",
          "2016-12-31 00:59:35.994000",
          "2016-12-31 01:05:49.844000",
          "2016-12-31 01:15:11.165000",
          "2016-12-31 01:15:21.110000",
          "2016-12-31 01:20:40.484000",
          "2016-12-31 01:27:38.427000",
          "2016-12-31 01:34:37.391000",
          "2016-12-31 01:36:10.738000",
          "2016-12-31 01:41:38.406000",
          "2016-12-31 01:44:31.551000",
          "2016-12-31 01:52:06.917000",
          "2016-12-31 02:14:45.216000",
          "2016-12-31 02:25:15.734000",
          "2016-12-31 02:25:17.804000",
          "2016-12-31 02:26:01.645000",
          "2016-12-31 02:28:09.446000",
          "2016-12-31 02:48:26.366000",
          "2016-12-31 03:15:39.532000",
          "2016-12-31 03:23:56.347000",
          "2016-12-31 03:39:29.385000",
          "2016-12-31 03:57:47.827000",
          "2016-12-31 04:20:22.594000",
          "2016-12-31 05:23:50.521000",
          "2016-12-31 05:32:25.928000",
          "2016-12-31 06:02:46.346000",
          "2016-12-31 06:07:31.666000",
          "2016-12-31 06:30:36.581000",
          "2016-12-31 07:01:54.837000",
          "2016-12-31 07:27:03.009000",
          "2016-12-31 07:33:50.782000",
          "2016-12-31 08:33:14.663000",
          "2016-12-31 08:35:54.878000",
          "2016-12-31 08:42:20.755000",
          "2016-12-31 08:49:20.096000",
          "2016-12-31 09:01:12.394000",
          "2016-12-31 09:29:22",
          "2016-12-31 09:30:38.655000",
          "2016-12-31 09:50:30.018000",
          "2016-12-31 09:52:56.434000",
          "2016-12-31 10:16:48",
          "2016-12-31 10:31:44.891000",
          "2016-12-31 10:38:03.139000",
          "2016-12-31 10:40:25.801000",
          "2016-12-31 10:59:12.842000",
          "2016-12-31 11:11:01.612000",
          "2016-12-31 11:20:38.254000",
          "2016-12-31 12:10:56.702000",
          "2016-12-31 12:29:09.298000",
          "2016-12-31 12:31:30.436000",
          "2016-12-31 12:34:52.132000",
          "2016-12-31 12:48:56.159000",
          "2016-12-31 12:52:50.350000",
          "2016-12-31 13:00:13.725000",
          "2016-12-31 13:07:17.982000",
          "2016-12-31 13:26:01.826000",
          "2016-12-31 13:27:08.788000",
          "2016-12-31 13:31:37.069000",
          "2016-12-31 13:45:09.009000",
          "2016-12-31 13:47:12.077000",
          "2016-12-31 14:12:43.784000",
          "2016-12-31 14:19:05.910000",
          "2016-12-31 14:34:08.651000",
          "2016-12-31 14:36:15.281000",
          "2016-12-31 15:04:48.099000",
          "2016-12-31 15:08:51.718000",
          "2016-12-31 15:12:49.852000",
          "2016-12-31 15:29:50.145000",
          "2016-12-31 15:40:58.070000",
          "2016-12-31 15:44:31.622000",
          "2016-12-31 15:49:21.283000",
          "2016-12-31 15:59:22.555000",
          "2016-12-31 15:59:43",
          "2016-12-31 16:00:24.193000",
          "2016-12-31 16:14:31.495000",
          "2016-12-31 16:18:13.639000",
          "2016-12-31 16:19:55.030000",
          "2016-12-31 16:21:55.452000",
          "2016-12-31 16:27:07.111000",
          "2016-12-31 16:50:26.789000",
          "2016-12-31 16:57:57.153000",
          "2016-12-31 17:01:21.594000",
          "2016-12-31 17:10:49.625000",
          "2016-12-31 17:27:59.556000",
          "2016-12-31 17:48:58.509000",
          "2016-12-31 17:52:56.198000",
          "2016-12-31 17:59:33.607000",
          "2016-12-31 17:59:45.181000",
          "2016-12-31 18:05:38.646000",
          "2016-12-31 18:15:51.817000",
          "2016-12-31 18:23:46.585000",
          "2016-12-31 18:24:54.730000",
          "2016-12-31 18:44:46.446000",
          "2016-12-31 18:52:32",
          "2016-12-31 18:53:04.218000",
          "2016-12-31 19:01:25.706000",
          "2016-12-31 19:08:29.050000",
          "2016-12-31 19:20:57.095000",
          "2016-12-31 19:26:03.079000",
          "2016-12-31 19:53:45.481000",
          "2016-12-31 19:57:21.154000",
          "2016-12-31 20:01:01.703000",
          "2016-12-31 20:01:18.371000",
          "2016-12-31 20:17:40.369000",
          "2016-12-31 20:21:36.634000",
          "2016-12-31 20:22:43.406000",
          "2016-12-31 20:35:19.450000",
          "2016-12-31 20:36:25.249000",
          "2016-12-31 20:41:38.396000",
          "2016-12-31 20:52:00.717000",
          "2016-12-31 21:03:49.339000",
          "2016-12-31 21:11:16.192000",
          "2016-12-31 21:33:25.495000",
          "2016-12-31 21:34:27.564000",
          "2016-12-31 21:51:01.507000",
          "2016-12-31 21:58:43.265000",
          "2016-12-31 22:15:22.537000",
          "2016-12-31 22:17:45.433000",
          "2016-12-31 22:25:12.360000",
          "2016-12-31 23:04:27.063000",
          "2016-12-31 23:13:08.790000",
          "2016-12-31 23:26:55.147000",
          "2016-12-31 23:51:28.020000"
         ],
         "y": [
          1.09861,
          0.69315,
          1.09861,
          5.5134300000000005,
          0.69315,
          1.38629,
          5.710430000000001,
          1.9459099999999998,
          2.07944,
          3.55535,
          0.69315,
          1.09861,
          2.70805,
          1.60944,
          0.69315,
          3.21888,
          1.60944,
          0.69315,
          0.69315,
          0.69315,
          2.56495,
          2.77259,
          1.09861,
          3.09104,
          0.69315,
          1.7917599999999998,
          1.9459099999999998,
          3.2581,
          0.69315,
          4.07754,
          0.69315,
          2.6390599999999997,
          1.7917599999999998,
          1.60944,
          1.60944,
          0.69315,
          1.38629,
          2.83321,
          6.41182,
          1.60944,
          1.60944,
          0.69315,
          0.69315,
          2.3979,
          1.60944,
          3.17805,
          0.69315,
          6.807930000000001,
          3.78419,
          2.6390599999999997,
          1.38629,
          1.7917599999999998,
          1.7917599999999998,
          6.13556,
          1.09861,
          1.7917599999999998,
          4.276669999999999,
          3.68888,
          1.38629,
          2.83321,
          5.8944,
          0.69315,
          2.3025900000000004,
          1.60944,
          1.9459099999999998,
          1.09861,
          1.38629,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          3.2581,
          2.07944,
          4.74493,
          0.69315,
          1.9459099999999998,
          0.69315,
          0.69315,
          1.09861,
          1.38629,
          0.69315,
          2.3979,
          2.70805,
          3.80666,
          0.69315,
          1.38629,
          1.38629,
          0.69315,
          0.69315,
          1.38629,
          0.69315,
          0.69315,
          1.09861,
          1.09861,
          1.38629,
          1.09861,
          3.6375900000000003,
          0.69315,
          0.69315,
          1.9459099999999998,
          1.09861,
          0.69315,
          0.69315,
          0.69315,
          1.09861,
          2.19722,
          2.3025900000000004,
          0.69315,
          1.60944,
          0.69315,
          2.07944,
          1.38629,
          1.7917599999999998,
          5.68698,
          1.38629,
          0.69315,
          3.4657400000000003,
          2.07944,
          6.566669999999999,
          6.3716099999999996,
          2.4849099999999997,
          5.92959,
          1.09861,
          2.3025900000000004,
          2.19722,
          0.69315,
          0.69315,
          0.69315,
          4.56435,
          0.69315,
          2.3979,
          1.9459099999999998,
          2.19722,
          1.9459099999999998,
          2.56495,
          0.69315,
          1.7917599999999998,
          4.06044,
          0.69315,
          1.7917599999999998,
          1.7917599999999998,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          3.7376699999999996,
          1.09861,
          1.09861,
          0.69315,
          1.7917599999999998,
          1.60944,
          2.3025900000000004,
          1.60944,
          3.6375900000000003,
          2.56495,
          3.6109199999999997,
          2.56495,
          3.4657400000000003,
          3.29584,
          1.09861,
          2.3025900000000004,
          0.69315,
          3.43399,
          1.38629,
          0.69315,
          3.2581,
          1.38629,
          2.07944,
          1.38629,
          1.09861,
          0.69315,
          1.38629,
          2.70805,
          2.19722,
          1.38629,
          1.09861,
          2.07944,
          0.69315,
          1.60944,
          1.9459099999999998,
          0.69315,
          2.99573,
          4.110869999999999,
          0.69315,
          0.69315,
          2.6390599999999997,
          2.83321,
          3.68888,
          1.38629,
          0.69315,
          1.09861,
          0.69315,
          1.60944,
          2.3979,
          1.09861,
          0.69315,
          0.69315,
          1.60944,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          2.07944,
          0.69315,
          2.19722,
          1.60944,
          3.09104,
          1.38629,
          2.3979,
          1.60944,
          10.85902,
          4.33073,
          1.60944,
          1.09861,
          1.38629,
          1.38629,
          2.77259,
          1.7917599999999998,
          7.170889999999999,
          3.2581,
          5.42053,
          1.09861,
          0.69315,
          2.07944,
          3.78419,
          1.9459099999999998,
          2.07944,
          0.69315,
          3.9512400000000003,
          1.38629,
          0.69315,
          4.35671,
          0.69315,
          5.0626,
          4.07754,
          0.69315,
          1.7917599999999998,
          0.69315,
          1.9459099999999998,
          1.60944,
          2.19722,
          2.6390599999999997,
          1.7917599999999998,
          1.7917599999999998,
          0.69315,
          1.09861,
          0.69315,
          5.09987,
          6.272880000000001,
          1.09861,
          3.3673,
          3.09104,
          1.09861,
          1.09861,
          0.69315,
          3.7376699999999996,
          0.69315,
          0.69315,
          0.69315,
          1.7917599999999998,
          3.93183,
          2.94444,
          2.19722,
          1.09861,
          0.69315,
          3.04452,
          3.21888,
          1.38629,
          0.69315,
          2.56495,
          6.013719999999999,
          0.69315,
          0.69315,
          1.09861,
          1.7917599999999998,
          0.69315,
          5.96101,
          0.69315,
          6.90875,
          1.60944,
          0.69315,
          1.38629,
          0.69315,
          1.38629,
          2.3979,
          0.69315,
          0.69315,
          0.69315,
          1.7917599999999998,
          2.3979,
          3.3322,
          2.3979,
          1.38629,
          0.69315,
          2.6390599999999997,
          2.70805,
          0.69315,
          1.09861,
          1.38629,
          0.69315,
          5.48064,
          3.09104,
          0.69315,
          3.3673,
          4.3438099999999995,
          1.60944,
          0.69315,
          5.28827,
          3.8918199999999996,
          0.69315,
          5.1299,
          0.69315,
          8.101980000000001,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          1.09861,
          2.83321,
          5.17048,
          1.9459099999999998,
          1.38629,
          1.38629,
          0.69315,
          3.21888,
          1.09861,
          2.56495,
          1.60944,
          1.9459099999999998,
          3.09104,
          2.83321,
          1.09861,
          2.3979,
          5.07517,
          2.6390599999999997,
          1.7917599999999998,
          6.2106,
          5.05625,
          0.69315,
          1.38629,
          6.90875,
          4.94876,
          6.570880000000001,
          1.09861,
          3.66356,
          3.17805,
          0.69315,
          0.69315,
          1.9459099999999998,
          1.38629,
          4.20469,
          1.60944,
          0.69315,
          1.09861,
          0.69315,
          3.09104,
          2.6390599999999997,
          1.09861,
          0.69315,
          4.69135,
          0.69315,
          0.69315,
          1.9459099999999998,
          1.38629,
          0.69315,
          1.09861,
          2.77259,
          7.00397,
          1.60944,
          1.60944,
          0.69315,
          1.38629,
          1.09861,
          3.55535,
          1.9459099999999998,
          3.3673,
          1.09861,
          2.07944,
          1.38629,
          0.69315,
          3.4657400000000003,
          1.09861,
          1.38629,
          2.07944,
          0.69315,
          2.77259,
          2.3025900000000004,
          3.09104,
          0.69315,
          2.4849099999999997,
          7.00397,
          1.7917599999999998,
          3.29584,
          0.69315,
          1.38629,
          1.9459099999999998,
          3.7376699999999996,
          0.69315,
          2.3025900000000004,
          0.69315,
          6.15698,
          1.38629,
          0.69315,
          1.09861,
          3.04452,
          5.73979,
          1.60944,
          1.09861,
          0.69315,
          1.7917599999999998,
          0.69315,
          2.94444,
          0.69315,
          5.24702,
          0.69315,
          1.60944,
          2.94444,
          1.9459099999999998,
          2.94444,
          3.6109199999999997,
          3.3322,
          3.17805,
          4.91265,
          1.38629,
          2.3025900000000004,
          4.7706800000000005,
          1.9459099999999998,
          1.60944,
          6.19236,
          1.38629,
          1.60944,
          5.51745,
          0.69315,
          0.69315,
          0.69315,
          1.7917599999999998,
          2.07944,
          1.38629,
          2.6390599999999997,
          1.38629,
          4.31749,
          5.627619999999999,
          0.69315,
          0.69315,
          0.69315,
          3.55535,
          1.09861,
          1.09861,
          1.60944,
          2.4849099999999997,
          0.69315,
          1.60944,
          0.69315,
          3.80666,
          1.09861,
          3.1354900000000003,
          1.7917599999999998,
          0.69315,
          1.60944,
          0.69315,
          0.69315,
          2.3979,
          1.38629,
          1.7917599999999998,
          1.09861,
          0.69315,
          0.69315,
          1.09861,
          1.09861,
          1.9459099999999998,
          2.07944,
          1.09861,
          3.66356,
          1.38629,
          1.09861,
          1.7917599999999998,
          0.69315,
          2.19722,
          1.09861,
          0.69315,
          0.69315,
          0.69315,
          4.23411,
          0.69315,
          1.9459099999999998,
          0.69315,
          1.09861,
          0.69315,
          1.7917599999999998,
          0.69315,
          0.69315,
          4.86753,
          1.60944,
          0.69315,
          0.69315,
          2.99573,
          4.8040199999999995,
          4.39445,
          3.7612,
          1.38629,
          0.69315,
          0.69315,
          6.03069,
          3.09104,
          0.69315,
          1.38629,
          0.69315,
          0.69315,
          4.43082,
          1.38629,
          7.37838,
          1.09861,
          3.17805,
          1.60944,
          4.23411,
          2.70805,
          1.7917599999999998,
          1.09861,
          2.4849099999999997,
          4.955830000000001,
          1.09861,
          0.69315,
          0.69315,
          0.69315,
          1.09861,
          2.3025900000000004,
          1.9459099999999998,
          1.38629,
          0.69315,
          1.38629,
          2.89037,
          2.4849099999999997,
          4.7362,
          3.21888,
          0.69315,
          1.09861,
          0.69315,
          1.7917599999999998,
          0.69315,
          4.96981,
          1.09861,
          2.3979,
          1.9459099999999998,
          0.69315,
          2.4849099999999997,
          3.21888,
          2.4849099999999997,
          2.6390599999999997,
          1.09861,
          2.70805,
          2.3979,
          1.38629,
          1.38629,
          2.89037,
          1.38629,
          1.38629,
          0.69315,
          1.60944,
          0.69315,
          4.47734,
          5.40268,
          5.50126,
          1.38629,
          5.58725,
          2.4849099999999997,
          1.09861,
          0.69315,
          0.69315,
          1.60944,
          1.38629,
          4.17439,
          1.38629,
          1.60944,
          1.38629,
          2.56495,
          1.38629,
          2.6390599999999997,
          1.38629,
          2.3025900000000004,
          0.69315,
          1.38629,
          0.69315,
          1.38629,
          3.04452,
          0.69315,
          4.110869999999999,
          0.69315,
          2.3025900000000004,
          1.09861,
          1.09861,
          2.07944,
          0.69315,
          0.69315,
          0.69315,
          5.33272,
          1.38629,
          0.69315,
          0.69315,
          1.38629,
          0.69315,
          0.69315,
          1.09861,
          2.77259,
          0.69315,
          0.69315,
          0.69315,
          1.60944,
          0.69315,
          1.38629,
          3.3673,
          4.0073300000000005,
          1.38629,
          1.60944,
          1.9459099999999998,
          0.69315,
          3.71357,
          1.7917599999999998,
          0.69315,
          1.09861,
          1.60944,
          0.69315,
          3.29584,
          4.41884,
          1.60944,
          1.7917599999999998,
          0.69315,
          3.71357,
          1.60944,
          1.09861,
          1.09861,
          0.69315,
          6.6695,
          1.09861,
          0.69315,
          1.38629,
          1.60944,
          1.38629,
          3.66356,
          0.69315,
          0.69315,
          1.7917599999999998,
          0.69315,
          3.8918199999999996,
          1.09861,
          0.69315,
          5.52146,
          1.09861,
          1.09861,
          2.3979,
          1.38629,
          1.38629,
          1.09861,
          6.4552,
          2.07944,
          2.19722,
          4.23411,
          1.09861,
          1.38629,
          1.9459099999999998,
          4.99043,
          0.69315,
          8.50127,
          0.69315,
          1.09861,
          1.09861,
          2.3979,
          1.38629,
          1.09861,
          1.7917599999999998,
          0.69315,
          0.69315,
          1.60944,
          4.07754,
          1.09861,
          2.70805,
          3.7376699999999996,
          3.98898,
          1.09861,
          2.56495,
          0.69315,
          0.69315,
          3.3673,
          0.69315,
          1.09861,
          2.83321,
          2.3025900000000004,
          0.69315,
          0.69315,
          4.57471,
          1.38629,
          0.69315,
          3.2581,
          1.38629,
          0.69315,
          0.69315,
          2.6390599999999997,
          0.69315,
          1.38629,
          1.60944,
          1.38629,
          0.69315,
          1.9459099999999998,
          1.9459099999999998,
          5.64545,
          1.60944,
          0.69315,
          2.94444,
          0.69315,
          0.69315,
          2.19722,
          0.69315,
          2.4849099999999997,
          1.38629,
          0.69315,
          1.09861,
          6.18002,
          1.7917599999999998,
          1.09861,
          1.09861,
          4.65396,
          6.90875,
          0.69315,
          5.0304400000000005,
          5.24175,
          0.69315,
          0.69315,
          1.60944,
          1.38629,
          2.19722,
          0.69315,
          0.69315,
          1.9459099999999998,
          1.38629,
          2.19722,
          3.4657400000000003,
          0.69315,
          1.09861,
          2.3979,
          1.09861,
          1.38629,
          2.07944,
          1.09861,
          1.38629,
          1.09861,
          2.83321,
          1.38629,
          7.972810000000001,
          4.41884,
          2.07944,
          0.69315,
          0.69315,
          6.43455,
          0.69315,
          3.80666,
          0.69315,
          0.69315,
          1.38629,
          1.09861,
          6.47851,
          1.60944,
          0.69315,
          0.69315,
          1.38629,
          2.89037,
          2.70805,
          1.7917599999999998,
          1.09861,
          3.3673,
          0.69315,
          0.69315,
          0.69315,
          5.17615,
          0.69315,
          0.69315,
          1.60944,
          1.7917599999999998,
          1.60944,
          0.69315,
          1.09861,
          1.09861,
          2.3979,
          1.60944,
          0.69315,
          1.38629,
          8.18897,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          0.69315,
          3.78419,
          0.69315,
          0.69315,
          5.710430000000001,
          4.8598099999999995,
          1.7917599999999998,
          1.9459099999999998,
          3.9512400000000003,
          3.4657400000000003,
          4.67283,
          3.82864,
          1.38629,
          1.09861,
          5.54518,
          2.19722,
          1.38629,
          0.69315,
          1.38629,
          1.09861,
          0.69315,
          1.38629,
          7.7411,
          0.69315,
          2.99573,
          1.60944,
          6.50429,
          1.9459099999999998,
          0.69315,
          0.69315,
          1.09861,
          2.3979,
          0.69315,
          2.07944,
          3.09104,
          1.09861,
          0.69315,
          1.09861,
          2.89037,
          0.69315,
          1.7917599999999998,
          1.38629,
          2.19722,
          0.69315,
          2.77259,
          2.19722,
          4.40672,
          2.83321,
          1.60944,
          4.304069999999999,
          3.09104,
          1.09861,
          3.52636,
          1.38629,
          0.69315,
          0.69315,
          0.69315,
          1.09861,
          3.43399,
          1.9459099999999998,
          1.60944,
          1.60944,
          2.3979,
          2.99573,
          1.60944,
          0.69315,
          4.304069999999999,
          4.20469,
          5.463830000000001,
          1.60944,
          2.6390599999999997,
          2.94444,
          3.9702900000000003,
          1.09861,
          1.9459099999999998,
          2.70805,
          6.10479,
          2.6390599999999997,
          3.49651,
          3.68888,
          1.09861,
          4.31749,
          0.69315,
          1.7917599999999998,
          1.09861,
          1.38629,
          1.7917599999999998,
          4.7706800000000005,
          0.69315,
          1.09861,
          1.09861,
          0.69315,
          0.69315,
          1.7917599999999998,
          2.19722,
          1.38629,
          0.69315,
          0.69315,
          3.55535,
          0.69315,
          4.49981,
          2.3025900000000004,
          1.9459099999999998,
          6.639880000000001,
          2.07944,
          1.09861,
          1.09861,
          1.38629,
          3.1354900000000003,
          5.0876,
          0.69315,
          1.7917599999999998,
          5.05625,
          1.7917599999999998,
          0.69315,
          2.56495,
          0.69315,
          1.38629,
          1.09861,
          2.3979,
          0.69315,
          1.7917599999999998,
          1.09861,
          2.56495,
          2.07944,
          2.89037,
          0.69315,
          3.04452,
          0.69315,
          1.09861,
          0.69315,
          2.4849099999999997,
          1.7917599999999998,
          1.38629,
          0.69315,
          2.94444,
          2.99573,
          0.69315,
          1.38629,
          0.69315,
          1.09861,
          3.21888,
          1.09861,
          4.75359,
          0.69315,
          0.69315,
          4.21951,
          2.19722,
          1.9459099999999998,
          3.78419,
          2.77259,
          0.69315,
          1.38629,
          2.56495,
          1.9459099999999998,
          5.53339,
          0.69315,
          5.77144,
          0.69315,
          1.38629,
          0.69315,
          0.69315,
          1.09861,
          0.69315,
          4.21951,
          2.07944,
          0.69315,
          2.19722,
          1.9459099999999998,
          1.9459099999999998,
          2.70805,
          0.69315,
          2.07944,
          2.77259,
          3.55535,
          1.60944,
          1.09861,
          0.69315,
          0.69315,
          1.09861,
          1.09861,
          0.69315,
          0.69315,
          1.38629,
          0.69315,
          1.60944,
          1.7917599999999998,
          2.07944,
          6.09131,
          2.3025900000000004,
          3.78419,
          2.07944,
          1.9459099999999998,
          3.66356,
          0.69315,
          1.60944,
          1.7917599999999998,
          0.69315,
          2.77259,
          1.60944,
          1.38629,
          1.38629,
          6.13773,
          1.60944,
          1.38629,
          0.69315,
          2.4849099999999997,
          1.09861,
          3.17805
         ]
        }
       ],
       "layout": {
        "title": "Online users"
       }
      },
      "text/html": [
       "<div id=\"9c181fba-e1aa-4791-8c2b-f60230acfbac\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9c181fba-e1aa-4791-8c2b-f60230acfbac\", [{\"y\": [1.09861, 0.69315, 1.09861, 5.5134300000000005, 0.69315, 1.38629, 5.710430000000001, 1.9459099999999998, 2.07944, 3.55535, 0.69315, 1.09861, 2.70805, 1.60944, 0.69315, 3.21888, 1.60944, 0.69315, 0.69315, 0.69315, 2.56495, 2.77259, 1.09861, 3.09104, 0.69315, 1.7917599999999998, 1.9459099999999998, 3.2581, 0.69315, 4.07754, 0.69315, 2.6390599999999997, 1.7917599999999998, 1.60944, 1.60944, 0.69315, 1.38629, 2.83321, 6.41182, 1.60944, 1.60944, 0.69315, 0.69315, 2.3979, 1.60944, 3.17805, 0.69315, 6.807930000000001, 3.78419, 2.6390599999999997, 1.38629, 1.7917599999999998, 1.7917599999999998, 6.13556, 1.09861, 1.7917599999999998, 4.276669999999999, 3.68888, 1.38629, 2.83321, 5.8944, 0.69315, 2.3025900000000004, 1.60944, 1.9459099999999998, 1.09861, 1.38629, 0.69315, 0.69315, 0.69315, 0.69315, 3.2581, 2.07944, 4.74493, 0.69315, 1.9459099999999998, 0.69315, 0.69315, 1.09861, 1.38629, 0.69315, 2.3979, 2.70805, 3.80666, 0.69315, 1.38629, 1.38629, 0.69315, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 1.09861, 1.38629, 1.09861, 3.6375900000000003, 0.69315, 0.69315, 1.9459099999999998, 1.09861, 0.69315, 0.69315, 0.69315, 1.09861, 2.19722, 2.3025900000000004, 0.69315, 1.60944, 0.69315, 2.07944, 1.38629, 1.7917599999999998, 5.68698, 1.38629, 0.69315, 3.4657400000000003, 2.07944, 6.566669999999999, 6.3716099999999996, 2.4849099999999997, 5.92959, 1.09861, 2.3025900000000004, 2.19722, 0.69315, 0.69315, 0.69315, 4.56435, 0.69315, 2.3979, 1.9459099999999998, 2.19722, 1.9459099999999998, 2.56495, 0.69315, 1.7917599999999998, 4.06044, 0.69315, 1.7917599999999998, 1.7917599999999998, 0.69315, 0.69315, 0.69315, 0.69315, 3.7376699999999996, 1.09861, 1.09861, 0.69315, 1.7917599999999998, 1.60944, 2.3025900000000004, 1.60944, 3.6375900000000003, 2.56495, 3.6109199999999997, 2.56495, 3.4657400000000003, 3.29584, 1.09861, 2.3025900000000004, 0.69315, 3.43399, 1.38629, 0.69315, 3.2581, 1.38629, 2.07944, 1.38629, 1.09861, 0.69315, 1.38629, 2.70805, 2.19722, 1.38629, 1.09861, 2.07944, 0.69315, 1.60944, 1.9459099999999998, 0.69315, 2.99573, 4.110869999999999, 0.69315, 0.69315, 2.6390599999999997, 2.83321, 3.68888, 1.38629, 0.69315, 1.09861, 0.69315, 1.60944, 2.3979, 1.09861, 0.69315, 0.69315, 1.60944, 0.69315, 0.69315, 0.69315, 0.69315, 2.07944, 0.69315, 2.19722, 1.60944, 3.09104, 1.38629, 2.3979, 1.60944, 10.85902, 4.33073, 1.60944, 1.09861, 1.38629, 1.38629, 2.77259, 1.7917599999999998, 7.170889999999999, 3.2581, 5.42053, 1.09861, 0.69315, 2.07944, 3.78419, 1.9459099999999998, 2.07944, 0.69315, 3.9512400000000003, 1.38629, 0.69315, 4.35671, 0.69315, 5.0626, 4.07754, 0.69315, 1.7917599999999998, 0.69315, 1.9459099999999998, 1.60944, 2.19722, 2.6390599999999997, 1.7917599999999998, 1.7917599999999998, 0.69315, 1.09861, 0.69315, 5.09987, 6.272880000000001, 1.09861, 3.3673, 3.09104, 1.09861, 1.09861, 0.69315, 3.7376699999999996, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 3.93183, 2.94444, 2.19722, 1.09861, 0.69315, 3.04452, 3.21888, 1.38629, 0.69315, 2.56495, 6.013719999999999, 0.69315, 0.69315, 1.09861, 1.7917599999999998, 0.69315, 5.96101, 0.69315, 6.90875, 1.60944, 0.69315, 1.38629, 0.69315, 1.38629, 2.3979, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 2.3979, 3.3322, 2.3979, 1.38629, 0.69315, 2.6390599999999997, 2.70805, 0.69315, 1.09861, 1.38629, 0.69315, 5.48064, 3.09104, 0.69315, 3.3673, 4.3438099999999995, 1.60944, 0.69315, 5.28827, 3.8918199999999996, 0.69315, 5.1299, 0.69315, 8.101980000000001, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 1.09861, 2.83321, 5.17048, 1.9459099999999998, 1.38629, 1.38629, 0.69315, 3.21888, 1.09861, 2.56495, 1.60944, 1.9459099999999998, 3.09104, 2.83321, 1.09861, 2.3979, 5.07517, 2.6390599999999997, 1.7917599999999998, 6.2106, 5.05625, 0.69315, 1.38629, 6.90875, 4.94876, 6.570880000000001, 1.09861, 3.66356, 3.17805, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 4.20469, 1.60944, 0.69315, 1.09861, 0.69315, 3.09104, 2.6390599999999997, 1.09861, 0.69315, 4.69135, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 0.69315, 1.09861, 2.77259, 7.00397, 1.60944, 1.60944, 0.69315, 1.38629, 1.09861, 3.55535, 1.9459099999999998, 3.3673, 1.09861, 2.07944, 1.38629, 0.69315, 3.4657400000000003, 1.09861, 1.38629, 2.07944, 0.69315, 2.77259, 2.3025900000000004, 3.09104, 0.69315, 2.4849099999999997, 7.00397, 1.7917599999999998, 3.29584, 0.69315, 1.38629, 1.9459099999999998, 3.7376699999999996, 0.69315, 2.3025900000000004, 0.69315, 6.15698, 1.38629, 0.69315, 1.09861, 3.04452, 5.73979, 1.60944, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 2.94444, 0.69315, 5.24702, 0.69315, 1.60944, 2.94444, 1.9459099999999998, 2.94444, 3.6109199999999997, 3.3322, 3.17805, 4.91265, 1.38629, 2.3025900000000004, 4.7706800000000005, 1.9459099999999998, 1.60944, 6.19236, 1.38629, 1.60944, 5.51745, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 2.07944, 1.38629, 2.6390599999999997, 1.38629, 4.31749, 5.627619999999999, 0.69315, 0.69315, 0.69315, 3.55535, 1.09861, 1.09861, 1.60944, 2.4849099999999997, 0.69315, 1.60944, 0.69315, 3.80666, 1.09861, 3.1354900000000003, 1.7917599999999998, 0.69315, 1.60944, 0.69315, 0.69315, 2.3979, 1.38629, 1.7917599999999998, 1.09861, 0.69315, 0.69315, 1.09861, 1.09861, 1.9459099999999998, 2.07944, 1.09861, 3.66356, 1.38629, 1.09861, 1.7917599999999998, 0.69315, 2.19722, 1.09861, 0.69315, 0.69315, 0.69315, 4.23411, 0.69315, 1.9459099999999998, 0.69315, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 0.69315, 4.86753, 1.60944, 0.69315, 0.69315, 2.99573, 4.8040199999999995, 4.39445, 3.7612, 1.38629, 0.69315, 0.69315, 6.03069, 3.09104, 0.69315, 1.38629, 0.69315, 0.69315, 4.43082, 1.38629, 7.37838, 1.09861, 3.17805, 1.60944, 4.23411, 2.70805, 1.7917599999999998, 1.09861, 2.4849099999999997, 4.955830000000001, 1.09861, 0.69315, 0.69315, 0.69315, 1.09861, 2.3025900000000004, 1.9459099999999998, 1.38629, 0.69315, 1.38629, 2.89037, 2.4849099999999997, 4.7362, 3.21888, 0.69315, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 4.96981, 1.09861, 2.3979, 1.9459099999999998, 0.69315, 2.4849099999999997, 3.21888, 2.4849099999999997, 2.6390599999999997, 1.09861, 2.70805, 2.3979, 1.38629, 1.38629, 2.89037, 1.38629, 1.38629, 0.69315, 1.60944, 0.69315, 4.47734, 5.40268, 5.50126, 1.38629, 5.58725, 2.4849099999999997, 1.09861, 0.69315, 0.69315, 1.60944, 1.38629, 4.17439, 1.38629, 1.60944, 1.38629, 2.56495, 1.38629, 2.6390599999999997, 1.38629, 2.3025900000000004, 0.69315, 1.38629, 0.69315, 1.38629, 3.04452, 0.69315, 4.110869999999999, 0.69315, 2.3025900000000004, 1.09861, 1.09861, 2.07944, 0.69315, 0.69315, 0.69315, 5.33272, 1.38629, 0.69315, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 2.77259, 0.69315, 0.69315, 0.69315, 1.60944, 0.69315, 1.38629, 3.3673, 4.0073300000000005, 1.38629, 1.60944, 1.9459099999999998, 0.69315, 3.71357, 1.7917599999999998, 0.69315, 1.09861, 1.60944, 0.69315, 3.29584, 4.41884, 1.60944, 1.7917599999999998, 0.69315, 3.71357, 1.60944, 1.09861, 1.09861, 0.69315, 6.6695, 1.09861, 0.69315, 1.38629, 1.60944, 1.38629, 3.66356, 0.69315, 0.69315, 1.7917599999999998, 0.69315, 3.8918199999999996, 1.09861, 0.69315, 5.52146, 1.09861, 1.09861, 2.3979, 1.38629, 1.38629, 1.09861, 6.4552, 2.07944, 2.19722, 4.23411, 1.09861, 1.38629, 1.9459099999999998, 4.99043, 0.69315, 8.50127, 0.69315, 1.09861, 1.09861, 2.3979, 1.38629, 1.09861, 1.7917599999999998, 0.69315, 0.69315, 1.60944, 4.07754, 1.09861, 2.70805, 3.7376699999999996, 3.98898, 1.09861, 2.56495, 0.69315, 0.69315, 3.3673, 0.69315, 1.09861, 2.83321, 2.3025900000000004, 0.69315, 0.69315, 4.57471, 1.38629, 0.69315, 3.2581, 1.38629, 0.69315, 0.69315, 2.6390599999999997, 0.69315, 1.38629, 1.60944, 1.38629, 0.69315, 1.9459099999999998, 1.9459099999999998, 5.64545, 1.60944, 0.69315, 2.94444, 0.69315, 0.69315, 2.19722, 0.69315, 2.4849099999999997, 1.38629, 0.69315, 1.09861, 6.18002, 1.7917599999999998, 1.09861, 1.09861, 4.65396, 6.90875, 0.69315, 5.0304400000000005, 5.24175, 0.69315, 0.69315, 1.60944, 1.38629, 2.19722, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 2.19722, 3.4657400000000003, 0.69315, 1.09861, 2.3979, 1.09861, 1.38629, 2.07944, 1.09861, 1.38629, 1.09861, 2.83321, 1.38629, 7.972810000000001, 4.41884, 2.07944, 0.69315, 0.69315, 6.43455, 0.69315, 3.80666, 0.69315, 0.69315, 1.38629, 1.09861, 6.47851, 1.60944, 0.69315, 0.69315, 1.38629, 2.89037, 2.70805, 1.7917599999999998, 1.09861, 3.3673, 0.69315, 0.69315, 0.69315, 5.17615, 0.69315, 0.69315, 1.60944, 1.7917599999999998, 1.60944, 0.69315, 1.09861, 1.09861, 2.3979, 1.60944, 0.69315, 1.38629, 8.18897, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 3.78419, 0.69315, 0.69315, 5.710430000000001, 4.8598099999999995, 1.7917599999999998, 1.9459099999999998, 3.9512400000000003, 3.4657400000000003, 4.67283, 3.82864, 1.38629, 1.09861, 5.54518, 2.19722, 1.38629, 0.69315, 1.38629, 1.09861, 0.69315, 1.38629, 7.7411, 0.69315, 2.99573, 1.60944, 6.50429, 1.9459099999999998, 0.69315, 0.69315, 1.09861, 2.3979, 0.69315, 2.07944, 3.09104, 1.09861, 0.69315, 1.09861, 2.89037, 0.69315, 1.7917599999999998, 1.38629, 2.19722, 0.69315, 2.77259, 2.19722, 4.40672, 2.83321, 1.60944, 4.304069999999999, 3.09104, 1.09861, 3.52636, 1.38629, 0.69315, 0.69315, 0.69315, 1.09861, 3.43399, 1.9459099999999998, 1.60944, 1.60944, 2.3979, 2.99573, 1.60944, 0.69315, 4.304069999999999, 4.20469, 5.463830000000001, 1.60944, 2.6390599999999997, 2.94444, 3.9702900000000003, 1.09861, 1.9459099999999998, 2.70805, 6.10479, 2.6390599999999997, 3.49651, 3.68888, 1.09861, 4.31749, 0.69315, 1.7917599999999998, 1.09861, 1.38629, 1.7917599999999998, 4.7706800000000005, 0.69315, 1.09861, 1.09861, 0.69315, 0.69315, 1.7917599999999998, 2.19722, 1.38629, 0.69315, 0.69315, 3.55535, 0.69315, 4.49981, 2.3025900000000004, 1.9459099999999998, 6.639880000000001, 2.07944, 1.09861, 1.09861, 1.38629, 3.1354900000000003, 5.0876, 0.69315, 1.7917599999999998, 5.05625, 1.7917599999999998, 0.69315, 2.56495, 0.69315, 1.38629, 1.09861, 2.3979, 0.69315, 1.7917599999999998, 1.09861, 2.56495, 2.07944, 2.89037, 0.69315, 3.04452, 0.69315, 1.09861, 0.69315, 2.4849099999999997, 1.7917599999999998, 1.38629, 0.69315, 2.94444, 2.99573, 0.69315, 1.38629, 0.69315, 1.09861, 3.21888, 1.09861, 4.75359, 0.69315, 0.69315, 4.21951, 2.19722, 1.9459099999999998, 3.78419, 2.77259, 0.69315, 1.38629, 2.56495, 1.9459099999999998, 5.53339, 0.69315, 5.77144, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 0.69315, 4.21951, 2.07944, 0.69315, 2.19722, 1.9459099999999998, 1.9459099999999998, 2.70805, 0.69315, 2.07944, 2.77259, 3.55535, 1.60944, 1.09861, 0.69315, 0.69315, 1.09861, 1.09861, 0.69315, 0.69315, 1.38629, 0.69315, 1.60944, 1.7917599999999998, 2.07944, 6.09131, 2.3025900000000004, 3.78419, 2.07944, 1.9459099999999998, 3.66356, 0.69315, 1.60944, 1.7917599999999998, 0.69315, 2.77259, 1.60944, 1.38629, 1.38629, 6.13773, 1.60944, 1.38629, 0.69315, 2.4849099999999997, 1.09861, 3.17805], \"type\": \"scatter\", \"name\": \"Label\", \"mode\": \"lines\", \"x\": [\"2016-12-23 15:53:30.718000\", \"2016-12-23 16:01:01.565000\", \"2016-12-23 16:22:24.813000\", \"2016-12-23 16:46:18.982000\", \"2016-12-23 16:49:19.775000\", \"2016-12-23 16:52:58.407000\", \"2016-12-23 16:58:18.601000\", \"2016-12-23 17:01:10.853000\", \"2016-12-23 17:28:20.805000\", \"2016-12-23 17:37:43.643000\", \"2016-12-23 17:53:38.413000\", \"2016-12-23 17:56:26.242000\", \"2016-12-23 18:17:59.181000\", \"2016-12-23 18:19:16.464000\", \"2016-12-23 18:22:39.945000\", \"2016-12-23 18:26:08\", \"2016-12-23 18:38:13.888000\", \"2016-12-23 18:40:45.846000\", \"2016-12-23 19:10:46.357000\", \"2016-12-23 19:23:45.172000\", \"2016-12-23 19:29:01.670000\", \"2016-12-23 19:39:33.457000\", \"2016-12-23 19:40:53.997000\", \"2016-12-23 19:58:01.981000\", \"2016-12-23 20:04:00.106000\", \"2016-12-23 20:23:28.587000\", \"2016-12-23 20:39:51.860000\", \"2016-12-23 21:10:09.295000\", \"2016-12-23 21:25:25.159000\", \"2016-12-23 21:30:11.194000\", \"2016-12-23 21:35:36.930000\", \"2016-12-23 21:38:17.414000\", \"2016-12-23 21:54:19.476000\", \"2016-12-23 21:57:12.602000\", \"2016-12-23 22:03:51.205000\", \"2016-12-23 22:05:17.490000\", \"2016-12-23 22:11:43.077000\", \"2016-12-23 22:35:12.931000\", \"2016-12-23 22:39:40.040000\", \"2016-12-23 23:38:05.726000\", \"2016-12-23 23:38:06.062000\", \"2016-12-23 23:47:34.435000\", \"2016-12-23 23:57:53.877000\", \"2016-12-24 00:01:07.669000\", \"2016-12-24 00:05:26.151000\", \"2016-12-24 00:08:49.419000\", \"2016-12-24 00:15:53.132000\", \"2016-12-24 00:19:06.082000\", \"2016-12-24 00:26:33.146000\", \"2016-12-24 00:38:54.359000\", \"2016-12-24 00:53:51.615000\", \"2016-12-24 00:58:10.609000\", \"2016-12-24 02:06:13.271000\", \"2016-12-24 02:13:48.916000\", \"2016-12-24 02:47:03.725000\", \"2016-12-24 03:22:46.851000\", \"2016-12-24 03:36:45.364000\", \"2016-12-24 03:37:00.894000\", \"2016-12-24 03:50:27.996000\", \"2016-12-24 03:56:01.347000\", \"2016-12-24 04:07:44.752000\", \"2016-12-24 04:13:29.771000\", \"2016-12-24 04:39:27.901000\", \"2016-12-24 05:12:09.540000\", \"2016-12-24 05:17:10.088000\", \"2016-12-24 06:04:55.942000\", \"2016-12-24 06:17:10.332000\", \"2016-12-24 06:17:19.029000\", \"2016-12-24 07:29:21.748000\", \"2016-12-24 08:05:38.002000\", \"2016-12-24 08:11:01.367000\", \"2016-12-24 08:40:16.309000\", \"2016-12-24 08:40:39.604000\", \"2016-12-24 09:26:45.069000\", \"2016-12-24 10:35:53.938000\", \"2016-12-24 10:59:27.579000\", \"2016-12-24 11:21:26.862000\", \"2016-12-24 11:45:31.389000\", \"2016-12-24 12:47:47.786000\", \"2016-12-24 13:01:37.613000\", \"2016-12-24 13:03:42.031000\", \"2016-12-24 13:04:31.551000\", \"2016-12-24 13:05:46.233000\", \"2016-12-24 13:07:10.670000\", \"2016-12-24 13:20:12.217000\", \"2016-12-24 13:22:55.304000\", \"2016-12-24 13:29:57.720000\", \"2016-12-24 13:43:59.174000\", \"2016-12-24 13:47:08.209000\", \"2016-12-24 14:11:46.525000\", \"2016-12-24 14:34:48.982000\", \"2016-12-24 14:56:09.018000\", \"2016-12-24 15:03:03.805000\", \"2016-12-24 15:31:40.185000\", \"2016-12-24 15:57:51.214000\", \"2016-12-24 15:58:50.199000\", \"2016-12-24 16:01:01.540000\", \"2016-12-24 16:02:20.287000\", \"2016-12-24 16:03:45.710000\", \"2016-12-24 16:10:45.552000\", \"2016-12-24 16:28:22.363000\", \"2016-12-24 16:33:31.462000\", \"2016-12-24 16:39:54.555000\", \"2016-12-24 16:42:38.858000\", \"2016-12-24 17:35:47.354000\", \"2016-12-24 17:59:05.886000\", \"2016-12-24 18:21:13.427000\", \"2016-12-24 18:35:04.966000\", \"2016-12-24 19:01:01.737000\", \"2016-12-24 19:26:08.771000\", \"2016-12-24 20:03:32.943000\", \"2016-12-24 22:30:37.611000\", \"2016-12-25 00:54:39.256000\", \"2016-12-25 01:10:33.156000\", \"2016-12-25 01:29:55.583000\", \"2016-12-25 02:00:35\", \"2016-12-25 02:01:01.281000\", \"2016-12-25 02:36:47.459000\", \"2016-12-25 03:07:57.271000\", \"2016-12-25 03:45:35.359000\", \"2016-12-25 04:28:28.426000\", \"2016-12-25 04:45:32.030000\", \"2016-12-25 04:59:24.268000\", \"2016-12-25 05:53:44.534000\", \"2016-12-25 07:06:42.985000\", \"2016-12-25 07:43:27.635000\", \"2016-12-25 08:17:17.006000\", \"2016-12-25 08:40:14.899000\", \"2016-12-25 08:58:58.175000\", \"2016-12-25 10:47:01.541000\", \"2016-12-25 10:48:53.564000\", \"2016-12-25 10:51:14.297000\", \"2016-12-25 11:04:11.885000\", \"2016-12-25 11:22:01.003000\", \"2016-12-25 11:23:06.895000\", \"2016-12-25 13:06:35.738000\", \"2016-12-25 13:20:00.450000\", \"2016-12-25 13:20:39.624000\", \"2016-12-25 14:07:47.185000\", \"2016-12-25 14:28:02.507000\", \"2016-12-25 14:59:48.358000\", \"2016-12-25 15:30:46.848000\", \"2016-12-25 15:50:32.652000\", \"2016-12-25 16:33:57.347000\", \"2016-12-25 16:39:25.081000\", \"2016-12-25 16:44:39.622000\", \"2016-12-25 16:46:35\", \"2016-12-25 17:00:28.900000\", \"2016-12-25 17:14:27.688000\", \"2016-12-25 17:18:53.969000\", \"2016-12-25 17:25:10.509000\", \"2016-12-25 17:25:12.173000\", \"2016-12-25 18:55:42.621000\", \"2016-12-25 19:31:42.824000\", \"2016-12-25 20:25:31.956000\", \"2016-12-25 20:48:29.348000\", \"2016-12-25 20:57:53.312000\", \"2016-12-25 21:02:08.670000\", \"2016-12-25 21:05:35.551000\", \"2016-12-25 21:25:15.101000\", \"2016-12-25 21:35:09.018000\", \"2016-12-25 21:35:25.562000\", \"2016-12-25 23:31:22.500000\", \"2016-12-25 23:49:42.495000\", \"2016-12-26\", \"2016-12-26 00:21:31.175000\", \"2016-12-26 00:38:00.047000\", \"2016-12-26 01:00:05.202000\", \"2016-12-26 01:05:25.473000\", \"2016-12-26 01:48:51.935000\", \"2016-12-26 01:57:41.679000\", \"2016-12-26 02:37:29.819000\", \"2016-12-26 02:48:07.067000\", \"2016-12-26 02:57:07.649000\", \"2016-12-26 02:57:20.146000\", \"2016-12-26 02:59:58.497000\", \"2016-12-26 03:39:15.024000\", \"2016-12-26 04:35:04.722000\", \"2016-12-26 04:54:40.497000\", \"2016-12-26 04:56:17.856000\", \"2016-12-26 05:31:08.076000\", \"2016-12-26 06:09:37.064000\", \"2016-12-26 06:23:23.349000\", \"2016-12-26 06:28:45.144000\", \"2016-12-26 06:37:50.493000\", \"2016-12-26 06:55:50.031000\", \"2016-12-26 07:17:44.230000\", \"2016-12-26 07:19:55.989000\", \"2016-12-26 07:22:04.687000\", \"2016-12-26 07:27:59.865000\", \"2016-12-26 07:55:02.563000\", \"2016-12-26 08:00:22.561000\", \"2016-12-26 08:03:50.527000\", \"2016-12-26 08:22:16.742000\", \"2016-12-26 08:42:57.641000\", \"2016-12-26 08:44:53.593000\", \"2016-12-26 09:09:19.620000\", \"2016-12-26 10:16:52.874000\", \"2016-12-26 10:18:48.546000\", \"2016-12-26 11:18:19.097000\", \"2016-12-26 11:33:23.325000\", \"2016-12-26 11:33:26.244000\", \"2016-12-26 12:19:02.234000\", \"2016-12-26 12:29:18.035000\", \"2016-12-26 12:39:36.314000\", \"2016-12-26 13:01:01.557000\", \"2016-12-26 13:02:31.711000\", \"2016-12-26 13:21:03.790000\", \"2016-12-26 13:21:04.587000\", \"2016-12-26 13:37:31.998000\", \"2016-12-26 13:53:50.173000\", \"2016-12-26 13:57:17.487000\", \"2016-12-26 14:01:01.490000\", \"2016-12-26 14:07:42.244000\", \"2016-12-26 14:27:23.319000\", \"2016-12-26 14:45:28.681000\", \"2016-12-26 15:01:01.665000\", \"2016-12-26 15:01:43.347000\", \"2016-12-26 15:08:39.037000\", \"2016-12-26 15:12:55.687000\", \"2016-12-26 15:34:58.118000\", \"2016-12-26 15:38:05.408000\", \"2016-12-26 16:01:56.605000\", \"2016-12-26 16:14:14.702000\", \"2016-12-26 16:26:16.277000\", \"2016-12-26 16:32:50.694000\", \"2016-12-26 16:34:14.270000\", \"2016-12-26 16:38:13.877000\", \"2016-12-26 16:38:22.330000\", \"2016-12-26 16:51:09.117000\", \"2016-12-26 16:59:05.550000\", \"2016-12-26 17:00:33.343000\", \"2016-12-26 17:09:39.318000\", \"2016-12-26 17:16:15.874000\", \"2016-12-26 17:23:48.318000\", \"2016-12-26 17:32:19.061000\", \"2016-12-26 17:36:01.287000\", \"2016-12-26 17:47:45.283000\", \"2016-12-26 17:50:59.828000\", \"2016-12-26 17:52:45.569000\", \"2016-12-26 17:52:54.504000\", \"2016-12-26 17:54:09.147000\", \"2016-12-26 18:22:18.642000\", \"2016-12-26 18:30:55.693000\", \"2016-12-26 18:39:26.847000\", \"2016-12-26 18:53:12.883000\", \"2016-12-26 18:59:43.557000\", \"2016-12-26 19:02:36.546000\", \"2016-12-26 19:10:08.267000\", \"2016-12-26 19:14:27.728000\", \"2016-12-26 19:25:56.098000\", \"2016-12-26 19:31:09.234000\", \"2016-12-26 20:03:05.951000\", \"2016-12-26 20:19:06.019000\", \"2016-12-26 20:20:43.254000\", \"2016-12-26 20:38:58.641000\", \"2016-12-26 20:40:22.647000\", \"2016-12-26 20:42:24.307000\", \"2016-12-26 20:52:32.405000\", \"2016-12-26 21:09:07.047000\", \"2016-12-26 21:16:07.582000\", \"2016-12-26 21:38:00.223000\", \"2016-12-26 21:52:40.449000\", \"2016-12-26 22:09:05.074000\", \"2016-12-26 22:32:58.110000\", \"2016-12-26 22:34:42.467000\", \"2016-12-26 22:39:24.100000\", \"2016-12-26 22:51:08.174000\", \"2016-12-26 23:30:45.086000\", \"2016-12-26 23:31:48.388000\", \"2016-12-26 23:45:32.558000\", \"2016-12-26 23:48:55.415000\", \"2016-12-26 23:50:58.725000\", \"2016-12-27 00:10:56.507000\", \"2016-12-27 00:12:25.769000\", \"2016-12-27 00:21:07.144000\", \"2016-12-27 00:24:03.622000\", \"2016-12-27 00:26:59.103000\", \"2016-12-27 00:31:56.572000\", \"2016-12-27 00:56:50.919000\", \"2016-12-27 00:57:43.988000\", \"2016-12-27 00:59:15.112000\", \"2016-12-27 01:02:12.447000\", \"2016-12-27 01:53:35.636000\", \"2016-12-27 01:56:28.693000\", \"2016-12-27 02:10:37.541000\", \"2016-12-27 02:51:05.955000\", \"2016-12-27 03:02:21.454000\", \"2016-12-27 03:13:36.603000\", \"2016-12-27 03:41:52.699000\", \"2016-12-27 04:21:52.996000\", \"2016-12-27 05:22:46.005000\", \"2016-12-27 05:31:44.983000\", \"2016-12-27 05:34:07.339000\", \"2016-12-27 05:34:19.172000\", \"2016-12-27 05:36:39.768000\", \"2016-12-27 05:57:18.494000\", \"2016-12-27 06:20:14.361000\", \"2016-12-27 06:45:38.565000\", \"2016-12-27 07:32:05.729000\", \"2016-12-27 07:45:17.220000\", \"2016-12-27 07:50:35.605000\", \"2016-12-27 07:52:17.448000\", \"2016-12-27 08:09:50.698000\", \"2016-12-27 08:15:06.870000\", \"2016-12-27 08:17:00.248000\", \"2016-12-27 08:40:03.420000\", \"2016-12-27 08:58:18.968000\", \"2016-12-27 08:59:57.428000\", \"2016-12-27 09:11:57.488000\", \"2016-12-27 09:17:50.462000\", \"2016-12-27 09:19:52.271000\", \"2016-12-27 09:22:50.150000\", \"2016-12-27 09:24:04.851000\", \"2016-12-27 09:31:01.492000\", \"2016-12-27 09:41:00.220000\", \"2016-12-27 10:01:00.810000\", \"2016-12-27 10:16:11.886000\", \"2016-12-27 10:17:12.984000\", \"2016-12-27 10:30:09.817000\", \"2016-12-27 10:31:31.191000\", \"2016-12-27 11:10:39.581000\", \"2016-12-27 11:17:56.221000\", \"2016-12-27 11:36:32.030000\", \"2016-12-27 11:37:20.562000\", \"2016-12-27 11:41:06.393000\", \"2016-12-27 11:53:18.178000\", \"2016-12-27 12:15:29.080000\", \"2016-12-27 12:24:01.005000\", \"2016-12-27 12:38:36.671000\", \"2016-12-27 12:44:37.706000\", \"2016-12-27 12:49:48.705000\", \"2016-12-27 12:49:49.043000\", \"2016-12-27 13:00:07.343000\", \"2016-12-27 13:06:20.535000\", \"2016-12-27 13:36:19.857000\", \"2016-12-27 13:39:57.766000\", \"2016-12-27 13:45:27.238000\", \"2016-12-27 14:34:20.029000\", \"2016-12-27 14:36:23.842000\", \"2016-12-27 14:44:34.554000\", \"2016-12-27 14:50:40.296000\", \"2016-12-27 15:00:45.144000\", \"2016-12-27 15:01:01.667000\", \"2016-12-27 15:02:16.286000\", \"2016-12-27 15:13:18.097000\", \"2016-12-27 15:19:34.577000\", \"2016-12-27 15:28:47.733000\", \"2016-12-27 15:29:15.708000\", \"2016-12-27 15:44:58.981000\", \"2016-12-27 15:57:51\", \"2016-12-27 15:58:16.265000\", \"2016-12-27 16:00:36.265000\", \"2016-12-27 16:03:16.628000\", \"2016-12-27 16:05:03.228000\", \"2016-12-27 16:11:01.549000\", \"2016-12-27 16:13:38.863000\", \"2016-12-27 16:18:41.925000\", \"2016-12-27 16:22:59.453000\", \"2016-12-27 16:26:14.625000\", \"2016-12-27 16:32:39.773000\", \"2016-12-27 16:32:54.447000\", \"2016-12-27 16:51:06.965000\", \"2016-12-27 17:06:01.247000\", \"2016-12-27 17:13:11.955000\", \"2016-12-27 17:17:46.117000\", \"2016-12-27 17:21:35.418000\", \"2016-12-27 17:26:57.681000\", \"2016-12-27 17:31:29.663000\", \"2016-12-27 17:33:31.823000\", \"2016-12-27 17:39:55.090000\", \"2016-12-27 17:49:30.095000\", \"2016-12-27 17:50:03.615000\", \"2016-12-27 17:57:26.524000\", \"2016-12-27 18:00:32.782000\", \"2016-12-27 18:21:21.221000\", \"2016-12-27 18:24:39.637000\", \"2016-12-27 18:26:11.219000\", \"2016-12-27 18:57:55.257000\", \"2016-12-27 19:01:01.933000\", \"2016-12-27 19:06:01.367000\", \"2016-12-27 19:09:17.345000\", \"2016-12-27 19:16:38.349000\", \"2016-12-27 19:27:03.422000\", \"2016-12-27 19:45:18.662000\", \"2016-12-27 20:02:02.480000\", \"2016-12-27 20:06:09.511000\", \"2016-12-27 20:14:24.124000\", \"2016-12-27 20:18:20.553000\", \"2016-12-27 20:19:46.777000\", \"2016-12-27 20:34:33.344000\", \"2016-12-27 21:11:39.893000\", \"2016-12-27 21:13:19.574000\", \"2016-12-27 21:28:09.806000\", \"2016-12-27 21:50:04.874000\", \"2016-12-27 21:50:09.009000\", \"2016-12-27 21:56:47.213000\", \"2016-12-27 21:57:50.674000\", \"2016-12-27 22:06:10.192000\", \"2016-12-27 22:11:47.947000\", \"2016-12-27 22:15:27.411000\", \"2016-12-27 22:20:19.100000\", \"2016-12-27 22:35:43.612000\", \"2016-12-27 22:35:46.495000\", \"2016-12-27 22:43:36.707000\", \"2016-12-27 23:01:01.395000\", \"2016-12-27 23:07:05.130000\", \"2016-12-27 23:09:28.107000\", \"2016-12-27 23:09:59.658000\", \"2016-12-27 23:10:32.779000\", \"2016-12-27 23:19:26.314000\", \"2016-12-27 23:32:13.220000\", \"2016-12-27 23:51:47.548000\", \"2016-12-28 00:13:24.219000\", \"2016-12-28 00:29:32.608000\", \"2016-12-28 00:37:13.109000\", \"2016-12-28 00:41:56.860000\", \"2016-12-28 01:01:01.660000\", \"2016-12-28 01:23:59.111000\", \"2016-12-28 01:27:03.530000\", \"2016-12-28 01:29:03.860000\", \"2016-12-28 01:39:43.215000\", \"2016-12-28 01:48:38.683000\", \"2016-12-28 01:51:00\", \"2016-12-28 01:55:14.359000\", \"2016-12-28 02:23:48.555000\", \"2016-12-28 02:56:07.156000\", \"2016-12-28 03:01:01.669000\", \"2016-12-28 03:26:04.344000\", \"2016-12-28 03:29:18.178000\", \"2016-12-28 03:37:43.371000\", \"2016-12-28 03:44:00.822000\", \"2016-12-28 03:44:25.645000\", \"2016-12-28 03:49:58.404000\", \"2016-12-28 03:53:06.033000\", \"2016-12-28 04:19:10.608000\", \"2016-12-28 04:21:08.021000\", \"2016-12-28 04:33:59.286000\", \"2016-12-28 04:46:06.926000\", \"2016-12-28 04:57:24.595000\", \"2016-12-28 05:15:34.533000\", \"2016-12-28 05:34:45.101000\", \"2016-12-28 05:40:29.287000\", \"2016-12-28 05:50:47.216000\", \"2016-12-28 05:51:15.388000\", \"2016-12-28 06:05:16.719000\", \"2016-12-28 06:06:34.389000\", \"2016-12-28 06:21:01.645000\", \"2016-12-28 06:29:20.191000\", \"2016-12-28 06:33:14\", \"2016-12-28 06:41:05.636000\", \"2016-12-28 06:41:36.878000\", \"2016-12-28 07:00:44.698000\", \"2016-12-28 07:04:57.531000\", \"2016-12-28 07:12:01.850000\", \"2016-12-28 07:31:43.567000\", \"2016-12-28 07:33:53.575000\", \"2016-12-28 07:35:43.120000\", \"2016-12-28 07:52:18.878000\", \"2016-12-28 08:03:39.732000\", \"2016-12-28 08:17:18.171000\", \"2016-12-28 08:20:21.759000\", \"2016-12-28 08:25:42.916000\", \"2016-12-28 08:34:46.026000\", \"2016-12-28 08:38:54.337000\", \"2016-12-28 08:44:01.827000\", \"2016-12-28 09:04:56.095000\", \"2016-12-28 09:31:15.812000\", \"2016-12-28 09:35:36.281000\", \"2016-12-28 09:57:24.762000\", \"2016-12-28 10:06:16.031000\", \"2016-12-28 10:10:33.506000\", \"2016-12-28 10:12:46.334000\", \"2016-12-28 10:16:01.149000\", \"2016-12-28 10:39:04.865000\", \"2016-12-28 11:08:32.163000\", \"2016-12-28 11:17:05.106000\", \"2016-12-28 11:25:44.148000\", \"2016-12-28 11:36:59.576000\", \"2016-12-28 11:38:55.244000\", \"2016-12-28 11:43:02.230000\", \"2016-12-28 12:04:37.801000\", \"2016-12-28 12:04:49.895000\", \"2016-12-28 12:29:11.151000\", \"2016-12-28 12:29:35.442000\", \"2016-12-28 12:34:09.930000\", \"2016-12-28 12:46:07.547000\", \"2016-12-28 12:50:07.671000\", \"2016-12-28 12:51:01.572000\", \"2016-12-28 13:07:31.291000\", \"2016-12-28 13:22:06.465000\", \"2016-12-28 13:23:05.109000\", \"2016-12-28 13:27:48.878000\", \"2016-12-28 13:30:26.480000\", \"2016-12-28 13:32:24.273000\", \"2016-12-28 13:46:23.063000\", \"2016-12-28 13:52:34.395000\", \"2016-12-28 14:05:22.217000\", \"2016-12-28 14:08:48.218000\", \"2016-12-28 14:10:17.193000\", \"2016-12-28 14:14:43.629000\", \"2016-12-28 14:16:42.628000\", \"2016-12-28 14:22:08.088000\", \"2016-12-28 14:28:57.481000\", \"2016-12-28 14:32:49.656000\", \"2016-12-28 14:41:18.505000\", \"2016-12-28 14:47:42.050000\", \"2016-12-28 15:24:51.100000\", \"2016-12-28 15:26:10.273000\", \"2016-12-28 15:33:42.176000\", \"2016-12-28 15:39:20.180000\", \"2016-12-28 15:41:18.227000\", \"2016-12-28 15:43:51.482000\", \"2016-12-28 15:48:35.941000\", \"2016-12-28 15:49:18.688000\", \"2016-12-28 15:51:39.588000\", \"2016-12-28 15:52:46.486000\", \"2016-12-28 16:04:08.196000\", \"2016-12-28 16:07:36.760000\", \"2016-12-28 16:13:41.686000\", \"2016-12-28 16:22:29.472000\", \"2016-12-28 16:28:15.579000\", \"2016-12-28 16:43:07.160000\", \"2016-12-28 17:09:15.707000\", \"2016-12-28 17:10:14.131000\", \"2016-12-28 17:33:07.680000\", \"2016-12-28 17:34:39.247000\", \"2016-12-28 17:35:17.852000\", \"2016-12-28 17:41:41.184000\", \"2016-12-28 18:04:39.292000\", \"2016-12-28 18:05:39.765000\", \"2016-12-28 18:16:41.424000\", \"2016-12-28 18:23:05.465000\", \"2016-12-28 18:30:43.179000\", \"2016-12-28 18:31:04.625000\", \"2016-12-28 18:35:13.076000\", \"2016-12-28 18:44:14.579000\", \"2016-12-28 18:47:00.188000\", \"2016-12-28 19:01:01.476000\", \"2016-12-28 19:06:46.216000\", \"2016-12-28 19:10:19.755000\", \"2016-12-28 19:12:58.363000\", \"2016-12-28 19:19:50.705000\", \"2016-12-28 19:32:36.585000\", \"2016-12-28 19:33:57.863000\", \"2016-12-28 19:41:21.141000\", \"2016-12-28 19:59:18.840000\", \"2016-12-28 20:07:53.242000\", \"2016-12-28 20:09:52.559000\", \"2016-12-28 20:15:21.649000\", \"2016-12-28 20:25:57.082000\", \"2016-12-28 20:26:35.300000\", \"2016-12-28 20:42:07.314000\", \"2016-12-28 20:50:46.968000\", \"2016-12-28 21:07:16.547000\", \"2016-12-28 21:08:50.776000\", \"2016-12-28 21:27:25\", \"2016-12-28 21:28:40.897000\", \"2016-12-28 21:33:50.344000\", \"2016-12-28 21:36:51.324000\", \"2016-12-28 21:40:17.142000\", \"2016-12-28 21:44:56\", \"2016-12-28 21:55:39.135000\", \"2016-12-28 22:00:50.630000\", \"2016-12-28 22:05:17.323000\", \"2016-12-28 22:12:07.710000\", \"2016-12-28 22:15:27.638000\", \"2016-12-28 22:16:13.779000\", \"2016-12-28 22:21:37.827000\", \"2016-12-28 22:34:25.333000\", \"2016-12-28 22:45:18.090000\", \"2016-12-28 22:46:05.596000\", \"2016-12-28 22:51:01.400000\", \"2016-12-28 22:54:45.823000\", \"2016-12-28 22:55:33.931000\", \"2016-12-28 23:03:21.456000\", \"2016-12-28 23:32:41.720000\", \"2016-12-28 23:41:42.869000\", \"2016-12-28 23:59:46.258000\", \"2016-12-29\", \"2016-12-29 00:00:31.579000\", \"2016-12-29 00:34:18.429000\", \"2016-12-29 00:36:56.428000\", \"2016-12-29 00:49:18.921000\", \"2016-12-29 01:07:44.100000\", \"2016-12-29 01:42:15.520000\", \"2016-12-29 02:01:01.460000\", \"2016-12-29 02:44:21.762000\", \"2016-12-29 03:08:41.077000\", \"2016-12-29 03:12:27.413000\", \"2016-12-29 03:21:48.894000\", \"2016-12-29 03:30:42.819000\", \"2016-12-29 03:46:13.334000\", \"2016-12-29 04:02:06.870000\", \"2016-12-29 04:07:09.509000\", \"2016-12-29 04:21:51.261000\", \"2016-12-29 04:27:15.836000\", \"2016-12-29 04:30:22.597000\", \"2016-12-29 04:31:25.017000\", \"2016-12-29 04:34:51.464000\", \"2016-12-29 04:58:48.920000\", \"2016-12-29 05:07:10\", \"2016-12-29 05:23:50.483000\", \"2016-12-29 05:25:28.416000\", \"2016-12-29 06:02:15.988000\", \"2016-12-29 06:04:47.114000\", \"2016-12-29 06:22:13.709000\", \"2016-12-29 07:14:38.454000\", \"2016-12-29 07:16:39.292000\", \"2016-12-29 07:29:05.287000\", \"2016-12-29 07:29:20.709000\", \"2016-12-29 09:11:19.132000\", \"2016-12-29 09:18:29.526000\", \"2016-12-29 09:29:41.795000\", \"2016-12-29 09:55:33.418000\", \"2016-12-29 10:14:43.116000\", \"2016-12-29 10:25:58.814000\", \"2016-12-29 10:36:27.125000\", \"2016-12-29 10:45:18.841000\", \"2016-12-29 11:02:36.465000\", \"2016-12-29 11:04:47.104000\", \"2016-12-29 11:12:18.160000\", \"2016-12-29 11:14:40.721000\", \"2016-12-29 11:43:39.203000\", \"2016-12-29 11:47:50.820000\", \"2016-12-29 11:52:19.828000\", \"2016-12-29 12:05:26.038000\", \"2016-12-29 12:08:10.174000\", \"2016-12-29 12:25:58.606000\", \"2016-12-29 12:31:00\", \"2016-12-29 12:36:49.894000\", \"2016-12-29 12:54:24.328000\", \"2016-12-29 13:00:00\", \"2016-12-29 13:19:07.957000\", \"2016-12-29 13:23:10\", \"2016-12-29 13:25:47.217000\", \"2016-12-29 13:47:27.267000\", \"2016-12-29 13:47:45.003000\", \"2016-12-29 14:01:01.710000\", \"2016-12-29 14:04:22.212000\", \"2016-12-29 14:08:58.140000\", \"2016-12-29 14:11:42.766000\", \"2016-12-29 14:26:01.423000\", \"2016-12-29 14:31:07.959000\", \"2016-12-29 14:31:55.394000\", \"2016-12-29 14:35:06.301000\", \"2016-12-29 14:37:12.173000\", \"2016-12-29 14:46:47.128000\", \"2016-12-29 14:48:41.731000\", \"2016-12-29 14:55:43.134000\", \"2016-12-29 15:00:10\", \"2016-12-29 15:00:28.101000\", \"2016-12-29 15:01:01.595000\", \"2016-12-29 15:01:01.605000\", \"2016-12-29 15:01:40.762000\", \"2016-12-29 15:05:39.080000\", \"2016-12-29 15:08:42.843000\", \"2016-12-29 15:16:40.770000\", \"2016-12-29 15:35:50.221000\", \"2016-12-29 15:42:53.174000\", \"2016-12-29 15:45:07.824000\", \"2016-12-29 16:06:01.985000\", \"2016-12-29 16:15:59.635000\", \"2016-12-29 16:23:25.839000\", \"2016-12-29 16:25:05\", \"2016-12-29 16:30:43.257000\", \"2016-12-29 16:33:07.765000\", \"2016-12-29 16:34:30.986000\", \"2016-12-29 16:44:40.105000\", \"2016-12-29 16:47:44.744000\", \"2016-12-29 16:48:18.333000\", \"2016-12-29 16:49:02.612000\", \"2016-12-29 16:52:03.192000\", \"2016-12-29 16:56:20.935000\", \"2016-12-29 17:01:01.377000\", \"2016-12-29 17:18:16.308000\", \"2016-12-29 17:23:32.461000\", \"2016-12-29 17:25:00.830000\", \"2016-12-29 17:34:31.438000\", \"2016-12-29 17:40:59.606000\", \"2016-12-29 17:41:01.496000\", \"2016-12-29 17:46:54.338000\", \"2016-12-29 17:58:54.092000\", \"2016-12-29 18:08:04.788000\", \"2016-12-29 18:10:08.172000\", \"2016-12-29 18:12:15.791000\", \"2016-12-29 18:15:04.697000\", \"2016-12-29 18:16:06.138000\", \"2016-12-29 18:16:24.860000\", \"2016-12-29 18:30:00.471000\", \"2016-12-29 18:32:13.788000\", \"2016-12-29 18:36:34.542000\", \"2016-12-29 18:51:15.579000\", \"2016-12-29 18:55:52.317000\", \"2016-12-29 19:02:32.474000\", \"2016-12-29 19:02:43.051000\", \"2016-12-29 19:10:18.184000\", \"2016-12-29 19:11:33.446000\", \"2016-12-29 19:27:36.343000\", \"2016-12-29 19:42:01.659000\", \"2016-12-29 19:42:33.005000\", \"2016-12-29 19:46:43.945000\", \"2016-12-29 19:48:27.965000\", \"2016-12-29 19:50:34.237000\", \"2016-12-29 19:53:29.281000\", \"2016-12-29 19:55:27.819000\", \"2016-12-29 19:58:36.809000\", \"2016-12-29 20:21:26.149000\", \"2016-12-29 20:29:26.506000\", \"2016-12-29 20:40:27.834000\", \"2016-12-29 20:46:18.379000\", \"2016-12-29 20:46:25.053000\", \"2016-12-29 20:56:50.666000\", \"2016-12-29 21:23:00.562000\", \"2016-12-29 21:24:36.111000\", \"2016-12-29 21:27:42.537000\", \"2016-12-29 21:37:30.820000\", \"2016-12-29 22:02:28.956000\", \"2016-12-29 22:26:02.330000\", \"2016-12-29 22:38:59.486000\", \"2016-12-29 22:49:26.276000\", \"2016-12-29 22:55:20.911000\", \"2016-12-29 23:09:28.832000\", \"2016-12-29 23:13:54.502000\", \"2016-12-29 23:28:26.912000\", \"2016-12-29 23:33:04.678000\", \"2016-12-29 23:35:54.027000\", \"2016-12-29 23:43:52.232000\", \"2016-12-30\", \"2016-12-30 00:07:45.976000\", \"2016-12-30 00:15:44.080000\", \"2016-12-30 00:35:40.782000\", \"2016-12-30 00:36:03.961000\", \"2016-12-30 00:51:43.870000\", \"2016-12-30 01:04:17.799000\", \"2016-12-30 01:16:29.549000\", \"2016-12-30 01:16:34.417000\", \"2016-12-30 01:25:48.749000\", \"2016-12-30 01:48:48.459000\", \"2016-12-30 02:03:48.380000\", \"2016-12-30 02:20:54.168000\", \"2016-12-30 02:35:12.742000\", \"2016-12-30 02:35:21.907000\", \"2016-12-30 02:35:23.380000\", \"2016-12-30 02:41:40.048000\", \"2016-12-30 03:12:26.662000\", \"2016-12-30 04:59:09.866000\", \"2016-12-30 05:08:49.714000\", \"2016-12-30 05:09:44.937000\", \"2016-12-30 05:17:48.548000\", \"2016-12-30 05:38:03.834000\", \"2016-12-30 05:39:45.764000\", \"2016-12-30 05:55:24.223000\", \"2016-12-30 06:05:55.939000\", \"2016-12-30 06:09:35.775000\", \"2016-12-30 06:23:00.922000\", \"2016-12-30 06:41:22.485000\", \"2016-12-30 06:51:29\", \"2016-12-30 06:58:55.588000\", \"2016-12-30 07:17:55.365000\", \"2016-12-30 07:19:58.934000\", \"2016-12-30 07:27:23.653000\", \"2016-12-30 07:34:27.879000\", \"2016-12-30 07:48:41.792000\", \"2016-12-30 08:06:27.929000\", \"2016-12-30 08:16:24.143000\", \"2016-12-30 08:25:29.348000\", \"2016-12-30 09:16:03.929000\", \"2016-12-30 09:34:38.852000\", \"2016-12-30 09:36:02.740000\", \"2016-12-30 09:47:33.987000\", \"2016-12-30 09:56:07\", \"2016-12-30 09:57:30.992000\", \"2016-12-30 10:07:12.573000\", \"2016-12-30 10:41:28.397000\", \"2016-12-30 11:06:08.895000\", \"2016-12-30 11:22:45.655000\", \"2016-12-30 11:26:51.213000\", \"2016-12-30 11:35:48.575000\", \"2016-12-30 11:39:03.484000\", \"2016-12-30 11:49:03.072000\", \"2016-12-30 11:52:03.382000\", \"2016-12-30 12:01:35.448000\", \"2016-12-30 12:44:18.534000\", \"2016-12-30 12:46:23\", \"2016-12-30 12:59:20.144000\", \"2016-12-30 13:07:27.965000\", \"2016-12-30 13:10:36.360000\", \"2016-12-30 13:13:44.283000\", \"2016-12-30 13:18:30.169000\", \"2016-12-30 13:18:48.104000\", \"2016-12-30 13:26:10.240000\", \"2016-12-30 13:27:43.890000\", \"2016-12-30 13:29:07.907000\", \"2016-12-30 13:29:45.175000\", \"2016-12-30 13:31:01.619000\", \"2016-12-30 13:31:02.351000\", \"2016-12-30 13:44:40.740000\", \"2016-12-30 13:53:00.132000\", \"2016-12-30 13:59:40.624000\", \"2016-12-30 14:00:35.175000\", \"2016-12-30 14:31:01.720000\", \"2016-12-30 14:35:53.690000\", \"2016-12-30 14:36:20.589000\", \"2016-12-30 14:52:59.871000\", \"2016-12-30 15:01:02.380000\", \"2016-12-30 15:05:07.509000\", \"2016-12-30 15:14:34.823000\", \"2016-12-30 15:17:28.727000\", \"2016-12-30 15:25:42.789000\", \"2016-12-30 15:34:45.005000\", \"2016-12-30 15:45:42.006000\", \"2016-12-30 15:48:04.684000\", \"2016-12-30 15:52:38.360000\", \"2016-12-30 16:07:04.628000\", \"2016-12-30 16:07:37.269000\", \"2016-12-30 16:12:07.412000\", \"2016-12-30 16:15:04.657000\", \"2016-12-30 16:18:43.101000\", \"2016-12-30 16:36:52.008000\", \"2016-12-30 16:38:11.413000\", \"2016-12-30 16:39:15.346000\", \"2016-12-30 17:05:45.721000\", \"2016-12-30 17:10:38.888000\", \"2016-12-30 17:21:30.774000\", \"2016-12-30 17:22:30.625000\", \"2016-12-30 17:30:11.934000\", \"2016-12-30 17:43:42\", \"2016-12-30 17:44:00.028000\", \"2016-12-30 17:50:00.505000\", \"2016-12-30 17:51:57.233000\", \"2016-12-30 17:54:32.217000\", \"2016-12-30 18:02:16.846000\", \"2016-12-30 18:04:09.465000\", \"2016-12-30 18:11:24.969000\", \"2016-12-30 18:14:17.342000\", \"2016-12-30 18:22:02.476000\", \"2016-12-30 18:30:19.634000\", \"2016-12-30 18:33:18.083000\", \"2016-12-30 18:36:47.455000\", \"2016-12-30 18:36:54.008000\", \"2016-12-30 18:39:34.862000\", \"2016-12-30 18:44:16.084000\", \"2016-12-30 18:54:40.834000\", \"2016-12-30 19:11:17.762000\", \"2016-12-30 19:15:38.041000\", \"2016-12-30 19:24:48.902000\", \"2016-12-30 19:24:51.518000\", \"2016-12-30 19:25:54.446000\", \"2016-12-30 19:25:59.206000\", \"2016-12-30 19:31:01.315000\", \"2016-12-30 19:39:21.884000\", \"2016-12-30 20:29:25.392000\", \"2016-12-30 20:38:07.759000\", \"2016-12-30 20:39:07.243000\", \"2016-12-30 20:49:04.016000\", \"2016-12-30 20:53:25.981000\", \"2016-12-30 21:02:52.260000\", \"2016-12-30 21:10:19.869000\", \"2016-12-30 21:10:22.047000\", \"2016-12-30 21:35:46.214000\", \"2016-12-30 21:43:44.431000\", \"2016-12-30 21:48:51.285000\", \"2016-12-30 22:08:21.619000\", \"2016-12-30 22:21:27.664000\", \"2016-12-30 22:46:52.845000\", \"2016-12-30 22:53:06.466000\", \"2016-12-30 22:53:11.166000\", \"2016-12-30 23:02:42\", \"2016-12-30 23:07:33.314000\", \"2016-12-30 23:18:24.581000\", \"2016-12-30 23:18:53.055000\", \"2016-12-30 23:30:36.206000\", \"2016-12-30 23:30:44.951000\", \"2016-12-30 23:32:03.275000\", \"2016-12-30 23:37:44.312000\", \"2016-12-31\", \"2016-12-31 00:16:00.449000\", \"2016-12-31 00:59:35.994000\", \"2016-12-31 01:05:49.844000\", \"2016-12-31 01:15:11.165000\", \"2016-12-31 01:15:21.110000\", \"2016-12-31 01:20:40.484000\", \"2016-12-31 01:27:38.427000\", \"2016-12-31 01:34:37.391000\", \"2016-12-31 01:36:10.738000\", \"2016-12-31 01:41:38.406000\", \"2016-12-31 01:44:31.551000\", \"2016-12-31 01:52:06.917000\", \"2016-12-31 02:14:45.216000\", \"2016-12-31 02:25:15.734000\", \"2016-12-31 02:25:17.804000\", \"2016-12-31 02:26:01.645000\", \"2016-12-31 02:28:09.446000\", \"2016-12-31 02:48:26.366000\", \"2016-12-31 03:15:39.532000\", \"2016-12-31 03:23:56.347000\", \"2016-12-31 03:39:29.385000\", \"2016-12-31 03:57:47.827000\", \"2016-12-31 04:20:22.594000\", \"2016-12-31 05:23:50.521000\", \"2016-12-31 05:32:25.928000\", \"2016-12-31 06:02:46.346000\", \"2016-12-31 06:07:31.666000\", \"2016-12-31 06:30:36.581000\", \"2016-12-31 07:01:54.837000\", \"2016-12-31 07:27:03.009000\", \"2016-12-31 07:33:50.782000\", \"2016-12-31 08:33:14.663000\", \"2016-12-31 08:35:54.878000\", \"2016-12-31 08:42:20.755000\", \"2016-12-31 08:49:20.096000\", \"2016-12-31 09:01:12.394000\", \"2016-12-31 09:29:22\", \"2016-12-31 09:30:38.655000\", \"2016-12-31 09:50:30.018000\", \"2016-12-31 09:52:56.434000\", \"2016-12-31 10:16:48\", \"2016-12-31 10:31:44.891000\", \"2016-12-31 10:38:03.139000\", \"2016-12-31 10:40:25.801000\", \"2016-12-31 10:59:12.842000\", \"2016-12-31 11:11:01.612000\", \"2016-12-31 11:20:38.254000\", \"2016-12-31 12:10:56.702000\", \"2016-12-31 12:29:09.298000\", \"2016-12-31 12:31:30.436000\", \"2016-12-31 12:34:52.132000\", \"2016-12-31 12:48:56.159000\", \"2016-12-31 12:52:50.350000\", \"2016-12-31 13:00:13.725000\", \"2016-12-31 13:07:17.982000\", \"2016-12-31 13:26:01.826000\", \"2016-12-31 13:27:08.788000\", \"2016-12-31 13:31:37.069000\", \"2016-12-31 13:45:09.009000\", \"2016-12-31 13:47:12.077000\", \"2016-12-31 14:12:43.784000\", \"2016-12-31 14:19:05.910000\", \"2016-12-31 14:34:08.651000\", \"2016-12-31 14:36:15.281000\", \"2016-12-31 15:04:48.099000\", \"2016-12-31 15:08:51.718000\", \"2016-12-31 15:12:49.852000\", \"2016-12-31 15:29:50.145000\", \"2016-12-31 15:40:58.070000\", \"2016-12-31 15:44:31.622000\", \"2016-12-31 15:49:21.283000\", \"2016-12-31 15:59:22.555000\", \"2016-12-31 15:59:43\", \"2016-12-31 16:00:24.193000\", \"2016-12-31 16:14:31.495000\", \"2016-12-31 16:18:13.639000\", \"2016-12-31 16:19:55.030000\", \"2016-12-31 16:21:55.452000\", \"2016-12-31 16:27:07.111000\", \"2016-12-31 16:50:26.789000\", \"2016-12-31 16:57:57.153000\", \"2016-12-31 17:01:21.594000\", \"2016-12-31 17:10:49.625000\", \"2016-12-31 17:27:59.556000\", \"2016-12-31 17:48:58.509000\", \"2016-12-31 17:52:56.198000\", \"2016-12-31 17:59:33.607000\", \"2016-12-31 17:59:45.181000\", \"2016-12-31 18:05:38.646000\", \"2016-12-31 18:15:51.817000\", \"2016-12-31 18:23:46.585000\", \"2016-12-31 18:24:54.730000\", \"2016-12-31 18:44:46.446000\", \"2016-12-31 18:52:32\", \"2016-12-31 18:53:04.218000\", \"2016-12-31 19:01:25.706000\", \"2016-12-31 19:08:29.050000\", \"2016-12-31 19:20:57.095000\", \"2016-12-31 19:26:03.079000\", \"2016-12-31 19:53:45.481000\", \"2016-12-31 19:57:21.154000\", \"2016-12-31 20:01:01.703000\", \"2016-12-31 20:01:18.371000\", \"2016-12-31 20:17:40.369000\", \"2016-12-31 20:21:36.634000\", \"2016-12-31 20:22:43.406000\", \"2016-12-31 20:35:19.450000\", \"2016-12-31 20:36:25.249000\", \"2016-12-31 20:41:38.396000\", \"2016-12-31 20:52:00.717000\", \"2016-12-31 21:03:49.339000\", \"2016-12-31 21:11:16.192000\", \"2016-12-31 21:33:25.495000\", \"2016-12-31 21:34:27.564000\", \"2016-12-31 21:51:01.507000\", \"2016-12-31 21:58:43.265000\", \"2016-12-31 22:15:22.537000\", \"2016-12-31 22:17:45.433000\", \"2016-12-31 22:25:12.360000\", \"2016-12-31 23:04:27.063000\", \"2016-12-31 23:13:08.790000\", \"2016-12-31 23:26:55.147000\", \"2016-12-31 23:51:28.020000\"]}], {\"title\": \"Online users\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": false})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9c181fba-e1aa-4791-8c2b-f60230acfbac\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9c181fba-e1aa-4791-8c2b-f60230acfbac\", [{\"y\": [1.09861, 0.69315, 1.09861, 5.5134300000000005, 0.69315, 1.38629, 5.710430000000001, 1.9459099999999998, 2.07944, 3.55535, 0.69315, 1.09861, 2.70805, 1.60944, 0.69315, 3.21888, 1.60944, 0.69315, 0.69315, 0.69315, 2.56495, 2.77259, 1.09861, 3.09104, 0.69315, 1.7917599999999998, 1.9459099999999998, 3.2581, 0.69315, 4.07754, 0.69315, 2.6390599999999997, 1.7917599999999998, 1.60944, 1.60944, 0.69315, 1.38629, 2.83321, 6.41182, 1.60944, 1.60944, 0.69315, 0.69315, 2.3979, 1.60944, 3.17805, 0.69315, 6.807930000000001, 3.78419, 2.6390599999999997, 1.38629, 1.7917599999999998, 1.7917599999999998, 6.13556, 1.09861, 1.7917599999999998, 4.276669999999999, 3.68888, 1.38629, 2.83321, 5.8944, 0.69315, 2.3025900000000004, 1.60944, 1.9459099999999998, 1.09861, 1.38629, 0.69315, 0.69315, 0.69315, 0.69315, 3.2581, 2.07944, 4.74493, 0.69315, 1.9459099999999998, 0.69315, 0.69315, 1.09861, 1.38629, 0.69315, 2.3979, 2.70805, 3.80666, 0.69315, 1.38629, 1.38629, 0.69315, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 1.09861, 1.38629, 1.09861, 3.6375900000000003, 0.69315, 0.69315, 1.9459099999999998, 1.09861, 0.69315, 0.69315, 0.69315, 1.09861, 2.19722, 2.3025900000000004, 0.69315, 1.60944, 0.69315, 2.07944, 1.38629, 1.7917599999999998, 5.68698, 1.38629, 0.69315, 3.4657400000000003, 2.07944, 6.566669999999999, 6.3716099999999996, 2.4849099999999997, 5.92959, 1.09861, 2.3025900000000004, 2.19722, 0.69315, 0.69315, 0.69315, 4.56435, 0.69315, 2.3979, 1.9459099999999998, 2.19722, 1.9459099999999998, 2.56495, 0.69315, 1.7917599999999998, 4.06044, 0.69315, 1.7917599999999998, 1.7917599999999998, 0.69315, 0.69315, 0.69315, 0.69315, 3.7376699999999996, 1.09861, 1.09861, 0.69315, 1.7917599999999998, 1.60944, 2.3025900000000004, 1.60944, 3.6375900000000003, 2.56495, 3.6109199999999997, 2.56495, 3.4657400000000003, 3.29584, 1.09861, 2.3025900000000004, 0.69315, 3.43399, 1.38629, 0.69315, 3.2581, 1.38629, 2.07944, 1.38629, 1.09861, 0.69315, 1.38629, 2.70805, 2.19722, 1.38629, 1.09861, 2.07944, 0.69315, 1.60944, 1.9459099999999998, 0.69315, 2.99573, 4.110869999999999, 0.69315, 0.69315, 2.6390599999999997, 2.83321, 3.68888, 1.38629, 0.69315, 1.09861, 0.69315, 1.60944, 2.3979, 1.09861, 0.69315, 0.69315, 1.60944, 0.69315, 0.69315, 0.69315, 0.69315, 2.07944, 0.69315, 2.19722, 1.60944, 3.09104, 1.38629, 2.3979, 1.60944, 10.85902, 4.33073, 1.60944, 1.09861, 1.38629, 1.38629, 2.77259, 1.7917599999999998, 7.170889999999999, 3.2581, 5.42053, 1.09861, 0.69315, 2.07944, 3.78419, 1.9459099999999998, 2.07944, 0.69315, 3.9512400000000003, 1.38629, 0.69315, 4.35671, 0.69315, 5.0626, 4.07754, 0.69315, 1.7917599999999998, 0.69315, 1.9459099999999998, 1.60944, 2.19722, 2.6390599999999997, 1.7917599999999998, 1.7917599999999998, 0.69315, 1.09861, 0.69315, 5.09987, 6.272880000000001, 1.09861, 3.3673, 3.09104, 1.09861, 1.09861, 0.69315, 3.7376699999999996, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 3.93183, 2.94444, 2.19722, 1.09861, 0.69315, 3.04452, 3.21888, 1.38629, 0.69315, 2.56495, 6.013719999999999, 0.69315, 0.69315, 1.09861, 1.7917599999999998, 0.69315, 5.96101, 0.69315, 6.90875, 1.60944, 0.69315, 1.38629, 0.69315, 1.38629, 2.3979, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 2.3979, 3.3322, 2.3979, 1.38629, 0.69315, 2.6390599999999997, 2.70805, 0.69315, 1.09861, 1.38629, 0.69315, 5.48064, 3.09104, 0.69315, 3.3673, 4.3438099999999995, 1.60944, 0.69315, 5.28827, 3.8918199999999996, 0.69315, 5.1299, 0.69315, 8.101980000000001, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 1.09861, 2.83321, 5.17048, 1.9459099999999998, 1.38629, 1.38629, 0.69315, 3.21888, 1.09861, 2.56495, 1.60944, 1.9459099999999998, 3.09104, 2.83321, 1.09861, 2.3979, 5.07517, 2.6390599999999997, 1.7917599999999998, 6.2106, 5.05625, 0.69315, 1.38629, 6.90875, 4.94876, 6.570880000000001, 1.09861, 3.66356, 3.17805, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 4.20469, 1.60944, 0.69315, 1.09861, 0.69315, 3.09104, 2.6390599999999997, 1.09861, 0.69315, 4.69135, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 0.69315, 1.09861, 2.77259, 7.00397, 1.60944, 1.60944, 0.69315, 1.38629, 1.09861, 3.55535, 1.9459099999999998, 3.3673, 1.09861, 2.07944, 1.38629, 0.69315, 3.4657400000000003, 1.09861, 1.38629, 2.07944, 0.69315, 2.77259, 2.3025900000000004, 3.09104, 0.69315, 2.4849099999999997, 7.00397, 1.7917599999999998, 3.29584, 0.69315, 1.38629, 1.9459099999999998, 3.7376699999999996, 0.69315, 2.3025900000000004, 0.69315, 6.15698, 1.38629, 0.69315, 1.09861, 3.04452, 5.73979, 1.60944, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 2.94444, 0.69315, 5.24702, 0.69315, 1.60944, 2.94444, 1.9459099999999998, 2.94444, 3.6109199999999997, 3.3322, 3.17805, 4.91265, 1.38629, 2.3025900000000004, 4.7706800000000005, 1.9459099999999998, 1.60944, 6.19236, 1.38629, 1.60944, 5.51745, 0.69315, 0.69315, 0.69315, 1.7917599999999998, 2.07944, 1.38629, 2.6390599999999997, 1.38629, 4.31749, 5.627619999999999, 0.69315, 0.69315, 0.69315, 3.55535, 1.09861, 1.09861, 1.60944, 2.4849099999999997, 0.69315, 1.60944, 0.69315, 3.80666, 1.09861, 3.1354900000000003, 1.7917599999999998, 0.69315, 1.60944, 0.69315, 0.69315, 2.3979, 1.38629, 1.7917599999999998, 1.09861, 0.69315, 0.69315, 1.09861, 1.09861, 1.9459099999999998, 2.07944, 1.09861, 3.66356, 1.38629, 1.09861, 1.7917599999999998, 0.69315, 2.19722, 1.09861, 0.69315, 0.69315, 0.69315, 4.23411, 0.69315, 1.9459099999999998, 0.69315, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 0.69315, 4.86753, 1.60944, 0.69315, 0.69315, 2.99573, 4.8040199999999995, 4.39445, 3.7612, 1.38629, 0.69315, 0.69315, 6.03069, 3.09104, 0.69315, 1.38629, 0.69315, 0.69315, 4.43082, 1.38629, 7.37838, 1.09861, 3.17805, 1.60944, 4.23411, 2.70805, 1.7917599999999998, 1.09861, 2.4849099999999997, 4.955830000000001, 1.09861, 0.69315, 0.69315, 0.69315, 1.09861, 2.3025900000000004, 1.9459099999999998, 1.38629, 0.69315, 1.38629, 2.89037, 2.4849099999999997, 4.7362, 3.21888, 0.69315, 1.09861, 0.69315, 1.7917599999999998, 0.69315, 4.96981, 1.09861, 2.3979, 1.9459099999999998, 0.69315, 2.4849099999999997, 3.21888, 2.4849099999999997, 2.6390599999999997, 1.09861, 2.70805, 2.3979, 1.38629, 1.38629, 2.89037, 1.38629, 1.38629, 0.69315, 1.60944, 0.69315, 4.47734, 5.40268, 5.50126, 1.38629, 5.58725, 2.4849099999999997, 1.09861, 0.69315, 0.69315, 1.60944, 1.38629, 4.17439, 1.38629, 1.60944, 1.38629, 2.56495, 1.38629, 2.6390599999999997, 1.38629, 2.3025900000000004, 0.69315, 1.38629, 0.69315, 1.38629, 3.04452, 0.69315, 4.110869999999999, 0.69315, 2.3025900000000004, 1.09861, 1.09861, 2.07944, 0.69315, 0.69315, 0.69315, 5.33272, 1.38629, 0.69315, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 2.77259, 0.69315, 0.69315, 0.69315, 1.60944, 0.69315, 1.38629, 3.3673, 4.0073300000000005, 1.38629, 1.60944, 1.9459099999999998, 0.69315, 3.71357, 1.7917599999999998, 0.69315, 1.09861, 1.60944, 0.69315, 3.29584, 4.41884, 1.60944, 1.7917599999999998, 0.69315, 3.71357, 1.60944, 1.09861, 1.09861, 0.69315, 6.6695, 1.09861, 0.69315, 1.38629, 1.60944, 1.38629, 3.66356, 0.69315, 0.69315, 1.7917599999999998, 0.69315, 3.8918199999999996, 1.09861, 0.69315, 5.52146, 1.09861, 1.09861, 2.3979, 1.38629, 1.38629, 1.09861, 6.4552, 2.07944, 2.19722, 4.23411, 1.09861, 1.38629, 1.9459099999999998, 4.99043, 0.69315, 8.50127, 0.69315, 1.09861, 1.09861, 2.3979, 1.38629, 1.09861, 1.7917599999999998, 0.69315, 0.69315, 1.60944, 4.07754, 1.09861, 2.70805, 3.7376699999999996, 3.98898, 1.09861, 2.56495, 0.69315, 0.69315, 3.3673, 0.69315, 1.09861, 2.83321, 2.3025900000000004, 0.69315, 0.69315, 4.57471, 1.38629, 0.69315, 3.2581, 1.38629, 0.69315, 0.69315, 2.6390599999999997, 0.69315, 1.38629, 1.60944, 1.38629, 0.69315, 1.9459099999999998, 1.9459099999999998, 5.64545, 1.60944, 0.69315, 2.94444, 0.69315, 0.69315, 2.19722, 0.69315, 2.4849099999999997, 1.38629, 0.69315, 1.09861, 6.18002, 1.7917599999999998, 1.09861, 1.09861, 4.65396, 6.90875, 0.69315, 5.0304400000000005, 5.24175, 0.69315, 0.69315, 1.60944, 1.38629, 2.19722, 0.69315, 0.69315, 1.9459099999999998, 1.38629, 2.19722, 3.4657400000000003, 0.69315, 1.09861, 2.3979, 1.09861, 1.38629, 2.07944, 1.09861, 1.38629, 1.09861, 2.83321, 1.38629, 7.972810000000001, 4.41884, 2.07944, 0.69315, 0.69315, 6.43455, 0.69315, 3.80666, 0.69315, 0.69315, 1.38629, 1.09861, 6.47851, 1.60944, 0.69315, 0.69315, 1.38629, 2.89037, 2.70805, 1.7917599999999998, 1.09861, 3.3673, 0.69315, 0.69315, 0.69315, 5.17615, 0.69315, 0.69315, 1.60944, 1.7917599999999998, 1.60944, 0.69315, 1.09861, 1.09861, 2.3979, 1.60944, 0.69315, 1.38629, 8.18897, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315, 3.78419, 0.69315, 0.69315, 5.710430000000001, 4.8598099999999995, 1.7917599999999998, 1.9459099999999998, 3.9512400000000003, 3.4657400000000003, 4.67283, 3.82864, 1.38629, 1.09861, 5.54518, 2.19722, 1.38629, 0.69315, 1.38629, 1.09861, 0.69315, 1.38629, 7.7411, 0.69315, 2.99573, 1.60944, 6.50429, 1.9459099999999998, 0.69315, 0.69315, 1.09861, 2.3979, 0.69315, 2.07944, 3.09104, 1.09861, 0.69315, 1.09861, 2.89037, 0.69315, 1.7917599999999998, 1.38629, 2.19722, 0.69315, 2.77259, 2.19722, 4.40672, 2.83321, 1.60944, 4.304069999999999, 3.09104, 1.09861, 3.52636, 1.38629, 0.69315, 0.69315, 0.69315, 1.09861, 3.43399, 1.9459099999999998, 1.60944, 1.60944, 2.3979, 2.99573, 1.60944, 0.69315, 4.304069999999999, 4.20469, 5.463830000000001, 1.60944, 2.6390599999999997, 2.94444, 3.9702900000000003, 1.09861, 1.9459099999999998, 2.70805, 6.10479, 2.6390599999999997, 3.49651, 3.68888, 1.09861, 4.31749, 0.69315, 1.7917599999999998, 1.09861, 1.38629, 1.7917599999999998, 4.7706800000000005, 0.69315, 1.09861, 1.09861, 0.69315, 0.69315, 1.7917599999999998, 2.19722, 1.38629, 0.69315, 0.69315, 3.55535, 0.69315, 4.49981, 2.3025900000000004, 1.9459099999999998, 6.639880000000001, 2.07944, 1.09861, 1.09861, 1.38629, 3.1354900000000003, 5.0876, 0.69315, 1.7917599999999998, 5.05625, 1.7917599999999998, 0.69315, 2.56495, 0.69315, 1.38629, 1.09861, 2.3979, 0.69315, 1.7917599999999998, 1.09861, 2.56495, 2.07944, 2.89037, 0.69315, 3.04452, 0.69315, 1.09861, 0.69315, 2.4849099999999997, 1.7917599999999998, 1.38629, 0.69315, 2.94444, 2.99573, 0.69315, 1.38629, 0.69315, 1.09861, 3.21888, 1.09861, 4.75359, 0.69315, 0.69315, 4.21951, 2.19722, 1.9459099999999998, 3.78419, 2.77259, 0.69315, 1.38629, 2.56495, 1.9459099999999998, 5.53339, 0.69315, 5.77144, 0.69315, 1.38629, 0.69315, 0.69315, 1.09861, 0.69315, 4.21951, 2.07944, 0.69315, 2.19722, 1.9459099999999998, 1.9459099999999998, 2.70805, 0.69315, 2.07944, 2.77259, 3.55535, 1.60944, 1.09861, 0.69315, 0.69315, 1.09861, 1.09861, 0.69315, 0.69315, 1.38629, 0.69315, 1.60944, 1.7917599999999998, 2.07944, 6.09131, 2.3025900000000004, 3.78419, 2.07944, 1.9459099999999998, 3.66356, 0.69315, 1.60944, 1.7917599999999998, 0.69315, 2.77259, 1.60944, 1.38629, 1.38629, 6.13773, 1.60944, 1.38629, 0.69315, 2.4849099999999997, 1.09861, 3.17805], \"type\": \"scatter\", \"name\": \"Label\", \"mode\": \"lines\", \"x\": [\"2016-12-23 15:53:30.718000\", \"2016-12-23 16:01:01.565000\", \"2016-12-23 16:22:24.813000\", \"2016-12-23 16:46:18.982000\", \"2016-12-23 16:49:19.775000\", \"2016-12-23 16:52:58.407000\", \"2016-12-23 16:58:18.601000\", \"2016-12-23 17:01:10.853000\", \"2016-12-23 17:28:20.805000\", \"2016-12-23 17:37:43.643000\", \"2016-12-23 17:53:38.413000\", \"2016-12-23 17:56:26.242000\", \"2016-12-23 18:17:59.181000\", \"2016-12-23 18:19:16.464000\", \"2016-12-23 18:22:39.945000\", \"2016-12-23 18:26:08\", \"2016-12-23 18:38:13.888000\", \"2016-12-23 18:40:45.846000\", \"2016-12-23 19:10:46.357000\", \"2016-12-23 19:23:45.172000\", \"2016-12-23 19:29:01.670000\", \"2016-12-23 19:39:33.457000\", \"2016-12-23 19:40:53.997000\", \"2016-12-23 19:58:01.981000\", \"2016-12-23 20:04:00.106000\", \"2016-12-23 20:23:28.587000\", \"2016-12-23 20:39:51.860000\", \"2016-12-23 21:10:09.295000\", \"2016-12-23 21:25:25.159000\", \"2016-12-23 21:30:11.194000\", \"2016-12-23 21:35:36.930000\", \"2016-12-23 21:38:17.414000\", \"2016-12-23 21:54:19.476000\", \"2016-12-23 21:57:12.602000\", \"2016-12-23 22:03:51.205000\", \"2016-12-23 22:05:17.490000\", \"2016-12-23 22:11:43.077000\", \"2016-12-23 22:35:12.931000\", \"2016-12-23 22:39:40.040000\", \"2016-12-23 23:38:05.726000\", \"2016-12-23 23:38:06.062000\", \"2016-12-23 23:47:34.435000\", \"2016-12-23 23:57:53.877000\", \"2016-12-24 00:01:07.669000\", \"2016-12-24 00:05:26.151000\", \"2016-12-24 00:08:49.419000\", \"2016-12-24 00:15:53.132000\", \"2016-12-24 00:19:06.082000\", \"2016-12-24 00:26:33.146000\", \"2016-12-24 00:38:54.359000\", \"2016-12-24 00:53:51.615000\", \"2016-12-24 00:58:10.609000\", \"2016-12-24 02:06:13.271000\", \"2016-12-24 02:13:48.916000\", \"2016-12-24 02:47:03.725000\", \"2016-12-24 03:22:46.851000\", \"2016-12-24 03:36:45.364000\", \"2016-12-24 03:37:00.894000\", \"2016-12-24 03:50:27.996000\", \"2016-12-24 03:56:01.347000\", \"2016-12-24 04:07:44.752000\", \"2016-12-24 04:13:29.771000\", \"2016-12-24 04:39:27.901000\", \"2016-12-24 05:12:09.540000\", \"2016-12-24 05:17:10.088000\", \"2016-12-24 06:04:55.942000\", \"2016-12-24 06:17:10.332000\", \"2016-12-24 06:17:19.029000\", \"2016-12-24 07:29:21.748000\", \"2016-12-24 08:05:38.002000\", \"2016-12-24 08:11:01.367000\", \"2016-12-24 08:40:16.309000\", \"2016-12-24 08:40:39.604000\", \"2016-12-24 09:26:45.069000\", \"2016-12-24 10:35:53.938000\", \"2016-12-24 10:59:27.579000\", \"2016-12-24 11:21:26.862000\", \"2016-12-24 11:45:31.389000\", \"2016-12-24 12:47:47.786000\", \"2016-12-24 13:01:37.613000\", \"2016-12-24 13:03:42.031000\", \"2016-12-24 13:04:31.551000\", \"2016-12-24 13:05:46.233000\", \"2016-12-24 13:07:10.670000\", \"2016-12-24 13:20:12.217000\", \"2016-12-24 13:22:55.304000\", \"2016-12-24 13:29:57.720000\", \"2016-12-24 13:43:59.174000\", \"2016-12-24 13:47:08.209000\", \"2016-12-24 14:11:46.525000\", \"2016-12-24 14:34:48.982000\", \"2016-12-24 14:56:09.018000\", \"2016-12-24 15:03:03.805000\", \"2016-12-24 15:31:40.185000\", \"2016-12-24 15:57:51.214000\", \"2016-12-24 15:58:50.199000\", \"2016-12-24 16:01:01.540000\", \"2016-12-24 16:02:20.287000\", \"2016-12-24 16:03:45.710000\", \"2016-12-24 16:10:45.552000\", \"2016-12-24 16:28:22.363000\", \"2016-12-24 16:33:31.462000\", \"2016-12-24 16:39:54.555000\", \"2016-12-24 16:42:38.858000\", \"2016-12-24 17:35:47.354000\", \"2016-12-24 17:59:05.886000\", \"2016-12-24 18:21:13.427000\", \"2016-12-24 18:35:04.966000\", \"2016-12-24 19:01:01.737000\", \"2016-12-24 19:26:08.771000\", \"2016-12-24 20:03:32.943000\", \"2016-12-24 22:30:37.611000\", \"2016-12-25 00:54:39.256000\", \"2016-12-25 01:10:33.156000\", \"2016-12-25 01:29:55.583000\", \"2016-12-25 02:00:35\", \"2016-12-25 02:01:01.281000\", \"2016-12-25 02:36:47.459000\", \"2016-12-25 03:07:57.271000\", \"2016-12-25 03:45:35.359000\", \"2016-12-25 04:28:28.426000\", \"2016-12-25 04:45:32.030000\", \"2016-12-25 04:59:24.268000\", \"2016-12-25 05:53:44.534000\", \"2016-12-25 07:06:42.985000\", \"2016-12-25 07:43:27.635000\", \"2016-12-25 08:17:17.006000\", \"2016-12-25 08:40:14.899000\", \"2016-12-25 08:58:58.175000\", \"2016-12-25 10:47:01.541000\", \"2016-12-25 10:48:53.564000\", \"2016-12-25 10:51:14.297000\", \"2016-12-25 11:04:11.885000\", \"2016-12-25 11:22:01.003000\", \"2016-12-25 11:23:06.895000\", \"2016-12-25 13:06:35.738000\", \"2016-12-25 13:20:00.450000\", \"2016-12-25 13:20:39.624000\", \"2016-12-25 14:07:47.185000\", \"2016-12-25 14:28:02.507000\", \"2016-12-25 14:59:48.358000\", \"2016-12-25 15:30:46.848000\", \"2016-12-25 15:50:32.652000\", \"2016-12-25 16:33:57.347000\", \"2016-12-25 16:39:25.081000\", \"2016-12-25 16:44:39.622000\", \"2016-12-25 16:46:35\", \"2016-12-25 17:00:28.900000\", \"2016-12-25 17:14:27.688000\", \"2016-12-25 17:18:53.969000\", \"2016-12-25 17:25:10.509000\", \"2016-12-25 17:25:12.173000\", \"2016-12-25 18:55:42.621000\", \"2016-12-25 19:31:42.824000\", \"2016-12-25 20:25:31.956000\", \"2016-12-25 20:48:29.348000\", \"2016-12-25 20:57:53.312000\", \"2016-12-25 21:02:08.670000\", \"2016-12-25 21:05:35.551000\", \"2016-12-25 21:25:15.101000\", \"2016-12-25 21:35:09.018000\", \"2016-12-25 21:35:25.562000\", \"2016-12-25 23:31:22.500000\", \"2016-12-25 23:49:42.495000\", \"2016-12-26\", \"2016-12-26 00:21:31.175000\", \"2016-12-26 00:38:00.047000\", \"2016-12-26 01:00:05.202000\", \"2016-12-26 01:05:25.473000\", \"2016-12-26 01:48:51.935000\", \"2016-12-26 01:57:41.679000\", \"2016-12-26 02:37:29.819000\", \"2016-12-26 02:48:07.067000\", \"2016-12-26 02:57:07.649000\", \"2016-12-26 02:57:20.146000\", \"2016-12-26 02:59:58.497000\", \"2016-12-26 03:39:15.024000\", \"2016-12-26 04:35:04.722000\", \"2016-12-26 04:54:40.497000\", \"2016-12-26 04:56:17.856000\", \"2016-12-26 05:31:08.076000\", \"2016-12-26 06:09:37.064000\", \"2016-12-26 06:23:23.349000\", \"2016-12-26 06:28:45.144000\", \"2016-12-26 06:37:50.493000\", \"2016-12-26 06:55:50.031000\", \"2016-12-26 07:17:44.230000\", \"2016-12-26 07:19:55.989000\", \"2016-12-26 07:22:04.687000\", \"2016-12-26 07:27:59.865000\", \"2016-12-26 07:55:02.563000\", \"2016-12-26 08:00:22.561000\", \"2016-12-26 08:03:50.527000\", \"2016-12-26 08:22:16.742000\", \"2016-12-26 08:42:57.641000\", \"2016-12-26 08:44:53.593000\", \"2016-12-26 09:09:19.620000\", \"2016-12-26 10:16:52.874000\", \"2016-12-26 10:18:48.546000\", \"2016-12-26 11:18:19.097000\", \"2016-12-26 11:33:23.325000\", \"2016-12-26 11:33:26.244000\", \"2016-12-26 12:19:02.234000\", \"2016-12-26 12:29:18.035000\", \"2016-12-26 12:39:36.314000\", \"2016-12-26 13:01:01.557000\", \"2016-12-26 13:02:31.711000\", \"2016-12-26 13:21:03.790000\", \"2016-12-26 13:21:04.587000\", \"2016-12-26 13:37:31.998000\", \"2016-12-26 13:53:50.173000\", \"2016-12-26 13:57:17.487000\", \"2016-12-26 14:01:01.490000\", \"2016-12-26 14:07:42.244000\", \"2016-12-26 14:27:23.319000\", \"2016-12-26 14:45:28.681000\", \"2016-12-26 15:01:01.665000\", \"2016-12-26 15:01:43.347000\", \"2016-12-26 15:08:39.037000\", \"2016-12-26 15:12:55.687000\", \"2016-12-26 15:34:58.118000\", \"2016-12-26 15:38:05.408000\", \"2016-12-26 16:01:56.605000\", \"2016-12-26 16:14:14.702000\", \"2016-12-26 16:26:16.277000\", \"2016-12-26 16:32:50.694000\", \"2016-12-26 16:34:14.270000\", \"2016-12-26 16:38:13.877000\", \"2016-12-26 16:38:22.330000\", \"2016-12-26 16:51:09.117000\", \"2016-12-26 16:59:05.550000\", \"2016-12-26 17:00:33.343000\", \"2016-12-26 17:09:39.318000\", \"2016-12-26 17:16:15.874000\", \"2016-12-26 17:23:48.318000\", \"2016-12-26 17:32:19.061000\", \"2016-12-26 17:36:01.287000\", \"2016-12-26 17:47:45.283000\", \"2016-12-26 17:50:59.828000\", \"2016-12-26 17:52:45.569000\", \"2016-12-26 17:52:54.504000\", \"2016-12-26 17:54:09.147000\", \"2016-12-26 18:22:18.642000\", \"2016-12-26 18:30:55.693000\", \"2016-12-26 18:39:26.847000\", \"2016-12-26 18:53:12.883000\", \"2016-12-26 18:59:43.557000\", \"2016-12-26 19:02:36.546000\", \"2016-12-26 19:10:08.267000\", \"2016-12-26 19:14:27.728000\", \"2016-12-26 19:25:56.098000\", \"2016-12-26 19:31:09.234000\", \"2016-12-26 20:03:05.951000\", \"2016-12-26 20:19:06.019000\", \"2016-12-26 20:20:43.254000\", \"2016-12-26 20:38:58.641000\", \"2016-12-26 20:40:22.647000\", \"2016-12-26 20:42:24.307000\", \"2016-12-26 20:52:32.405000\", \"2016-12-26 21:09:07.047000\", \"2016-12-26 21:16:07.582000\", \"2016-12-26 21:38:00.223000\", \"2016-12-26 21:52:40.449000\", \"2016-12-26 22:09:05.074000\", \"2016-12-26 22:32:58.110000\", \"2016-12-26 22:34:42.467000\", \"2016-12-26 22:39:24.100000\", \"2016-12-26 22:51:08.174000\", \"2016-12-26 23:30:45.086000\", \"2016-12-26 23:31:48.388000\", \"2016-12-26 23:45:32.558000\", \"2016-12-26 23:48:55.415000\", \"2016-12-26 23:50:58.725000\", \"2016-12-27 00:10:56.507000\", \"2016-12-27 00:12:25.769000\", \"2016-12-27 00:21:07.144000\", \"2016-12-27 00:24:03.622000\", \"2016-12-27 00:26:59.103000\", \"2016-12-27 00:31:56.572000\", \"2016-12-27 00:56:50.919000\", \"2016-12-27 00:57:43.988000\", \"2016-12-27 00:59:15.112000\", \"2016-12-27 01:02:12.447000\", \"2016-12-27 01:53:35.636000\", \"2016-12-27 01:56:28.693000\", \"2016-12-27 02:10:37.541000\", \"2016-12-27 02:51:05.955000\", \"2016-12-27 03:02:21.454000\", \"2016-12-27 03:13:36.603000\", \"2016-12-27 03:41:52.699000\", \"2016-12-27 04:21:52.996000\", \"2016-12-27 05:22:46.005000\", \"2016-12-27 05:31:44.983000\", \"2016-12-27 05:34:07.339000\", \"2016-12-27 05:34:19.172000\", \"2016-12-27 05:36:39.768000\", \"2016-12-27 05:57:18.494000\", \"2016-12-27 06:20:14.361000\", \"2016-12-27 06:45:38.565000\", \"2016-12-27 07:32:05.729000\", \"2016-12-27 07:45:17.220000\", \"2016-12-27 07:50:35.605000\", \"2016-12-27 07:52:17.448000\", \"2016-12-27 08:09:50.698000\", \"2016-12-27 08:15:06.870000\", \"2016-12-27 08:17:00.248000\", \"2016-12-27 08:40:03.420000\", \"2016-12-27 08:58:18.968000\", \"2016-12-27 08:59:57.428000\", \"2016-12-27 09:11:57.488000\", \"2016-12-27 09:17:50.462000\", \"2016-12-27 09:19:52.271000\", \"2016-12-27 09:22:50.150000\", \"2016-12-27 09:24:04.851000\", \"2016-12-27 09:31:01.492000\", \"2016-12-27 09:41:00.220000\", \"2016-12-27 10:01:00.810000\", \"2016-12-27 10:16:11.886000\", \"2016-12-27 10:17:12.984000\", \"2016-12-27 10:30:09.817000\", \"2016-12-27 10:31:31.191000\", \"2016-12-27 11:10:39.581000\", \"2016-12-27 11:17:56.221000\", \"2016-12-27 11:36:32.030000\", \"2016-12-27 11:37:20.562000\", \"2016-12-27 11:41:06.393000\", \"2016-12-27 11:53:18.178000\", \"2016-12-27 12:15:29.080000\", \"2016-12-27 12:24:01.005000\", \"2016-12-27 12:38:36.671000\", \"2016-12-27 12:44:37.706000\", \"2016-12-27 12:49:48.705000\", \"2016-12-27 12:49:49.043000\", \"2016-12-27 13:00:07.343000\", \"2016-12-27 13:06:20.535000\", \"2016-12-27 13:36:19.857000\", \"2016-12-27 13:39:57.766000\", \"2016-12-27 13:45:27.238000\", \"2016-12-27 14:34:20.029000\", \"2016-12-27 14:36:23.842000\", \"2016-12-27 14:44:34.554000\", \"2016-12-27 14:50:40.296000\", \"2016-12-27 15:00:45.144000\", \"2016-12-27 15:01:01.667000\", \"2016-12-27 15:02:16.286000\", \"2016-12-27 15:13:18.097000\", \"2016-12-27 15:19:34.577000\", \"2016-12-27 15:28:47.733000\", \"2016-12-27 15:29:15.708000\", \"2016-12-27 15:44:58.981000\", \"2016-12-27 15:57:51\", \"2016-12-27 15:58:16.265000\", \"2016-12-27 16:00:36.265000\", \"2016-12-27 16:03:16.628000\", \"2016-12-27 16:05:03.228000\", \"2016-12-27 16:11:01.549000\", \"2016-12-27 16:13:38.863000\", \"2016-12-27 16:18:41.925000\", \"2016-12-27 16:22:59.453000\", \"2016-12-27 16:26:14.625000\", \"2016-12-27 16:32:39.773000\", \"2016-12-27 16:32:54.447000\", \"2016-12-27 16:51:06.965000\", \"2016-12-27 17:06:01.247000\", \"2016-12-27 17:13:11.955000\", \"2016-12-27 17:17:46.117000\", \"2016-12-27 17:21:35.418000\", \"2016-12-27 17:26:57.681000\", \"2016-12-27 17:31:29.663000\", \"2016-12-27 17:33:31.823000\", \"2016-12-27 17:39:55.090000\", \"2016-12-27 17:49:30.095000\", \"2016-12-27 17:50:03.615000\", \"2016-12-27 17:57:26.524000\", \"2016-12-27 18:00:32.782000\", \"2016-12-27 18:21:21.221000\", \"2016-12-27 18:24:39.637000\", \"2016-12-27 18:26:11.219000\", \"2016-12-27 18:57:55.257000\", \"2016-12-27 19:01:01.933000\", \"2016-12-27 19:06:01.367000\", \"2016-12-27 19:09:17.345000\", \"2016-12-27 19:16:38.349000\", \"2016-12-27 19:27:03.422000\", \"2016-12-27 19:45:18.662000\", \"2016-12-27 20:02:02.480000\", \"2016-12-27 20:06:09.511000\", \"2016-12-27 20:14:24.124000\", \"2016-12-27 20:18:20.553000\", \"2016-12-27 20:19:46.777000\", \"2016-12-27 20:34:33.344000\", \"2016-12-27 21:11:39.893000\", \"2016-12-27 21:13:19.574000\", \"2016-12-27 21:28:09.806000\", \"2016-12-27 21:50:04.874000\", \"2016-12-27 21:50:09.009000\", \"2016-12-27 21:56:47.213000\", \"2016-12-27 21:57:50.674000\", \"2016-12-27 22:06:10.192000\", \"2016-12-27 22:11:47.947000\", \"2016-12-27 22:15:27.411000\", \"2016-12-27 22:20:19.100000\", \"2016-12-27 22:35:43.612000\", \"2016-12-27 22:35:46.495000\", \"2016-12-27 22:43:36.707000\", \"2016-12-27 23:01:01.395000\", \"2016-12-27 23:07:05.130000\", \"2016-12-27 23:09:28.107000\", \"2016-12-27 23:09:59.658000\", \"2016-12-27 23:10:32.779000\", \"2016-12-27 23:19:26.314000\", \"2016-12-27 23:32:13.220000\", \"2016-12-27 23:51:47.548000\", \"2016-12-28 00:13:24.219000\", \"2016-12-28 00:29:32.608000\", \"2016-12-28 00:37:13.109000\", \"2016-12-28 00:41:56.860000\", \"2016-12-28 01:01:01.660000\", \"2016-12-28 01:23:59.111000\", \"2016-12-28 01:27:03.530000\", \"2016-12-28 01:29:03.860000\", \"2016-12-28 01:39:43.215000\", \"2016-12-28 01:48:38.683000\", \"2016-12-28 01:51:00\", \"2016-12-28 01:55:14.359000\", \"2016-12-28 02:23:48.555000\", \"2016-12-28 02:56:07.156000\", \"2016-12-28 03:01:01.669000\", \"2016-12-28 03:26:04.344000\", \"2016-12-28 03:29:18.178000\", \"2016-12-28 03:37:43.371000\", \"2016-12-28 03:44:00.822000\", \"2016-12-28 03:44:25.645000\", \"2016-12-28 03:49:58.404000\", \"2016-12-28 03:53:06.033000\", \"2016-12-28 04:19:10.608000\", \"2016-12-28 04:21:08.021000\", \"2016-12-28 04:33:59.286000\", \"2016-12-28 04:46:06.926000\", \"2016-12-28 04:57:24.595000\", \"2016-12-28 05:15:34.533000\", \"2016-12-28 05:34:45.101000\", \"2016-12-28 05:40:29.287000\", \"2016-12-28 05:50:47.216000\", \"2016-12-28 05:51:15.388000\", \"2016-12-28 06:05:16.719000\", \"2016-12-28 06:06:34.389000\", \"2016-12-28 06:21:01.645000\", \"2016-12-28 06:29:20.191000\", \"2016-12-28 06:33:14\", \"2016-12-28 06:41:05.636000\", \"2016-12-28 06:41:36.878000\", \"2016-12-28 07:00:44.698000\", \"2016-12-28 07:04:57.531000\", \"2016-12-28 07:12:01.850000\", \"2016-12-28 07:31:43.567000\", \"2016-12-28 07:33:53.575000\", \"2016-12-28 07:35:43.120000\", \"2016-12-28 07:52:18.878000\", \"2016-12-28 08:03:39.732000\", \"2016-12-28 08:17:18.171000\", \"2016-12-28 08:20:21.759000\", \"2016-12-28 08:25:42.916000\", \"2016-12-28 08:34:46.026000\", \"2016-12-28 08:38:54.337000\", \"2016-12-28 08:44:01.827000\", \"2016-12-28 09:04:56.095000\", \"2016-12-28 09:31:15.812000\", \"2016-12-28 09:35:36.281000\", \"2016-12-28 09:57:24.762000\", \"2016-12-28 10:06:16.031000\", \"2016-12-28 10:10:33.506000\", \"2016-12-28 10:12:46.334000\", \"2016-12-28 10:16:01.149000\", \"2016-12-28 10:39:04.865000\", \"2016-12-28 11:08:32.163000\", \"2016-12-28 11:17:05.106000\", \"2016-12-28 11:25:44.148000\", \"2016-12-28 11:36:59.576000\", \"2016-12-28 11:38:55.244000\", \"2016-12-28 11:43:02.230000\", \"2016-12-28 12:04:37.801000\", \"2016-12-28 12:04:49.895000\", \"2016-12-28 12:29:11.151000\", \"2016-12-28 12:29:35.442000\", \"2016-12-28 12:34:09.930000\", \"2016-12-28 12:46:07.547000\", \"2016-12-28 12:50:07.671000\", \"2016-12-28 12:51:01.572000\", \"2016-12-28 13:07:31.291000\", \"2016-12-28 13:22:06.465000\", \"2016-12-28 13:23:05.109000\", \"2016-12-28 13:27:48.878000\", \"2016-12-28 13:30:26.480000\", \"2016-12-28 13:32:24.273000\", \"2016-12-28 13:46:23.063000\", \"2016-12-28 13:52:34.395000\", \"2016-12-28 14:05:22.217000\", \"2016-12-28 14:08:48.218000\", \"2016-12-28 14:10:17.193000\", \"2016-12-28 14:14:43.629000\", \"2016-12-28 14:16:42.628000\", \"2016-12-28 14:22:08.088000\", \"2016-12-28 14:28:57.481000\", \"2016-12-28 14:32:49.656000\", \"2016-12-28 14:41:18.505000\", \"2016-12-28 14:47:42.050000\", \"2016-12-28 15:24:51.100000\", \"2016-12-28 15:26:10.273000\", \"2016-12-28 15:33:42.176000\", \"2016-12-28 15:39:20.180000\", \"2016-12-28 15:41:18.227000\", \"2016-12-28 15:43:51.482000\", \"2016-12-28 15:48:35.941000\", \"2016-12-28 15:49:18.688000\", \"2016-12-28 15:51:39.588000\", \"2016-12-28 15:52:46.486000\", \"2016-12-28 16:04:08.196000\", \"2016-12-28 16:07:36.760000\", \"2016-12-28 16:13:41.686000\", \"2016-12-28 16:22:29.472000\", \"2016-12-28 16:28:15.579000\", \"2016-12-28 16:43:07.160000\", \"2016-12-28 17:09:15.707000\", \"2016-12-28 17:10:14.131000\", \"2016-12-28 17:33:07.680000\", \"2016-12-28 17:34:39.247000\", \"2016-12-28 17:35:17.852000\", \"2016-12-28 17:41:41.184000\", \"2016-12-28 18:04:39.292000\", \"2016-12-28 18:05:39.765000\", \"2016-12-28 18:16:41.424000\", \"2016-12-28 18:23:05.465000\", \"2016-12-28 18:30:43.179000\", \"2016-12-28 18:31:04.625000\", \"2016-12-28 18:35:13.076000\", \"2016-12-28 18:44:14.579000\", \"2016-12-28 18:47:00.188000\", \"2016-12-28 19:01:01.476000\", \"2016-12-28 19:06:46.216000\", \"2016-12-28 19:10:19.755000\", \"2016-12-28 19:12:58.363000\", \"2016-12-28 19:19:50.705000\", \"2016-12-28 19:32:36.585000\", \"2016-12-28 19:33:57.863000\", \"2016-12-28 19:41:21.141000\", \"2016-12-28 19:59:18.840000\", \"2016-12-28 20:07:53.242000\", \"2016-12-28 20:09:52.559000\", \"2016-12-28 20:15:21.649000\", \"2016-12-28 20:25:57.082000\", \"2016-12-28 20:26:35.300000\", \"2016-12-28 20:42:07.314000\", \"2016-12-28 20:50:46.968000\", \"2016-12-28 21:07:16.547000\", \"2016-12-28 21:08:50.776000\", \"2016-12-28 21:27:25\", \"2016-12-28 21:28:40.897000\", \"2016-12-28 21:33:50.344000\", \"2016-12-28 21:36:51.324000\", \"2016-12-28 21:40:17.142000\", \"2016-12-28 21:44:56\", \"2016-12-28 21:55:39.135000\", \"2016-12-28 22:00:50.630000\", \"2016-12-28 22:05:17.323000\", \"2016-12-28 22:12:07.710000\", \"2016-12-28 22:15:27.638000\", \"2016-12-28 22:16:13.779000\", \"2016-12-28 22:21:37.827000\", \"2016-12-28 22:34:25.333000\", \"2016-12-28 22:45:18.090000\", \"2016-12-28 22:46:05.596000\", \"2016-12-28 22:51:01.400000\", \"2016-12-28 22:54:45.823000\", \"2016-12-28 22:55:33.931000\", \"2016-12-28 23:03:21.456000\", \"2016-12-28 23:32:41.720000\", \"2016-12-28 23:41:42.869000\", \"2016-12-28 23:59:46.258000\", \"2016-12-29\", \"2016-12-29 00:00:31.579000\", \"2016-12-29 00:34:18.429000\", \"2016-12-29 00:36:56.428000\", \"2016-12-29 00:49:18.921000\", \"2016-12-29 01:07:44.100000\", \"2016-12-29 01:42:15.520000\", \"2016-12-29 02:01:01.460000\", \"2016-12-29 02:44:21.762000\", \"2016-12-29 03:08:41.077000\", \"2016-12-29 03:12:27.413000\", \"2016-12-29 03:21:48.894000\", \"2016-12-29 03:30:42.819000\", \"2016-12-29 03:46:13.334000\", \"2016-12-29 04:02:06.870000\", \"2016-12-29 04:07:09.509000\", \"2016-12-29 04:21:51.261000\", \"2016-12-29 04:27:15.836000\", \"2016-12-29 04:30:22.597000\", \"2016-12-29 04:31:25.017000\", \"2016-12-29 04:34:51.464000\", \"2016-12-29 04:58:48.920000\", \"2016-12-29 05:07:10\", \"2016-12-29 05:23:50.483000\", \"2016-12-29 05:25:28.416000\", \"2016-12-29 06:02:15.988000\", \"2016-12-29 06:04:47.114000\", \"2016-12-29 06:22:13.709000\", \"2016-12-29 07:14:38.454000\", \"2016-12-29 07:16:39.292000\", \"2016-12-29 07:29:05.287000\", \"2016-12-29 07:29:20.709000\", \"2016-12-29 09:11:19.132000\", \"2016-12-29 09:18:29.526000\", \"2016-12-29 09:29:41.795000\", \"2016-12-29 09:55:33.418000\", \"2016-12-29 10:14:43.116000\", \"2016-12-29 10:25:58.814000\", \"2016-12-29 10:36:27.125000\", \"2016-12-29 10:45:18.841000\", \"2016-12-29 11:02:36.465000\", \"2016-12-29 11:04:47.104000\", \"2016-12-29 11:12:18.160000\", \"2016-12-29 11:14:40.721000\", \"2016-12-29 11:43:39.203000\", \"2016-12-29 11:47:50.820000\", \"2016-12-29 11:52:19.828000\", \"2016-12-29 12:05:26.038000\", \"2016-12-29 12:08:10.174000\", \"2016-12-29 12:25:58.606000\", \"2016-12-29 12:31:00\", \"2016-12-29 12:36:49.894000\", \"2016-12-29 12:54:24.328000\", \"2016-12-29 13:00:00\", \"2016-12-29 13:19:07.957000\", \"2016-12-29 13:23:10\", \"2016-12-29 13:25:47.217000\", \"2016-12-29 13:47:27.267000\", \"2016-12-29 13:47:45.003000\", \"2016-12-29 14:01:01.710000\", \"2016-12-29 14:04:22.212000\", \"2016-12-29 14:08:58.140000\", \"2016-12-29 14:11:42.766000\", \"2016-12-29 14:26:01.423000\", \"2016-12-29 14:31:07.959000\", \"2016-12-29 14:31:55.394000\", \"2016-12-29 14:35:06.301000\", \"2016-12-29 14:37:12.173000\", \"2016-12-29 14:46:47.128000\", \"2016-12-29 14:48:41.731000\", \"2016-12-29 14:55:43.134000\", \"2016-12-29 15:00:10\", \"2016-12-29 15:00:28.101000\", \"2016-12-29 15:01:01.595000\", \"2016-12-29 15:01:01.605000\", \"2016-12-29 15:01:40.762000\", \"2016-12-29 15:05:39.080000\", \"2016-12-29 15:08:42.843000\", \"2016-12-29 15:16:40.770000\", \"2016-12-29 15:35:50.221000\", \"2016-12-29 15:42:53.174000\", \"2016-12-29 15:45:07.824000\", \"2016-12-29 16:06:01.985000\", \"2016-12-29 16:15:59.635000\", \"2016-12-29 16:23:25.839000\", \"2016-12-29 16:25:05\", \"2016-12-29 16:30:43.257000\", \"2016-12-29 16:33:07.765000\", \"2016-12-29 16:34:30.986000\", \"2016-12-29 16:44:40.105000\", \"2016-12-29 16:47:44.744000\", \"2016-12-29 16:48:18.333000\", \"2016-12-29 16:49:02.612000\", \"2016-12-29 16:52:03.192000\", \"2016-12-29 16:56:20.935000\", \"2016-12-29 17:01:01.377000\", \"2016-12-29 17:18:16.308000\", \"2016-12-29 17:23:32.461000\", \"2016-12-29 17:25:00.830000\", \"2016-12-29 17:34:31.438000\", \"2016-12-29 17:40:59.606000\", \"2016-12-29 17:41:01.496000\", \"2016-12-29 17:46:54.338000\", \"2016-12-29 17:58:54.092000\", \"2016-12-29 18:08:04.788000\", \"2016-12-29 18:10:08.172000\", \"2016-12-29 18:12:15.791000\", \"2016-12-29 18:15:04.697000\", \"2016-12-29 18:16:06.138000\", \"2016-12-29 18:16:24.860000\", \"2016-12-29 18:30:00.471000\", \"2016-12-29 18:32:13.788000\", \"2016-12-29 18:36:34.542000\", \"2016-12-29 18:51:15.579000\", \"2016-12-29 18:55:52.317000\", \"2016-12-29 19:02:32.474000\", \"2016-12-29 19:02:43.051000\", \"2016-12-29 19:10:18.184000\", \"2016-12-29 19:11:33.446000\", \"2016-12-29 19:27:36.343000\", \"2016-12-29 19:42:01.659000\", \"2016-12-29 19:42:33.005000\", \"2016-12-29 19:46:43.945000\", \"2016-12-29 19:48:27.965000\", \"2016-12-29 19:50:34.237000\", \"2016-12-29 19:53:29.281000\", \"2016-12-29 19:55:27.819000\", \"2016-12-29 19:58:36.809000\", \"2016-12-29 20:21:26.149000\", \"2016-12-29 20:29:26.506000\", \"2016-12-29 20:40:27.834000\", \"2016-12-29 20:46:18.379000\", \"2016-12-29 20:46:25.053000\", \"2016-12-29 20:56:50.666000\", \"2016-12-29 21:23:00.562000\", \"2016-12-29 21:24:36.111000\", \"2016-12-29 21:27:42.537000\", \"2016-12-29 21:37:30.820000\", \"2016-12-29 22:02:28.956000\", \"2016-12-29 22:26:02.330000\", \"2016-12-29 22:38:59.486000\", \"2016-12-29 22:49:26.276000\", \"2016-12-29 22:55:20.911000\", \"2016-12-29 23:09:28.832000\", \"2016-12-29 23:13:54.502000\", \"2016-12-29 23:28:26.912000\", \"2016-12-29 23:33:04.678000\", \"2016-12-29 23:35:54.027000\", \"2016-12-29 23:43:52.232000\", \"2016-12-30\", \"2016-12-30 00:07:45.976000\", \"2016-12-30 00:15:44.080000\", \"2016-12-30 00:35:40.782000\", \"2016-12-30 00:36:03.961000\", \"2016-12-30 00:51:43.870000\", \"2016-12-30 01:04:17.799000\", \"2016-12-30 01:16:29.549000\", \"2016-12-30 01:16:34.417000\", \"2016-12-30 01:25:48.749000\", \"2016-12-30 01:48:48.459000\", \"2016-12-30 02:03:48.380000\", \"2016-12-30 02:20:54.168000\", \"2016-12-30 02:35:12.742000\", \"2016-12-30 02:35:21.907000\", \"2016-12-30 02:35:23.380000\", \"2016-12-30 02:41:40.048000\", \"2016-12-30 03:12:26.662000\", \"2016-12-30 04:59:09.866000\", \"2016-12-30 05:08:49.714000\", \"2016-12-30 05:09:44.937000\", \"2016-12-30 05:17:48.548000\", \"2016-12-30 05:38:03.834000\", \"2016-12-30 05:39:45.764000\", \"2016-12-30 05:55:24.223000\", \"2016-12-30 06:05:55.939000\", \"2016-12-30 06:09:35.775000\", \"2016-12-30 06:23:00.922000\", \"2016-12-30 06:41:22.485000\", \"2016-12-30 06:51:29\", \"2016-12-30 06:58:55.588000\", \"2016-12-30 07:17:55.365000\", \"2016-12-30 07:19:58.934000\", \"2016-12-30 07:27:23.653000\", \"2016-12-30 07:34:27.879000\", \"2016-12-30 07:48:41.792000\", \"2016-12-30 08:06:27.929000\", \"2016-12-30 08:16:24.143000\", \"2016-12-30 08:25:29.348000\", \"2016-12-30 09:16:03.929000\", \"2016-12-30 09:34:38.852000\", \"2016-12-30 09:36:02.740000\", \"2016-12-30 09:47:33.987000\", \"2016-12-30 09:56:07\", \"2016-12-30 09:57:30.992000\", \"2016-12-30 10:07:12.573000\", \"2016-12-30 10:41:28.397000\", \"2016-12-30 11:06:08.895000\", \"2016-12-30 11:22:45.655000\", \"2016-12-30 11:26:51.213000\", \"2016-12-30 11:35:48.575000\", \"2016-12-30 11:39:03.484000\", \"2016-12-30 11:49:03.072000\", \"2016-12-30 11:52:03.382000\", \"2016-12-30 12:01:35.448000\", \"2016-12-30 12:44:18.534000\", \"2016-12-30 12:46:23\", \"2016-12-30 12:59:20.144000\", \"2016-12-30 13:07:27.965000\", \"2016-12-30 13:10:36.360000\", \"2016-12-30 13:13:44.283000\", \"2016-12-30 13:18:30.169000\", \"2016-12-30 13:18:48.104000\", \"2016-12-30 13:26:10.240000\", \"2016-12-30 13:27:43.890000\", \"2016-12-30 13:29:07.907000\", \"2016-12-30 13:29:45.175000\", \"2016-12-30 13:31:01.619000\", \"2016-12-30 13:31:02.351000\", \"2016-12-30 13:44:40.740000\", \"2016-12-30 13:53:00.132000\", \"2016-12-30 13:59:40.624000\", \"2016-12-30 14:00:35.175000\", \"2016-12-30 14:31:01.720000\", \"2016-12-30 14:35:53.690000\", \"2016-12-30 14:36:20.589000\", \"2016-12-30 14:52:59.871000\", \"2016-12-30 15:01:02.380000\", \"2016-12-30 15:05:07.509000\", \"2016-12-30 15:14:34.823000\", \"2016-12-30 15:17:28.727000\", \"2016-12-30 15:25:42.789000\", \"2016-12-30 15:34:45.005000\", \"2016-12-30 15:45:42.006000\", \"2016-12-30 15:48:04.684000\", \"2016-12-30 15:52:38.360000\", \"2016-12-30 16:07:04.628000\", \"2016-12-30 16:07:37.269000\", \"2016-12-30 16:12:07.412000\", \"2016-12-30 16:15:04.657000\", \"2016-12-30 16:18:43.101000\", \"2016-12-30 16:36:52.008000\", \"2016-12-30 16:38:11.413000\", \"2016-12-30 16:39:15.346000\", \"2016-12-30 17:05:45.721000\", \"2016-12-30 17:10:38.888000\", \"2016-12-30 17:21:30.774000\", \"2016-12-30 17:22:30.625000\", \"2016-12-30 17:30:11.934000\", \"2016-12-30 17:43:42\", \"2016-12-30 17:44:00.028000\", \"2016-12-30 17:50:00.505000\", \"2016-12-30 17:51:57.233000\", \"2016-12-30 17:54:32.217000\", \"2016-12-30 18:02:16.846000\", \"2016-12-30 18:04:09.465000\", \"2016-12-30 18:11:24.969000\", \"2016-12-30 18:14:17.342000\", \"2016-12-30 18:22:02.476000\", \"2016-12-30 18:30:19.634000\", \"2016-12-30 18:33:18.083000\", \"2016-12-30 18:36:47.455000\", \"2016-12-30 18:36:54.008000\", \"2016-12-30 18:39:34.862000\", \"2016-12-30 18:44:16.084000\", \"2016-12-30 18:54:40.834000\", \"2016-12-30 19:11:17.762000\", \"2016-12-30 19:15:38.041000\", \"2016-12-30 19:24:48.902000\", \"2016-12-30 19:24:51.518000\", \"2016-12-30 19:25:54.446000\", \"2016-12-30 19:25:59.206000\", \"2016-12-30 19:31:01.315000\", \"2016-12-30 19:39:21.884000\", \"2016-12-30 20:29:25.392000\", \"2016-12-30 20:38:07.759000\", \"2016-12-30 20:39:07.243000\", \"2016-12-30 20:49:04.016000\", \"2016-12-30 20:53:25.981000\", \"2016-12-30 21:02:52.260000\", \"2016-12-30 21:10:19.869000\", \"2016-12-30 21:10:22.047000\", \"2016-12-30 21:35:46.214000\", \"2016-12-30 21:43:44.431000\", \"2016-12-30 21:48:51.285000\", \"2016-12-30 22:08:21.619000\", \"2016-12-30 22:21:27.664000\", \"2016-12-30 22:46:52.845000\", \"2016-12-30 22:53:06.466000\", \"2016-12-30 22:53:11.166000\", \"2016-12-30 23:02:42\", \"2016-12-30 23:07:33.314000\", \"2016-12-30 23:18:24.581000\", \"2016-12-30 23:18:53.055000\", \"2016-12-30 23:30:36.206000\", \"2016-12-30 23:30:44.951000\", \"2016-12-30 23:32:03.275000\", \"2016-12-30 23:37:44.312000\", \"2016-12-31\", \"2016-12-31 00:16:00.449000\", \"2016-12-31 00:59:35.994000\", \"2016-12-31 01:05:49.844000\", \"2016-12-31 01:15:11.165000\", \"2016-12-31 01:15:21.110000\", \"2016-12-31 01:20:40.484000\", \"2016-12-31 01:27:38.427000\", \"2016-12-31 01:34:37.391000\", \"2016-12-31 01:36:10.738000\", \"2016-12-31 01:41:38.406000\", \"2016-12-31 01:44:31.551000\", \"2016-12-31 01:52:06.917000\", \"2016-12-31 02:14:45.216000\", \"2016-12-31 02:25:15.734000\", \"2016-12-31 02:25:17.804000\", \"2016-12-31 02:26:01.645000\", \"2016-12-31 02:28:09.446000\", \"2016-12-31 02:48:26.366000\", \"2016-12-31 03:15:39.532000\", \"2016-12-31 03:23:56.347000\", \"2016-12-31 03:39:29.385000\", \"2016-12-31 03:57:47.827000\", \"2016-12-31 04:20:22.594000\", \"2016-12-31 05:23:50.521000\", \"2016-12-31 05:32:25.928000\", \"2016-12-31 06:02:46.346000\", \"2016-12-31 06:07:31.666000\", \"2016-12-31 06:30:36.581000\", \"2016-12-31 07:01:54.837000\", \"2016-12-31 07:27:03.009000\", \"2016-12-31 07:33:50.782000\", \"2016-12-31 08:33:14.663000\", \"2016-12-31 08:35:54.878000\", \"2016-12-31 08:42:20.755000\", \"2016-12-31 08:49:20.096000\", \"2016-12-31 09:01:12.394000\", \"2016-12-31 09:29:22\", \"2016-12-31 09:30:38.655000\", \"2016-12-31 09:50:30.018000\", \"2016-12-31 09:52:56.434000\", \"2016-12-31 10:16:48\", \"2016-12-31 10:31:44.891000\", \"2016-12-31 10:38:03.139000\", \"2016-12-31 10:40:25.801000\", \"2016-12-31 10:59:12.842000\", \"2016-12-31 11:11:01.612000\", \"2016-12-31 11:20:38.254000\", \"2016-12-31 12:10:56.702000\", \"2016-12-31 12:29:09.298000\", \"2016-12-31 12:31:30.436000\", \"2016-12-31 12:34:52.132000\", \"2016-12-31 12:48:56.159000\", \"2016-12-31 12:52:50.350000\", \"2016-12-31 13:00:13.725000\", \"2016-12-31 13:07:17.982000\", \"2016-12-31 13:26:01.826000\", \"2016-12-31 13:27:08.788000\", \"2016-12-31 13:31:37.069000\", \"2016-12-31 13:45:09.009000\", \"2016-12-31 13:47:12.077000\", \"2016-12-31 14:12:43.784000\", \"2016-12-31 14:19:05.910000\", \"2016-12-31 14:34:08.651000\", \"2016-12-31 14:36:15.281000\", \"2016-12-31 15:04:48.099000\", \"2016-12-31 15:08:51.718000\", \"2016-12-31 15:12:49.852000\", \"2016-12-31 15:29:50.145000\", \"2016-12-31 15:40:58.070000\", \"2016-12-31 15:44:31.622000\", \"2016-12-31 15:49:21.283000\", \"2016-12-31 15:59:22.555000\", \"2016-12-31 15:59:43\", \"2016-12-31 16:00:24.193000\", \"2016-12-31 16:14:31.495000\", \"2016-12-31 16:18:13.639000\", \"2016-12-31 16:19:55.030000\", \"2016-12-31 16:21:55.452000\", \"2016-12-31 16:27:07.111000\", \"2016-12-31 16:50:26.789000\", \"2016-12-31 16:57:57.153000\", \"2016-12-31 17:01:21.594000\", \"2016-12-31 17:10:49.625000\", \"2016-12-31 17:27:59.556000\", \"2016-12-31 17:48:58.509000\", \"2016-12-31 17:52:56.198000\", \"2016-12-31 17:59:33.607000\", \"2016-12-31 17:59:45.181000\", \"2016-12-31 18:05:38.646000\", \"2016-12-31 18:15:51.817000\", \"2016-12-31 18:23:46.585000\", \"2016-12-31 18:24:54.730000\", \"2016-12-31 18:44:46.446000\", \"2016-12-31 18:52:32\", \"2016-12-31 18:53:04.218000\", \"2016-12-31 19:01:25.706000\", \"2016-12-31 19:08:29.050000\", \"2016-12-31 19:20:57.095000\", \"2016-12-31 19:26:03.079000\", \"2016-12-31 19:53:45.481000\", \"2016-12-31 19:57:21.154000\", \"2016-12-31 20:01:01.703000\", \"2016-12-31 20:01:18.371000\", \"2016-12-31 20:17:40.369000\", \"2016-12-31 20:21:36.634000\", \"2016-12-31 20:22:43.406000\", \"2016-12-31 20:35:19.450000\", \"2016-12-31 20:36:25.249000\", \"2016-12-31 20:41:38.396000\", \"2016-12-31 20:52:00.717000\", \"2016-12-31 21:03:49.339000\", \"2016-12-31 21:11:16.192000\", \"2016-12-31 21:33:25.495000\", \"2016-12-31 21:34:27.564000\", \"2016-12-31 21:51:01.507000\", \"2016-12-31 21:58:43.265000\", \"2016-12-31 22:15:22.537000\", \"2016-12-31 22:17:45.433000\", \"2016-12-31 22:25:12.360000\", \"2016-12-31 23:04:27.063000\", \"2016-12-31 23:13:08.790000\", \"2016-12-31 23:26:55.147000\", \"2016-12-31 23:51:28.020000\"]}], {\"title\": \"Online users\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": false})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotly_df(df1, title = \"Online users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train into part_train and part_valid\n",
    "\n",
    "!split -l 36889 $PATH_TO_DATA/$OUT_TRAIN $PATH_TO_DATA/$OUT_TRAIN\"_\"\n",
    "\n",
    "!mv $PATH_TO_DATA/$OUT_TRAIN\"_aa\" $PATH_TO_DATA/$OUT_PART_TRAIN\n",
    "!mv $PATH_TO_DATA/$OUT_TRAIN\"_ab\" $PATH_TO_DATA/$OUT_PART_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from validation target file \n",
    "\n",
    "!cut -f 1 -d ' ' $PATH_TO_DATA/$OUT_PART_VALID > $PATH_TO_DATA/part_valid_target.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for a namespaces.\n",
      "using l2 regularization = 1e-05\n",
      "final_regressor = ../../data/arktur_medium//model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/arktur_medium//medium.cache\n",
      "Reading datafile = ../../data/arktur_medium//part_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "3.735672 3.735672            1            1.0   1.9328   0.0000     1178\n",
      "2.180715 0.625757            2            2.0   1.7891   0.9980      849\n",
      "1.720227 1.259739            4            4.0   0.8340   0.1097      405\n",
      "1.073640 0.427052            8            8.0   1.6958   1.9328     4483\n",
      "1.009323 0.945007           16           16.0   0.3266   0.3884      705\n",
      "0.692505 0.375686           32           32.0   1.3425   1.7541     1499\n",
      "0.541030 0.389556           64           64.0   0.0940   0.1249       97\n",
      "0.542049 0.543068          128          128.0   1.5040   1.1355     1596\n",
      "0.452265 0.362482          256          256.0   0.9962   0.8376     2857\n",
      "0.379711 0.307158          512          512.0   0.0940   0.1502      546\n",
      "0.412766 0.445820         1024         1024.0   1.7346   1.2158     2956\n",
      "0.417961 0.423157         2048         2048.0   1.2429   1.3687      992\n",
      "0.405256 0.392551         4096         4096.0   0.7321   0.5220     1455\n",
      "0.396764 0.388271         8192         8192.0   0.8340   0.4335      825\n",
      "0.393169 0.389575        16384        16384.0   1.4215   1.9023     2602\n",
      "0.393184 0.393199        32768        32768.0   1.3589   0.2605       24\n",
      "0.363163 0.363163        65536        65536.0   0.6657   0.4959       37 h\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# !rm $PATH_TO_DATA/medium.cache \n",
    "!vw -d $PATH_TO_DATA/$OUT_PART_TRAIN -k --cache_file=$PATH_TO_DATA/medium.cache \\\n",
    "  --loss_function squared --ngram=a2 --passes 5 -b 26 -l 1 --power_t=0.5 --l2=1e-5 \\\n",
    "  -f $PATH_TO_DATA/model.vw #--readable_model=$PATH_TO_DATA/readable_model.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50900557937563662"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict for validation and MAE\n",
    "\n",
    "!vw -i $PATH_TO_DATA/model.vw -t -d $PATH_TO_DATA/$OUT_PART_VALID \\\n",
    "-p $PATH_TO_DATA/part_valid_predictions.txt --quiet\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = np.loadtxt(PATH_TO_DATA + '/part_valid_target.txt')\n",
    "y_pred = np.loadtxt(PATH_TO_DATA + '/part_valid_predictions.txt')\n",
    "\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15308976943 squared_b_20_l_0.5_t_0.5.csv\n",
      "1.15625028783 at_squared_b_21_l_0.5_t_0.5.csv\n",
      "1.14395825558 log_na2_p5_b26_l_1_p05_l2_1e5.csv\n",
      "1.12767829627 log_abef_n2_p5_b24_l05_p05_l2_1e5.csv\n",
      "1.16862095356 squared_b_21_l_0.5_t_0.5_l2_1e-5.csv\n",
      "1.1935572218 na2_p5_b26_l_1_p05_l2_1e5.csv\n",
      "1.14225266476 squared_b_28_l_0.43_t_0.17.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = np.loadtxt(PATH_TO_DATA + '/test_target.txt')\n",
    "for filename in os.listdir(PATH_TO_DATA + \"/submit\"):\n",
    "    if filename.startswith(\"etha\"): continue\n",
    "    y_target = pd.read_csv(PATH_TO_DATA + '/submit/' + filename)\n",
    "    y_pred = y_target[\"log_recommends\"]\n",
    "\n",
    "    print(mean_absolute_error(y_true, y_pred), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Arktur | File| Features | VW Params | MAE |\n",
    "|------|------|------|------|------|\n",
    "| 0.89639 | squared_b_20_l_0.5_t_0.5.csv | content | --loss_function squared --ngram 2 --passes 3 -b 20 -l 0.5 --power_t 0.5 | |\n",
    "| 0.88511 | at_squared_b_21_l_0.5_t_0.5.csv | no punctuation, author, title, content | --loss_function squared --ngram 2 --passes 3 -b 21 -l 0.5 --power_t 0.5 | 1.047 |\n",
    "| not| not | no pun, author, title, content, month, dayofweek, hour | --loss_function squared --ngram 2 --passes 3 -b 21 -l 0.5 --power_t 0.5 | 1.027 |\n",
    "| not| not | no pun-scale, author, title, content, month, dayofweek, hour | --loss_function squared --ngram 2 --passes 3 -b 21 -l 0.5 --power_t 0.5 | 1.030 |\n",
    "| 0.88297 | squared_b_21_l_0.5_t_0.5_l2_1e-5.csv | nonpun, author, title, content, month, dayofweek, hour | --loss_function squared --ngram=2 --passes 3 -b 21 -l 0.5 --power_t 0.5 --l2=1e-5 | 1.019 |\n",
    "| 0.90253 | squared_b_21_l_0.7_t_0.5_l2_1e-5_new_features.csv | nonpun, author, title, content, month, dayofweek, hour, bin, domain | --loss_function squared --ngram=2 --passes 3 -b 21 -l 0.7 --power_t 0.5 --l2=1e-5 | 1.016 |\n",
    "| 0.89520 | squared_b_21_l_0.5_t_0.5_new_features.csv | nonpun, author, title, content, month, dayofweek, hour, bin, domain | --loss_function squared --ngram=2 --passes 3 -b 21 -l 0.5 --power_t 0.5 | 1.026 |\n",
    "| 0.85777 | g_3_p_5_b_21_l_0.5_t_0.5_l1_lower.csv | nonpun, low, content, author, title, domain, month, dayofweek, hour, bin | --loss_function squared --ngram=3 --passes 5 -b 21 -l 0.5 --power_t 0.5 --l2=1e-5 | 1.014 |\n",
    "| 0.85671 | g_3_p_5_b_21_l_0.5_t_0.5_l1_lower.csv | nonpun, low, content, author, title, domain, month, dayofweek, hour| --loss_function squared --ngram=3 --passes 5 -b 21 -l 0.5 --power_t 0.5 --l2=1e-5 | 1.014 |\n",
    "| 0.85415 | filtered_content_n2_p5_b28_l0.5_p0.5_l21e-5.csv | f_content | --loss_function squared --ngram=2 --passes 5 -b 28 -l 0.5 --power_t=0.5 --l2=1e-5 | 0.984 |\n",
    "| 0.82853 | lema_content_n2_p5_b24_l0.5_p0.5_l21e-5.csv | l_content | --loss_function squared --ngram=2 --passes 5 -b 24 -l 0.5 --power_t=0.5 --l2=1e-5 | 0.9813 |\n",
    "| 0.69766 | log_lema_features_n2_p5_b24_l0.5_p0.5_l21e-5.csv | log_lema_features | --loss_function squared --ngram=2 --passes 5 -b 24 -l 0.5 --power_t=0.5 --l2=1e-5 | log 0.4161 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "using l2 regularization = 1e-05\n",
      "final_regressor = ../../data/arktur_medium//model.vw\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/arktur_medium//medium.cache\n",
      "Reading datafile = ../../data/arktur_medium//train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "3.735672 3.735672            1            1.0   1.9328   0.0000     1181\n",
      "2.181843 0.628015            2            2.0   1.7891   0.9966      851\n",
      "1.717136 1.252428            4            4.0   0.8340   0.1174      409\n",
      "1.067019 0.416902            8            8.0   1.6958   1.9328     4489\n",
      "0.992346 0.917673           16           16.0   0.3266   0.3991      709\n",
      "0.680501 0.368656           32           32.0   1.3425   1.7396     1505\n",
      "0.534910 0.389320           64           64.0   0.0940   0.1465      101\n",
      "0.533513 0.532116          128          128.0   1.5040   1.1060     1603\n",
      "0.442231 0.350948          256          256.0   0.9962   0.7893     2863\n",
      "0.372177 0.302124          512          512.0   0.0940   0.1922      551\n",
      "0.405460 0.438743         1024         1024.0   1.7346   1.4022     2961\n",
      "0.410752 0.416045         2048         2048.0   1.2429   1.3580      995\n",
      "0.399066 0.387380         4096         4096.0   0.7321   0.6154     1459\n",
      "0.390826 0.382585         8192         8192.0   0.8340   0.4550      829\n",
      "0.388459 0.386093        16384        16384.0   1.4215   1.8227     2605\n",
      "0.389570 0.390680        32768        32768.0   1.3589   0.2624       25\n",
      "0.368459 0.368459        65536        65536.0   0.9102   1.7746     3499 h\n",
      "0.362656 0.356855       131072       131072.0   1.4013   1.1991      255 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 47430\n",
      "passes used = 5\n",
      "weighted example sum = 237150.000000\n",
      "weighted label sum = 145199.925622\n",
      "average loss = 0.349193 h\n",
      "best constant = 0.612270\n",
      "total feature number = 271035945\n"
     ]
    }
   ],
   "source": [
    "# !rm $PATH_TO_DATA/medium.cache \n",
    "!vw -d $PATH_TO_DATA/$OUT_TRAIN -k --cache_file $PATH_TO_DATA/medium.cache \\\n",
    "  --loss_function squared --ngram=2 --passes 5 -b 24 -l 0.5 --power_t=0.5 --l2=1e-5 \\\n",
    "  -f $PATH_TO_DATA/model.vw #--readable_model=$PATH_TO_DATA/readable_model.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0129379e8e40c88fdc0a7a490a1c47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create test data\n",
    "\n",
    "import codecs\n",
    "\n",
    "with codecs.open(os.path.join(PATH_TO_DATA, INP_TEST), encoding=\"utf-8\") as inp_json, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, INP_TEST_PARSED), encoding=\"utf-8\") as inp_parsed, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, \"test.csv\"), 'w', encoding=\"utf-8\") as out_test:\n",
    "    N = 39492        \n",
    "    header = False\n",
    "    \n",
    "    for n, (json_, parsed) in enumerate(tqdm_notebook(zip(inp_json, inp_parsed), total=N)):\n",
    "        if n == N: break\n",
    "        if not header:\n",
    "            out_test.write(\"Time,Label\\n\")\n",
    "            header = True\n",
    "\n",
    "        json_data = json.loads(json_)\n",
    "        url, content, tags = parsed.split(\",\")\n",
    "        tags = tags.strip()\n",
    "        \n",
    "#         out = to_vw1(1, content, tags, json_data)\n",
    "        out = to_csv(1, json_data)\n",
    "        out_test.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict for test\n",
    "\n",
    "!vw -i $PATH_TO_DATA/model.vw -t -d $PATH_TO_DATA/test.vw \\\n",
    "-p $PATH_TO_DATA/test_predictions.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create submission\n",
    "\n",
    "submission = pd.read_csv(PATH_TO_DATA + '/sample_submission.csv', index_col='id')\n",
    "if LOG:\n",
    "    submission['log_recommends'] = np.exp(np.loadtxt(PATH_TO_DATA + '/test_predictions.txt'))\n",
    "else:\n",
    "    submission['log_recommends'] = np.loadtxt(PATH_TO_DATA + '/test_predictions.txt')\n",
    "submission.to_csv(PATH_TO_DATA + '/submit/log_abdefg_n2_p5_b24_l05_p05_l2_1e5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 11:26:12,443 INFO     [root/vw-hyperopt:250]: loading true holdout class labels...\n",
      "2017-11-01 11:26:12,604 INFO     [root/vw-hyperopt:257]: holdout length: 15810\n",
      "2017-11-01 11:26:12,606 DEBUG    [root/vw-hyperopt:320]: starting hypersearch...\n",
      "2017-11-01 11:26:12,623 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.004462 seconds\n",
      "2017-11-01 11:26:12,627 INFO     [hyperopt.tpe/tpe:844]: TPE using 0 trials\n",
      "2017-11-01 11:26:12,632 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.1\n",
      "2017-11-01 11:26:12,641 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 2.293110126098825 --l2 4.394402312218596e-07 --l1 1.0884538019468025e-06 --ngram 2 --passes 4 --loss_function squared --power_t 0.15312831696127777 --bit_precision 25 \n",
      "2017-11-01 11:27:06,630 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:27:07,816 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 2.293110126098825 --l2 4.394402312218596e-07 --l1 1.0884538019468025e-06 --ngram 2 --passes 4 --loss_function squared --power_t 0.15312831696127777 --bit_precision 25 \n",
      "2017-11-01 11:27:07,819 INFO     [root/vw-hyperopt:284]: loss value: 0.306683\n",
      "2017-11-01 11:27:07,825 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:55.192362\n",
      "2017-11-01 11:27:07,954 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.015032 seconds\n",
      "2017-11-01 11:27:07,957 INFO     [hyperopt.tpe/tpe:842]: TPE using 1/1 trials with best loss 0.306683\n",
      "2017-11-01 11:27:07,963 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.2\n",
      "2017-11-01 11:27:07,967 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.011876485160767872 --l2 3.645645777723749e-08 --ngram 2 --passes 5 --loss_function squared --power_t 0.5007211571688552 --bit_precision 25 \n",
      "2017-11-01 11:28:25,199 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:28:26,057 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.011876485160767872 --l2 3.645645777723749e-08 --ngram 2 --passes 5 --loss_function squared --power_t 0.5007211571688552 --bit_precision 25 \n",
      "2017-11-01 11:28:26,059 INFO     [root/vw-hyperopt:284]: loss value: 0.274261\n",
      "2017-11-01 11:28:26,075 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:18.111713\n",
      "2017-11-01 11:28:26,222 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.008144 seconds\n",
      "2017-11-01 11:28:26,225 INFO     [hyperopt.tpe/tpe:842]: TPE using 2/2 trials with best loss 0.274261\n",
      "2017-11-01 11:28:26,232 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.3\n",
      "2017-11-01 11:28:26,234 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.4389095453827498 --ngram 2 --passes 8 --loss_function squared --power_t 0.5350673877791368 --bit_precision 25 \n",
      "2017-11-01 11:30:10,803 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:30:11,421 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.4389095453827498 --ngram 2 --passes 8 --loss_function squared --power_t 0.5350673877791368 --bit_precision 25 \n",
      "2017-11-01 11:30:11,423 INFO     [root/vw-hyperopt:284]: loss value: 0.304560\n",
      "2017-11-01 11:30:11,426 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:45.194435\n",
      "2017-11-01 11:30:11,556 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005437 seconds\n",
      "2017-11-01 11:30:11,557 INFO     [hyperopt.tpe/tpe:842]: TPE using 3/3 trials with best loss 0.274261\n",
      "2017-11-01 11:30:11,562 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.4\n",
      "2017-11-01 11:30:11,568 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.0150430081059378 --l2 2.4737993124714356e-06 --ngram 2 --passes 10 --loss_function squared --power_t 0.5301057060593999 --bit_precision 25 \n",
      "2017-11-01 11:32:26,737 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:32:27,370 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.0150430081059378 --l2 2.4737993124714356e-06 --ngram 2 --passes 10 --loss_function squared --power_t 0.5301057060593999 --bit_precision 25 \n",
      "2017-11-01 11:32:27,373 INFO     [root/vw-hyperopt:284]: loss value: 0.282851\n",
      "2017-11-01 11:32:27,377 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:15.814708\n",
      "2017-11-01 11:32:27,515 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005406 seconds\n",
      "2017-11-01 11:32:27,517 INFO     [hyperopt.tpe/tpe:842]: TPE using 4/4 trials with best loss 0.274261\n",
      "2017-11-01 11:32:27,521 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.5\n",
      "2017-11-01 11:32:27,522 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 7.847237899058886 --l2 0.0010240366655101315 --ngram 2 --passes 2 --loss_function squared --power_t 0.7201509665944924 --bit_precision 25 \n",
      "2017-11-01 11:33:04,224 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:33:04,818 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 7.847237899058886 --l2 0.0010240366655101315 --ngram 2 --passes 2 --loss_function squared --power_t 0.7201509665944924 --bit_precision 25 \n",
      "2017-11-01 11:33:04,820 INFO     [root/vw-hyperopt:284]: loss value: 0.267883\n",
      "2017-11-01 11:33:04,822 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:37.300624\n",
      "2017-11-01 11:33:04,988 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005527 seconds\n",
      "2017-11-01 11:33:04,990 INFO     [hyperopt.tpe/tpe:842]: TPE using 5/5 trials with best loss 0.267883\n",
      "2017-11-01 11:33:04,994 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.6\n",
      "2017-11-01 11:33:04,995 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.2085375984363987 --l2 4.806733274542402e-07 --l1 1.0216984098345993e-07 --ngram 2 --passes 9 --loss_function squared --power_t 0.5981516613997975 --bit_precision 25 \n",
      "2017-11-01 11:35:00,566 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:35:01,096 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.2085375984363987 --l2 4.806733274542402e-07 --l1 1.0216984098345993e-07 --ngram 2 --passes 9 --loss_function squared --power_t 0.5981516613997975 --bit_precision 25 \n",
      "2017-11-01 11:35:01,098 INFO     [root/vw-hyperopt:284]: loss value: 0.287999\n",
      "2017-11-01 11:35:01,108 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:56.114525\n",
      "2017-11-01 11:35:01,258 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005382 seconds\n",
      "2017-11-01 11:35:01,259 INFO     [hyperopt.tpe/tpe:842]: TPE using 6/6 trials with best loss 0.267883\n",
      "2017-11-01 11:35:01,266 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.7\n",
      "2017-11-01 11:35:01,267 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.294758659621406 --l2 0.00194707552590083 --l1 1.3980930204923765e-08 --ngram 2 --passes 4 --loss_function squared --power_t 0.9794242693622698 --bit_precision 25 \n",
      "2017-11-01 11:36:07,255 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 11:36:07,833 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.294758659621406 --l2 0.00194707552590083 --l1 1.3980930204923765e-08 --ngram 2 --passes 4 --loss_function squared --power_t 0.9794242693622698 --bit_precision 25 \n",
      "2017-11-01 11:36:07,836 INFO     [root/vw-hyperopt:284]: loss value: 0.244926\n",
      "2017-11-01 11:36:07,838 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:06.571576\n",
      "2017-11-01 11:36:07,964 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005542 seconds\n",
      "2017-11-01 11:36:07,966 INFO     [hyperopt.tpe/tpe:842]: TPE using 7/7 trials with best loss 0.244926\n",
      "2017-11-01 11:36:07,970 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.8\n",
      "2017-11-01 11:36:07,972 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.7353818623871349 --l1 4.535829279609697e-06 --ngram 2 --passes 2 --loss_function squared --power_t 0.020465817756499728 --bit_precision 25 \n",
      "2017-11-01 11:36:30,706 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:36:31,093 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.7353818623871349 --l1 4.535829279609697e-06 --ngram 2 --passes 2 --loss_function squared --power_t 0.020465817756499728 --bit_precision 25 \n",
      "2017-11-01 11:36:31,094 INFO     [root/vw-hyperopt:284]: loss value: 0.337506\n",
      "2017-11-01 11:36:31,096 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:23.125684\n",
      "2017-11-01 11:36:31,236 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006243 seconds\n",
      "2017-11-01 11:36:31,238 INFO     [hyperopt.tpe/tpe:842]: TPE using 8/8 trials with best loss 0.244926\n",
      "2017-11-01 11:36:31,243 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.9\n",
      "2017-11-01 11:36:31,248 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.0664227379293589 --l1 0.0018469016836109894 --ngram 2 --passes 3 --loss_function squared --power_t 0.953948279927446 --bit_precision 25 \n",
      "2017-11-01 11:37:07,940 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:37:08,461 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.0664227379293589 --l1 0.0018469016836109894 --ngram 2 --passes 3 --loss_function squared --power_t 0.953948279927446 --bit_precision 25 \n",
      "2017-11-01 11:37:08,464 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 11:37:08,467 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:37.223762\n",
      "2017-11-01 11:37:08,593 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006091 seconds\n",
      "2017-11-01 11:37:08,596 INFO     [hyperopt.tpe/tpe:842]: TPE using 9/9 trials with best loss 0.244926\n",
      "2017-11-01 11:37:08,602 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.10\n",
      "2017-11-01 11:37:08,605 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.4232520648942337 --l2 1.9464702740793615e-08 --ngram 2 --passes 3 --loss_function squared --power_t 0.4413971913362967 --bit_precision 25 \n",
      "2017-11-01 11:38:02,210 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:38:03,565 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.4232520648942337 --l2 1.9464702740793615e-08 --ngram 2 --passes 3 --loss_function squared --power_t 0.4413971913362967 --bit_precision 25 \n",
      "2017-11-01 11:38:03,570 INFO     [root/vw-hyperopt:284]: loss value: 0.330292\n",
      "2017-11-01 11:38:03,576 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:54.973977\n",
      "2017-11-01 11:38:03,771 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006720 seconds\n",
      "2017-11-01 11:38:03,779 INFO     [hyperopt.tpe/tpe:842]: TPE using 10/10 trials with best loss 0.244926\n",
      "2017-11-01 11:38:03,793 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.11\n",
      "2017-11-01 11:38:03,799 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.6532456222319654 --l2 2.1178488031044623e-07 --ngram 2 --passes 6 --loss_function squared --power_t 0.33101849887806456 --bit_precision 25 \n",
      "2017-11-01 11:39:33,580 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:39:34,826 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.6532456222319654 --l2 2.1178488031044623e-07 --ngram 2 --passes 6 --loss_function squared --power_t 0.33101849887806456 --bit_precision 25 \n",
      "2017-11-01 11:39:34,833 INFO     [root/vw-hyperopt:284]: loss value: 0.328500\n",
      "2017-11-01 11:39:34,835 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:31.042051\n",
      "2017-11-01 11:39:34,988 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006707 seconds\n",
      "2017-11-01 11:39:34,990 INFO     [hyperopt.tpe/tpe:842]: TPE using 11/11 trials with best loss 0.244926\n",
      "2017-11-01 11:39:34,997 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.12\n",
      "2017-11-01 11:39:35,000 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.3305433230493281 --l2 2.899193790696676e-05 --l1 6.120291022746614e-05 --ngram 2 --passes 8 --loss_function squared --power_t 0.6401065055580105 --bit_precision 25 \n",
      "2017-11-01 11:41:24,709 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:41:25,565 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.3305433230493281 --l2 2.899193790696676e-05 --l1 6.120291022746614e-05 --ngram 2 --passes 8 --loss_function squared --power_t 0.6401065055580105 --bit_precision 25 \n",
      "2017-11-01 11:41:25,575 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 11:41:25,585 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:50.588142\n",
      "2017-11-01 11:41:25,737 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007016 seconds\n",
      "2017-11-01 11:41:25,742 INFO     [hyperopt.tpe/tpe:842]: TPE using 12/12 trials with best loss 0.244926\n",
      "2017-11-01 11:41:25,765 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.13\n",
      "2017-11-01 11:41:25,770 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.08192697434428864 --l2 0.05849197403663761 --l1 5.769890849901068e-06 --ngram 2 --passes 9 --loss_function squared --power_t 0.5420068439103137 --bit_precision 25 \n",
      "2017-11-01 11:43:47,201 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:43:48,124 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.08192697434428864 --l2 0.05849197403663761 --l1 5.769890849901068e-06 --ngram 2 --passes 9 --loss_function squared --power_t 0.5420068439103137 --bit_precision 25 \n",
      "2017-11-01 11:43:48,133 INFO     [root/vw-hyperopt:284]: loss value: 0.242033\n",
      "2017-11-01 11:43:48,134 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:22.369327\n",
      "2017-11-01 11:43:48,350 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006510 seconds\n",
      "2017-11-01 11:43:48,352 INFO     [hyperopt.tpe/tpe:842]: TPE using 13/13 trials with best loss 0.242033\n",
      "2017-11-01 11:43:48,363 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.14\n",
      "2017-11-01 11:43:48,364 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.03488895272181752 --ngram 2 --passes 2 --loss_function squared --power_t 0.13995598650314572 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 11:44:30,583 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:44:31,772 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.03488895272181752 --ngram 2 --passes 2 --loss_function squared --power_t 0.13995598650314572 --bit_precision 25 \n",
      "2017-11-01 11:44:31,787 INFO     [root/vw-hyperopt:284]: loss value: 0.269393\n",
      "2017-11-01 11:44:31,789 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:43.426290\n",
      "2017-11-01 11:44:31,966 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.010433 seconds\n",
      "2017-11-01 11:44:31,970 INFO     [hyperopt.tpe/tpe:842]: TPE using 14/14 trials with best loss 0.242033\n",
      "2017-11-01 11:44:32,007 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.15\n",
      "2017-11-01 11:44:32,013 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 7.587952300997423 --l2 1.0556303069401517e-07 --l1 0.015514986488004286 --ngram 2 --passes 8 --loss_function squared --power_t 0.3345149807758149 --bit_precision 25 \n",
      "2017-11-01 11:46:27,131 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:46:27,997 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 7.587952300997423 --l2 1.0556303069401517e-07 --l1 0.015514986488004286 --ngram 2 --passes 8 --loss_function squared --power_t 0.3345149807758149 --bit_precision 25 \n",
      "2017-11-01 11:46:28,001 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 11:46:28,009 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:56.002612\n",
      "2017-11-01 11:46:28,221 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.014369 seconds\n",
      "2017-11-01 11:46:28,225 INFO     [hyperopt.tpe/tpe:842]: TPE using 15/15 trials with best loss 0.242033\n",
      "2017-11-01 11:46:28,232 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.16\n",
      "2017-11-01 11:46:28,240 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.41592362878638595 --l2 0.0031432694144127686 --ngram 2 --passes 9 --loss_function squared --power_t 0.71799020656777 --bit_precision 25 \n",
      "2017-11-01 11:48:48,420 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:48:49,514 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.41592362878638595 --l2 0.0031432694144127686 --ngram 2 --passes 9 --loss_function squared --power_t 0.71799020656777 --bit_precision 25 \n",
      "2017-11-01 11:48:49,517 INFO     [root/vw-hyperopt:284]: loss value: 0.222078\n",
      "2017-11-01 11:48:49,522 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:21.288699\n",
      "2017-11-01 11:48:49,724 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.009758 seconds\n",
      "2017-11-01 11:48:49,730 INFO     [hyperopt.tpe/tpe:842]: TPE using 16/16 trials with best loss 0.222078\n",
      "2017-11-01 11:48:49,737 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.17\n",
      "2017-11-01 11:48:49,742 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.03422553956793596 --l2 2.6228604756296547e-08 --ngram 2 --passes 6 --loss_function squared --power_t 0.7485886285396572 --bit_precision 25 \n",
      "2017-11-01 11:50:20,076 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:50:20,723 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.03422553956793596 --l2 2.6228604756296547e-08 --ngram 2 --passes 6 --loss_function squared --power_t 0.7485886285396572 --bit_precision 25 \n",
      "2017-11-01 11:50:20,736 INFO     [root/vw-hyperopt:284]: loss value: 0.280389\n",
      "2017-11-01 11:50:20,744 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:31.007038\n",
      "2017-11-01 11:50:20,898 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005687 seconds\n",
      "2017-11-01 11:50:20,900 INFO     [hyperopt.tpe/tpe:842]: TPE using 17/17 trials with best loss 0.222078\n",
      "2017-11-01 11:50:20,909 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.18\n",
      "2017-11-01 11:50:20,911 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.0975419106514124 --ngram 2 --passes 7 --loss_function squared --power_t 0.3931883659372686 --bit_precision 25 \n",
      "2017-11-01 11:51:46,707 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:51:47,451 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.0975419106514124 --ngram 2 --passes 7 --loss_function squared --power_t 0.3931883659372686 --bit_precision 25 \n",
      "2017-11-01 11:51:47,457 INFO     [root/vw-hyperopt:284]: loss value: 0.356968\n",
      "2017-11-01 11:51:47,458 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:26.549416\n",
      "2017-11-01 11:51:47,594 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006480 seconds\n",
      "2017-11-01 11:51:47,597 INFO     [hyperopt.tpe/tpe:842]: TPE using 18/18 trials with best loss 0.222078\n",
      "2017-11-01 11:51:47,604 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.19\n",
      "2017-11-01 11:51:47,628 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.024018500974103168 --l2 2.4048823201323425e-07 --l1 4.8239134914351626e-05 --ngram 2 --passes 10 --loss_function squared --power_t 0.3497753404532826 --bit_precision 25 \n",
      "2017-11-01 11:53:54,679 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:53:55,109 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.024018500974103168 --l2 2.4048823201323425e-07 --l1 4.8239134914351626e-05 --ngram 2 --passes 10 --loss_function squared --power_t 0.3497753404532826 --bit_precision 25 \n",
      "2017-11-01 11:53:55,111 INFO     [root/vw-hyperopt:284]: loss value: 0.222362\n",
      "2017-11-01 11:53:55,113 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:07.509264\n",
      "2017-11-01 11:53:55,258 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.011995 seconds\n",
      "2017-11-01 11:53:55,262 INFO     [hyperopt.tpe/tpe:842]: TPE using 19/19 trials with best loss 0.222078\n",
      "2017-11-01 11:53:55,271 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.20\n",
      "2017-11-01 11:53:55,274 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 5.497503636120276 --l2 0.07542932628758015 --ngram 2 --passes 7 --loss_function squared --power_t 0.9724746138207664 --bit_precision 25 \n",
      "2017-11-01 11:55:47,093 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:55:47,823 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 5.497503636120276 --l2 0.07542932628758015 --ngram 2 --passes 7 --loss_function squared --power_t 0.9724746138207664 --bit_precision 25 \n",
      "2017-11-01 11:55:47,847 INFO     [root/vw-hyperopt:284]: loss value: 0.303760\n",
      "2017-11-01 11:55:47,850 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:52.578263\n",
      "2017-11-01 11:55:47,979 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006592 seconds\n",
      "2017-11-01 11:55:47,982 INFO     [hyperopt.tpe/tpe:842]: TPE using 20/20 trials with best loss 0.222078\n",
      "2017-11-01 11:55:48,017 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.21\n",
      "2017-11-01 11:55:48,019 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.1258544581289467 --l2 0.0021794519770999554 --l1 0.0003054781983812668 --ngram 2 --passes 10 --loss_function squared --power_t 0.7609731161979919 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 11:58:03,128 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 11:58:03,565 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.1258544581289467 --l2 0.0021794519770999554 --l1 0.0003054781983812668 --ngram 2 --passes 10 --loss_function squared --power_t 0.7609731161979919 --bit_precision 25 \n",
      "2017-11-01 11:58:03,567 INFO     [root/vw-hyperopt:284]: loss value: 0.271779\n",
      "2017-11-01 11:58:03,570 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:15.552367\n",
      "2017-11-01 11:58:03,682 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006547 seconds\n",
      "2017-11-01 11:58:03,684 INFO     [hyperopt.tpe/tpe:842]: TPE using 21/21 trials with best loss 0.222078\n",
      "2017-11-01 11:58:03,705 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.22\n",
      "2017-11-01 11:58:03,708 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 3.3043752701203264 --l2 3.1466669500886764e-05 --l1 0.007426497341595218 --ngram 2 --passes 10 --loss_function squared --power_t 0.8547865689301662 --bit_precision 25 \n",
      "2017-11-01 12:00:17,543 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:00:17,959 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 3.3043752701203264 --l2 3.1466669500886764e-05 --l1 0.007426497341595218 --ngram 2 --passes 10 --loss_function squared --power_t 0.8547865689301662 --bit_precision 25 \n",
      "2017-11-01 12:00:17,961 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:00:17,973 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:14.267672\n",
      "2017-11-01 12:00:18,107 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005647 seconds\n",
      "2017-11-01 12:00:18,109 INFO     [hyperopt.tpe/tpe:842]: TPE using 22/22 trials with best loss 0.222078\n",
      "2017-11-01 12:00:18,144 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.23\n",
      "2017-11-01 12:00:18,154 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.18725080255903118 --l2 0.0002926528695193215 --l1 0.061311112340448025 --ngram 2 --passes 9 --loss_function squared --power_t 0.289468733879439 --bit_precision 25 \n",
      "2017-11-01 12:02:17,760 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:02:18,212 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.18725080255903118 --l2 0.0002926528695193215 --l1 0.061311112340448025 --ngram 2 --passes 9 --loss_function squared --power_t 0.289468733879439 --bit_precision 25 \n",
      "2017-11-01 12:02:18,214 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:02:18,217 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:00.072656\n",
      "2017-11-01 12:02:18,362 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007139 seconds\n",
      "2017-11-01 12:02:18,366 INFO     [hyperopt.tpe/tpe:842]: TPE using 23/23 trials with best loss 0.222078\n",
      "2017-11-01 12:02:18,384 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.24\n",
      "2017-11-01 12:02:18,386 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.023447607926853173 --l2 0.012950492151889616 --ngram 2 --passes 9 --loss_function squared --power_t 0.2213965084135578 --bit_precision 25 \n",
      "2017-11-01 12:04:23,441 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:04:24,198 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.023447607926853173 --l2 0.012950492151889616 --ngram 2 --passes 9 --loss_function squared --power_t 0.2213965084135578 --bit_precision 25 \n",
      "2017-11-01 12:04:24,200 INFO     [root/vw-hyperopt:284]: loss value: 0.222822\n",
      "2017-11-01 12:04:24,202 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:05.818388\n",
      "2017-11-01 12:04:24,348 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007742 seconds\n",
      "2017-11-01 12:04:24,350 INFO     [hyperopt.tpe/tpe:842]: TPE using 24/24 trials with best loss 0.222078\n",
      "2017-11-01 12:04:24,370 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.25\n",
      "2017-11-01 12:04:24,373 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.665061096252687 --l2 7.154712954417446e-06 --ngram 2 --passes 7 --loss_function squared --power_t 0.013345734951957067 --bit_precision 25 \n",
      "2017-11-01 12:05:51,763 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:05:52,558 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.665061096252687 --l2 7.154712954417446e-06 --ngram 2 --passes 7 --loss_function squared --power_t 0.013345734951957067 --bit_precision 25 \n",
      "2017-11-01 12:05:52,561 INFO     [root/vw-hyperopt:284]: loss value: 0.406918\n",
      "2017-11-01 12:05:52,562 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:28.192261\n",
      "2017-11-01 12:05:52,708 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005614 seconds\n",
      "2017-11-01 12:05:52,710 INFO     [hyperopt.tpe/tpe:842]: TPE using 25/25 trials with best loss 0.222078\n",
      "2017-11-01 12:05:52,730 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.26\n",
      "2017-11-01 12:05:52,732 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.10734200668839051 --l1 1.3248424555747733e-08 --ngram 2 --passes 10 --loss_function squared --power_t 0.8555692915651236 --bit_precision 25 \n",
      "2017-11-01 12:07:58,529 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:07:59,168 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.10734200668839051 --l1 1.3248424555747733e-08 --ngram 2 --passes 10 --loss_function squared --power_t 0.8555692915651236 --bit_precision 25 \n",
      "2017-11-01 12:07:59,170 INFO     [root/vw-hyperopt:284]: loss value: 0.262168\n",
      "2017-11-01 12:07:59,171 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:06.440882\n",
      "2017-11-01 12:07:59,336 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.013148 seconds\n",
      "2017-11-01 12:07:59,339 INFO     [hyperopt.tpe/tpe:842]: TPE using 26/26 trials with best loss 0.222078\n",
      "2017-11-01 12:07:59,367 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.27\n",
      "2017-11-01 12:07:59,373 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.05027365368663084 --l2 0.0001946048953632522 --ngram 2 --passes 8 --loss_function squared --power_t 0.6535629201656481 --bit_precision 25 \n",
      "2017-11-01 12:09:52,082 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:09:52,771 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.05027365368663084 --l2 0.0001946048953632522 --ngram 2 --passes 8 --loss_function squared --power_t 0.6535629201656481 --bit_precision 25 \n",
      "2017-11-01 12:09:52,773 INFO     [root/vw-hyperopt:284]: loss value: 0.270657\n",
      "2017-11-01 12:09:52,774 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:53.407628\n",
      "2017-11-01 12:09:52,913 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006949 seconds\n",
      "2017-11-01 12:09:52,916 INFO     [hyperopt.tpe/tpe:842]: TPE using 27/27 trials with best loss 0.222078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 12:09:52,935 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.28\n",
      "2017-11-01 12:09:52,937 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.19894874997088893 --l2 0.009018642389801285 --l1 0.0003914241279528883 --ngram 2 --passes 9 --loss_function squared --power_t 0.8698646197922101 --bit_precision 25 \n",
      "2017-11-01 12:11:42,465 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:11:42,822 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.19894874997088893 --l2 0.009018642389801285 --l1 0.0003914241279528883 --ngram 2 --passes 9 --loss_function squared --power_t 0.8698646197922101 --bit_precision 25 \n",
      "2017-11-01 12:11:42,824 INFO     [root/vw-hyperopt:284]: loss value: 0.319035\n",
      "2017-11-01 12:11:42,828 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:49.892195\n",
      "2017-11-01 12:11:42,930 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005505 seconds\n",
      "2017-11-01 12:11:42,932 INFO     [hyperopt.tpe/tpe:842]: TPE using 28/28 trials with best loss 0.222078\n",
      "2017-11-01 12:11:42,949 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.29\n",
      "2017-11-01 12:11:42,951 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 3.2254060247881813 --l2 2.684030142051249e-06 --l1 2.1871876118690943e-07 --ngram 2 --passes 10 --loss_function squared --power_t 0.09288943165847807 --bit_precision 25 \n",
      "2017-11-01 12:13:36,355 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:13:37,431 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 3.2254060247881813 --l2 2.684030142051249e-06 --l1 2.1871876118690943e-07 --ngram 2 --passes 10 --loss_function squared --power_t 0.09288943165847807 --bit_precision 25 \n",
      "2017-11-01 12:13:37,455 INFO     [root/vw-hyperopt:284]: loss value: 0.416733\n",
      "2017-11-01 12:13:37,458 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:54.508821\n",
      "2017-11-01 12:13:37,602 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006363 seconds\n",
      "2017-11-01 12:13:37,611 INFO     [hyperopt.tpe/tpe:842]: TPE using 29/29 trials with best loss 0.222078\n",
      "2017-11-01 12:13:37,630 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.30\n",
      "2017-11-01 12:13:37,634 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.010253784994926726 --ngram 2 --passes 5 --loss_function squared --power_t 0.4278889732017267 --bit_precision 25 \n",
      "2017-11-01 12:14:58,223 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:14:59,371 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.010253784994926726 --ngram 2 --passes 5 --loss_function squared --power_t 0.4278889732017267 --bit_precision 25 \n",
      "2017-11-01 12:14:59,373 INFO     [root/vw-hyperopt:284]: loss value: 0.275266\n",
      "2017-11-01 12:14:59,384 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:21.753502\n",
      "2017-11-01 12:14:59,626 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005372 seconds\n",
      "2017-11-01 12:14:59,630 INFO     [hyperopt.tpe/tpe:842]: TPE using 30/30 trials with best loss 0.222078\n",
      "2017-11-01 12:14:59,661 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.31\n",
      "2017-11-01 12:14:59,662 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.7549880526320939 --l2 0.020336308670620716 --l1 4.403406677600157e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.2555443179387362 --bit_precision 25 \n",
      "2017-11-01 12:16:48,965 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:16:50,066 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.7549880526320939 --l2 0.020336308670620716 --l1 4.403406677600157e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.2555443179387362 --bit_precision 25 \n",
      "2017-11-01 12:16:50,075 INFO     [root/vw-hyperopt:284]: loss value: 0.270871\n",
      "2017-11-01 12:16:50,077 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:50.416250\n",
      "2017-11-01 12:16:50,316 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.018820 seconds\n",
      "2017-11-01 12:16:50,320 INFO     [hyperopt.tpe/tpe:842]: TPE using 31/31 trials with best loss 0.222078\n",
      "2017-11-01 12:16:50,353 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.32\n",
      "2017-11-01 12:16:50,374 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.018825697783766217 --l2 0.00013303768390044373 --ngram 2 --passes 6 --loss_function squared --power_t 0.5972970335804252 --bit_precision 25 \n",
      "2017-11-01 12:18:19,552 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:18:20,805 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.018825697783766217 --l2 0.00013303768390044373 --ngram 2 --passes 6 --loss_function squared --power_t 0.5972970335804252 --bit_precision 25 \n",
      "2017-11-01 12:18:20,834 INFO     [root/vw-hyperopt:284]: loss value: 0.271584\n",
      "2017-11-01 12:18:20,836 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:30.483582\n",
      "2017-11-01 12:18:21,087 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005928 seconds\n",
      "2017-11-01 12:18:21,094 INFO     [hyperopt.tpe/tpe:842]: TPE using 32/32 trials with best loss 0.222078\n",
      "2017-11-01 12:18:21,162 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.33\n",
      "2017-11-01 12:18:21,168 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.13501293575446327 --l2 5.736484890226781e-06 --ngram 2 --passes 8 --loss_function squared --power_t 0.17421827964671754 --bit_precision 25 \n",
      "2017-11-01 12:20:11,593 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:20:12,766 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.13501293575446327 --l2 5.736484890226781e-06 --ngram 2 --passes 8 --loss_function squared --power_t 0.17421827964671754 --bit_precision 25 \n",
      "2017-11-01 12:20:12,774 INFO     [root/vw-hyperopt:284]: loss value: 0.318945\n",
      "2017-11-01 12:20:12,801 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:51.638487\n",
      "2017-11-01 12:20:13,012 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.018833 seconds\n",
      "2017-11-01 12:20:13,021 INFO     [hyperopt.tpe/tpe:842]: TPE using 33/33 trials with best loss 0.222078\n",
      "2017-11-01 12:20:13,050 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.34\n",
      "2017-11-01 12:20:13,052 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.2810665255449206 --l1 9.063362609004528e-08 --ngram 2 --passes 4 --loss_function squared --power_t 0.4643810094170593 --bit_precision 25 \n",
      "2017-11-01 12:21:11,844 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:21:12,890 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.2810665255449206 --l1 9.063362609004528e-08 --ngram 2 --passes 4 --loss_function squared --power_t 0.4643810094170593 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 12:21:12,892 INFO     [root/vw-hyperopt:284]: loss value: 0.315857\n",
      "2017-11-01 12:21:12,895 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:59.844249\n",
      "2017-11-01 12:21:13,108 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.013966 seconds\n",
      "2017-11-01 12:21:13,112 INFO     [hyperopt.tpe/tpe:842]: TPE using 34/34 trials with best loss 0.222078\n",
      "2017-11-01 12:21:13,140 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.35\n",
      "2017-11-01 12:21:13,151 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.571599509973544 --l2 0.0007448234328553491 --ngram 2 --passes 9 --loss_function squared --power_t 0.6854020078604 --bit_precision 25 \n",
      "2017-11-01 12:23:20,253 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:23:21,587 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.571599509973544 --l2 0.0007448234328553491 --ngram 2 --passes 9 --loss_function squared --power_t 0.6854020078604 --bit_precision 25 \n",
      "2017-11-01 12:23:21,604 INFO     [root/vw-hyperopt:284]: loss value: 0.262833\n",
      "2017-11-01 12:23:21,613 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:08.473399\n",
      "2017-11-01 12:23:21,848 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005283 seconds\n",
      "2017-11-01 12:23:21,853 INFO     [hyperopt.tpe/tpe:842]: TPE using 35/35 trials with best loss 0.222078\n",
      "2017-11-01 12:23:21,885 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.36\n",
      "2017-11-01 12:23:21,890 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.839903127894228 --l2 0.004763660907614437 --ngram 2 --passes 10 --loss_function squared --power_t 0.36599890842774746 --bit_precision 25 \n",
      "2017-11-01 12:25:47,700 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:25:48,309 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.839903127894228 --l2 0.004763660907614437 --ngram 2 --passes 10 --loss_function squared --power_t 0.36599890842774746 --bit_precision 25 \n",
      "2017-11-01 12:25:48,311 INFO     [root/vw-hyperopt:284]: loss value: 0.265000\n",
      "2017-11-01 12:25:48,312 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:26.427310\n",
      "2017-11-01 12:25:48,456 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005243 seconds\n",
      "2017-11-01 12:25:48,459 INFO     [hyperopt.tpe/tpe:842]: TPE using 36/36 trials with best loss 0.222078\n",
      "2017-11-01 12:25:48,478 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.37\n",
      "2017-11-01 12:25:48,481 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.014545276267847707 --l2 6.58575070798802e-07 --l1 0.07639858812347039 --ngram 2 --passes 1 --loss_function squared --power_t 0.5022661785292187 --bit_precision 25 \n",
      "2017-11-01 12:26:00,413 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:26:00,782 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.014545276267847707 --l2 6.58575070798802e-07 --l1 0.07639858812347039 --ngram 2 --passes 1 --loss_function squared --power_t 0.5022661785292187 --bit_precision 25 \n",
      "2017-11-01 12:26:00,783 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:26:00,785 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:12.306428\n",
      "2017-11-01 12:26:00,905 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006130 seconds\n",
      "2017-11-01 12:26:00,907 INFO     [hyperopt.tpe/tpe:842]: TPE using 37/37 trials with best loss 0.222078\n",
      "2017-11-01 12:26:00,933 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.38\n",
      "2017-11-01 12:26:00,935 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.1200160720096037 --l1 6.534705664075285e-06 --ngram 2 --passes 8 --loss_function squared --power_t 0.5731051279139144 --bit_precision 25 \n",
      "2017-11-01 12:27:34,476 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:27:34,862 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.1200160720096037 --l1 6.534705664075285e-06 --ngram 2 --passes 8 --loss_function squared --power_t 0.5731051279139144 --bit_precision 25 \n",
      "2017-11-01 12:27:34,863 INFO     [root/vw-hyperopt:284]: loss value: 0.247990\n",
      "2017-11-01 12:27:34,866 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:33.932600\n",
      "2017-11-01 12:27:34,980 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005485 seconds\n",
      "2017-11-01 12:27:34,982 INFO     [hyperopt.tpe/tpe:842]: TPE using 38/38 trials with best loss 0.222078\n",
      "2017-11-01 12:27:35,006 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.39\n",
      "2017-11-01 12:27:35,007 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.050975060162397744 --l2 6.123278525448562e-05 --ngram 2 --passes 5 --loss_function squared --power_t 0.8150578935979912 --bit_precision 25 \n",
      "2017-11-01 12:28:40,564 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:28:41,153 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.050975060162397744 --l2 6.123278525448562e-05 --ngram 2 --passes 5 --loss_function squared --power_t 0.8150578935979912 --bit_precision 25 \n",
      "2017-11-01 12:28:41,157 INFO     [root/vw-hyperopt:284]: loss value: 0.274793\n",
      "2017-11-01 12:28:41,159 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:06.153274\n",
      "2017-11-01 12:28:41,305 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005894 seconds\n",
      "2017-11-01 12:28:41,306 INFO     [hyperopt.tpe/tpe:842]: TPE using 39/39 trials with best loss 0.222078\n",
      "2017-11-01 12:28:41,322 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.40\n",
      "2017-11-01 12:28:41,323 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.32089277526560295 --l2 9.601179769215378e-08 --l1 0.0003273563853009612 --ngram 2 --passes 9 --loss_function squared --power_t 0.9224598685082692 --bit_precision 25 \n",
      "2017-11-01 12:30:26,040 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:30:26,403 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.32089277526560295 --l2 9.601179769215378e-08 --l1 0.0003273563853009612 --ngram 2 --passes 9 --loss_function squared --power_t 0.9224598685082692 --bit_precision 25 \n",
      "2017-11-01 12:30:26,416 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:30:26,421 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:45.099288\n",
      "2017-11-01 12:30:26,532 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005717 seconds\n",
      "2017-11-01 12:30:26,534 INFO     [hyperopt.tpe/tpe:842]: TPE using 40/40 trials with best loss 0.222078\n",
      "2017-11-01 12:30:26,555 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.41\n",
      "2017-11-01 12:30:26,557 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.08377689303096668 --l2 1.431668213005597e-06 --ngram 2 --passes 10 --loss_function squared --power_t 0.06494657822207217 --bit_precision 25 \n",
      "2017-11-01 12:32:25,407 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 12:32:26,014 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.08377689303096668 --l2 1.431668213005597e-06 --ngram 2 --passes 10 --loss_function squared --power_t 0.06494657822207217 --bit_precision 25 \n",
      "2017-11-01 12:32:26,016 INFO     [root/vw-hyperopt:284]: loss value: 0.340340\n",
      "2017-11-01 12:32:26,019 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:59.464158\n",
      "2017-11-01 12:32:26,163 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.017517 seconds\n",
      "2017-11-01 12:32:26,165 INFO     [hyperopt.tpe/tpe:842]: TPE using 41/41 trials with best loss 0.222078\n",
      "2017-11-01 12:32:26,183 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.42\n",
      "2017-11-01 12:32:26,184 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 2.793963723414218 --l1 8.78554820488186e-07 --ngram 2 --passes 6 --loss_function squared --power_t 0.7931566322593817 --bit_precision 25 \n",
      "2017-11-01 12:33:43,072 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:33:43,537 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 2.793963723414218 --l1 8.78554820488186e-07 --ngram 2 --passes 6 --loss_function squared --power_t 0.7931566322593817 --bit_precision 25 \n",
      "2017-11-01 12:33:43,539 INFO     [root/vw-hyperopt:284]: loss value: 0.233493\n",
      "2017-11-01 12:33:43,542 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:17.358370\n",
      "2017-11-01 12:33:43,656 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005462 seconds\n",
      "2017-11-01 12:33:43,659 INFO     [hyperopt.tpe/tpe:842]: TPE using 42/42 trials with best loss 0.222078\n",
      "2017-11-01 12:33:43,688 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.43\n",
      "2017-11-01 12:33:43,691 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.027853009516549246 --l2 1.3412532089101811e-05 --ngram 2 --passes 3 --loss_function squared --power_t 0.7075317975399508 --bit_precision 25 \n",
      "2017-11-01 12:34:29,497 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:34:30,098 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.027853009516549246 --l2 1.3412532089101811e-05 --ngram 2 --passes 3 --loss_function squared --power_t 0.7075317975399508 --bit_precision 25 \n",
      "2017-11-01 12:34:30,100 INFO     [root/vw-hyperopt:284]: loss value: 0.271441\n",
      "2017-11-01 12:34:30,103 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:46.415332\n",
      "2017-11-01 12:34:30,245 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005618 seconds\n",
      "2017-11-01 12:34:30,248 INFO     [hyperopt.tpe/tpe:842]: TPE using 43/43 trials with best loss 0.222078\n",
      "2017-11-01 12:34:30,267 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.44\n",
      "2017-11-01 12:34:30,269 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.4914050437608877 --l2 1.1016690210267063e-08 --l1 0.0024007633372470608 --ngram 2 --passes 4 --loss_function squared --power_t 0.6375239781752974 --bit_precision 25 \n",
      "2017-11-01 12:35:17,309 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:35:17,670 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.4914050437608877 --l2 1.1016690210267063e-08 --l1 0.0024007633372470608 --ngram 2 --passes 4 --loss_function squared --power_t 0.6375239781752974 --bit_precision 25 \n",
      "2017-11-01 12:35:17,672 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:35:17,673 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:47.406142\n",
      "2017-11-01 12:35:17,792 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005410 seconds\n",
      "2017-11-01 12:35:17,794 INFO     [hyperopt.tpe/tpe:842]: TPE using 44/44 trials with best loss 0.222078\n",
      "2017-11-01 12:35:17,822 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.45\n",
      "2017-11-01 12:35:17,824 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.16679214073198928 --ngram 2 --passes 5 --loss_function squared --power_t 0.4789647122655747 --bit_precision 25 \n",
      "2017-11-01 12:36:21,076 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:36:21,676 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.16679214073198928 --ngram 2 --passes 5 --loss_function squared --power_t 0.4789647122655747 --bit_precision 25 \n",
      "2017-11-01 12:36:21,678 INFO     [root/vw-hyperopt:284]: loss value: 0.312496\n",
      "2017-11-01 12:36:21,679 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:03.857619\n",
      "2017-11-01 12:36:21,847 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005875 seconds\n",
      "2017-11-01 12:36:21,849 INFO     [hyperopt.tpe/tpe:842]: TPE using 45/45 trials with best loss 0.222078\n",
      "2017-11-01 12:36:21,868 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.46\n",
      "2017-11-01 12:36:21,869 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.24550413134457386 --l2 0.0006511371176647887 --l1 2.0924080390725677e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.5406570336830807 --bit_precision 25 \n",
      "2017-11-01 12:37:44,454 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:37:44,833 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.24550413134457386 --l2 0.0006511371176647887 --l1 2.0924080390725677e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.5406570336830807 --bit_precision 25 \n",
      "2017-11-01 12:37:44,835 INFO     [root/vw-hyperopt:284]: loss value: 0.258552\n",
      "2017-11-01 12:37:44,839 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:22.971418\n",
      "2017-11-01 12:37:44,958 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005389 seconds\n",
      "2017-11-01 12:37:44,960 INFO     [hyperopt.tpe/tpe:842]: TPE using 46/46 trials with best loss 0.222078\n",
      "2017-11-01 12:37:44,980 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.47\n",
      "2017-11-01 12:37:44,990 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.047132431603619726 --l2 1.067301130236451e-06 --ngram 2 --passes 9 --loss_function squared --power_t 0.2989797579124236 --bit_precision 25 \n",
      "2017-11-01 12:39:36,279 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:39:36,835 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.047132431603619726 --l2 1.067301130236451e-06 --ngram 2 --passes 9 --loss_function squared --power_t 0.2989797579124236 --bit_precision 25 \n",
      "2017-11-01 12:39:36,838 INFO     [root/vw-hyperopt:284]: loss value: 0.317676\n",
      "2017-11-01 12:39:36,841 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:51.861642\n",
      "2017-11-01 12:39:36,956 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007354 seconds\n",
      "2017-11-01 12:39:36,962 INFO     [hyperopt.tpe/tpe:842]: TPE using 47/47 trials with best loss 0.222078\n",
      "2017-11-01 12:39:36,982 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.48\n",
      "2017-11-01 12:39:36,984 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.4112445117298063 --l2 2.309759855599042e-07 --l1 7.017988036228277e-07 --ngram 2 --passes 1 --loss_function squared --power_t 0.4181499409383732 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 12:39:49,927 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:39:50,342 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.4112445117298063 --l2 2.309759855599042e-07 --l1 7.017988036228277e-07 --ngram 2 --passes 1 --loss_function squared --power_t 0.4181499409383732 --bit_precision 25 \n",
      "2017-11-01 12:39:50,344 INFO     [root/vw-hyperopt:284]: loss value: 0.283118\n",
      "2017-11-01 12:39:50,354 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:13.364614\n",
      "2017-11-01 12:39:50,465 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005207 seconds\n",
      "2017-11-01 12:39:50,467 INFO     [hyperopt.tpe/tpe:842]: TPE using 48/48 trials with best loss 0.222078\n",
      "2017-11-01 12:39:50,495 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.49\n",
      "2017-11-01 12:39:50,497 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.07137288485683516 --ngram 2 --passes 8 --loss_function squared --power_t 0.2005305127522417 --bit_precision 25 \n",
      "2017-11-01 12:41:27,611 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:41:28,197 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.07137288485683516 --ngram 2 --passes 8 --loss_function squared --power_t 0.2005305127522417 --bit_precision 25 \n",
      "2017-11-01 12:41:28,198 INFO     [root/vw-hyperopt:284]: loss value: 0.300666\n",
      "2017-11-01 12:41:28,209 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:37.713309\n",
      "2017-11-01 12:41:28,355 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.018074 seconds\n",
      "2017-11-01 12:41:28,357 INFO     [hyperopt.tpe/tpe:842]: TPE using 49/49 trials with best loss 0.222078\n",
      "2017-11-01 12:41:28,376 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.50\n",
      "2017-11-01 12:41:28,378 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.499833615463356 --l2 0.039744980797669964 --l1 0.02288907311979918 --ngram 2 --passes 10 --loss_function squared --power_t 0.3751392905828332 --bit_precision 25 \n",
      "2017-11-01 12:44:08,247 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:44:08,626 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.499833615463356 --l2 0.039744980797669964 --l1 0.02288907311979918 --ngram 2 --passes 10 --loss_function squared --power_t 0.3751392905828332 --bit_precision 25 \n",
      "2017-11-01 12:44:08,628 INFO     [root/vw-hyperopt:284]: loss value: 0.214948\n",
      "2017-11-01 12:44:08,629 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:40.253037\n",
      "2017-11-01 12:44:08,754 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005426 seconds\n",
      "2017-11-01 12:44:08,756 INFO     [hyperopt.tpe/tpe:842]: TPE using 50/50 trials with best loss 0.214948\n",
      "2017-11-01 12:44:08,773 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.51\n",
      "2017-11-01 12:44:08,774 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 6.183206373160488 --l2 0.04816525275060894 --ngram 2 --passes 7 --loss_function squared --power_t 0.9301503376420177 --bit_precision 25 \n",
      "2017-11-01 12:45:48,524 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:45:49,024 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 6.183206373160488 --l2 0.04816525275060894 --ngram 2 --passes 7 --loss_function squared --power_t 0.9301503376420177 --bit_precision 25 \n",
      "2017-11-01 12:45:49,026 INFO     [root/vw-hyperopt:284]: loss value: 0.294030\n",
      "2017-11-01 12:45:49,044 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:40.270528\n",
      "2017-11-01 12:45:49,190 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005246 seconds\n",
      "2017-11-01 12:45:49,192 INFO     [hyperopt.tpe/tpe:842]: TPE using 51/51 trials with best loss 0.214948\n",
      "2017-11-01 12:45:49,222 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.52\n",
      "2017-11-01 12:45:49,224 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 9.963175829567923 --l2 0.0243881313635684 --l1 0.061923499985624936 --ngram 2 --passes 8 --loss_function squared --power_t 0.5743238249482877 --bit_precision 25 \n",
      "2017-11-01 12:47:39,861 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:47:40,242 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 9.963175829567923 --l2 0.0243881313635684 --l1 0.061923499985624936 --ngram 2 --passes 8 --loss_function squared --power_t 0.5743238249482877 --bit_precision 25 \n",
      "2017-11-01 12:47:40,244 INFO     [root/vw-hyperopt:284]: loss value: 0.232508\n",
      "2017-11-01 12:47:40,246 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:51.023633\n",
      "2017-11-01 12:47:40,356 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005392 seconds\n",
      "2017-11-01 12:47:40,358 INFO     [hyperopt.tpe/tpe:842]: TPE using 52/52 trials with best loss 0.214948\n",
      "2017-11-01 12:47:40,385 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.53\n",
      "2017-11-01 12:47:40,387 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 4.154034970984713 --l2 0.004189132635071206 --ngram 2 --passes 9 --loss_function squared --power_t 0.38913704263164534 --bit_precision 25 \n",
      "2017-11-01 12:49:42,666 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:49:43,266 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 4.154034970984713 --l2 0.004189132635071206 --ngram 2 --passes 9 --loss_function squared --power_t 0.38913704263164534 --bit_precision 25 \n",
      "2017-11-01 12:49:43,268 INFO     [root/vw-hyperopt:284]: loss value: 0.254041\n",
      "2017-11-01 12:49:43,270 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:02.884856\n",
      "2017-11-01 12:49:43,429 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.029139 seconds\n",
      "2017-11-01 12:49:43,431 INFO     [hyperopt.tpe/tpe:842]: TPE using 53/53 trials with best loss 0.214948\n",
      "2017-11-01 12:49:43,467 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.54\n",
      "2017-11-01 12:49:43,481 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.5997295418325226 --l1 0.013189790996822092 --ngram 2 --passes 3 --loss_function squared --power_t 0.12937087063316882 --bit_precision 25 \n",
      "2017-11-01 12:50:18,353 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:50:18,694 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.5997295418325226 --l1 0.013189790996822092 --ngram 2 --passes 3 --loss_function squared --power_t 0.12937087063316882 --bit_precision 25 \n",
      "2017-11-01 12:50:18,697 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:50:18,699 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:35.232343\n",
      "2017-11-01 12:50:18,814 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.009919 seconds\n",
      "2017-11-01 12:50:18,816 INFO     [hyperopt.tpe/tpe:842]: TPE using 54/54 trials with best loss 0.214948\n",
      "2017-11-01 12:50:18,836 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 12:50:18,838 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.9164534036926958 --l2 0.09174592724840162 --ngram 2 --passes 6 --loss_function squared --power_t 0.306961640735294 --bit_precision 25 \n",
      "2017-11-01 12:52:27,379 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:52:27,818 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.9164534036926958 --l2 0.09174592724840162 --ngram 2 --passes 6 --loss_function squared --power_t 0.306961640735294 --bit_precision 25 \n",
      "2017-11-01 12:52:27,820 INFO     [root/vw-hyperopt:284]: loss value: 0.215604\n",
      "2017-11-01 12:52:27,822 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:08.985799\n",
      "2017-11-01 12:52:27,967 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.011492 seconds\n",
      "2017-11-01 12:52:27,974 INFO     [hyperopt.tpe/tpe:842]: TPE using 55/55 trials with best loss 0.214948\n",
      "2017-11-01 12:52:28,006 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.56\n",
      "2017-11-01 12:52:28,010 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 2.0946165295993384 --l2 0.03757387166755166 --l1 0.001194306390240843 --ngram 2 --passes 2 --loss_function squared --power_t 0.244409360411663 --bit_precision 25 \n",
      "2017-11-01 12:53:00,599 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:53:00,987 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 2.0946165295993384 --l2 0.03757387166755166 --l1 0.001194306390240843 --ngram 2 --passes 2 --loss_function squared --power_t 0.244409360411663 --bit_precision 25 \n",
      "2017-11-01 12:53:00,991 INFO     [root/vw-hyperopt:284]: loss value: 0.222837\n",
      "2017-11-01 12:53:00,994 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:32.988193\n",
      "2017-11-01 12:53:01,116 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007434 seconds\n",
      "2017-11-01 12:53:01,120 INFO     [hyperopt.tpe/tpe:842]: TPE using 56/56 trials with best loss 0.214948\n",
      "2017-11-01 12:53:01,140 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.57\n",
      "2017-11-01 12:53:01,143 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.995110491423865 --l2 0.008724528721940059 --ngram 2 --passes 6 --loss_function squared --power_t 0.28732863612295556 --bit_precision 25 \n",
      "2017-11-01 12:54:31,094 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:54:31,645 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.995110491423865 --l2 0.008724528721940059 --ngram 2 --passes 6 --loss_function squared --power_t 0.28732863612295556 --bit_precision 25 \n",
      "2017-11-01 12:54:31,647 INFO     [root/vw-hyperopt:284]: loss value: 0.290623\n",
      "2017-11-01 12:54:31,649 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:30.509398\n",
      "2017-11-01 12:54:31,789 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005433 seconds\n",
      "2017-11-01 12:54:31,792 INFO     [hyperopt.tpe/tpe:842]: TPE using 57/57 trials with best loss 0.214948\n",
      "2017-11-01 12:54:31,812 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.58\n",
      "2017-11-01 12:54:31,814 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.8132736855203166 --l1 0.00531995952403377 --ngram 2 --passes 3 --loss_function squared --power_t 0.10862543681620063 --bit_precision 25 \n",
      "2017-11-01 12:55:06,730 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:55:07,156 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.8132736855203166 --l1 0.00531995952403377 --ngram 2 --passes 3 --loss_function squared --power_t 0.10862543681620063 --bit_precision 25 \n",
      "2017-11-01 12:55:07,169 INFO     [root/vw-hyperopt:284]: loss value: 0.319039\n",
      "2017-11-01 12:55:07,175 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:35.362695\n",
      "2017-11-01 12:55:07,285 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005399 seconds\n",
      "2017-11-01 12:55:07,287 INFO     [hyperopt.tpe/tpe:842]: TPE using 58/58 trials with best loss 0.214948\n",
      "2017-11-01 12:55:07,305 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.59\n",
      "2017-11-01 12:55:07,306 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.4202583136271942 --l2 0.09063591592661523 --ngram 2 --passes 4 --loss_function squared --power_t 0.06072645403690513 --bit_precision 25 \n",
      "2017-11-01 12:56:58,377 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:56:58,789 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.4202583136271942 --l2 0.09063591592661523 --ngram 2 --passes 4 --loss_function squared --power_t 0.06072645403690513 --bit_precision 25 \n",
      "2017-11-01 12:56:58,792 INFO     [root/vw-hyperopt:284]: loss value: 0.250625\n",
      "2017-11-01 12:56:58,793 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:51.488420\n",
      "2017-11-01 12:56:58,903 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005425 seconds\n",
      "2017-11-01 12:56:58,905 INFO     [hyperopt.tpe/tpe:842]: TPE using 59/59 trials with best loss 0.214948\n",
      "2017-11-01 12:56:58,942 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.60\n",
      "2017-11-01 12:56:58,943 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 4.778288247512726 --l2 0.0012427249508755003 --l1 0.03501431309372747 --ngram 2 --passes 2 --loss_function squared --power_t 0.3171168379652677 --bit_precision 25 \n",
      "2017-11-01 12:57:28,345 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 12:57:28,828 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 4.778288247512726 --l2 0.0012427249508755003 --l1 0.03501431309372747 --ngram 2 --passes 2 --loss_function squared --power_t 0.3171168379652677 --bit_precision 25 \n",
      "2017-11-01 12:57:28,830 INFO     [root/vw-hyperopt:284]: loss value: 0.280992\n",
      "2017-11-01 12:57:28,831 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:29.889791\n",
      "2017-11-01 12:57:28,942 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005303 seconds\n",
      "2017-11-01 12:57:28,943 INFO     [hyperopt.tpe/tpe:842]: TPE using 60/60 trials with best loss 0.214948\n",
      "2017-11-01 12:57:28,983 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.61\n",
      "2017-11-01 12:57:28,985 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 9.121483131960389 --l2 0.08097913763403942 --ngram 2 --passes 6 --loss_function squared --power_t 0.1786206734053912 --bit_precision 25 \n",
      "2017-11-01 13:00:50,197 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:00:50,612 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 9.121483131960389 --l2 0.08097913763403942 --ngram 2 --passes 6 --loss_function squared --power_t 0.1786206734053912 --bit_precision 25 \n",
      "2017-11-01 13:00:50,614 INFO     [root/vw-hyperopt:284]: loss value: 0.222794\n",
      "2017-11-01 13:00:50,617 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:03:21.633968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 13:00:50,731 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.009509 seconds\n",
      "2017-11-01 13:00:50,733 INFO     [hyperopt.tpe/tpe:842]: TPE using 61/61 trials with best loss 0.214948\n",
      "2017-11-01 13:00:50,753 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.62\n",
      "2017-11-01 13:00:50,754 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 7.046651625533099 --l2 0.00041436680340823533 --l1 0.00011049375780983791 --ngram 2 --passes 5 --loss_function squared --power_t 0.36800793335420817 --bit_precision 25 \n",
      "2017-11-01 13:01:49,677 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:01:50,106 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 7.046651625533099 --l2 0.00041436680340823533 --l1 0.00011049375780983791 --ngram 2 --passes 5 --loss_function squared --power_t 0.36800793335420817 --bit_precision 25 \n",
      "2017-11-01 13:01:50,107 INFO     [root/vw-hyperopt:284]: loss value: 0.253631\n",
      "2017-11-01 13:01:50,109 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:59.356046\n",
      "2017-11-01 13:01:50,250 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.022399 seconds\n",
      "2017-11-01 13:01:50,252 INFO     [hyperopt.tpe/tpe:842]: TPE using 62/62 trials with best loss 0.214948\n",
      "2017-11-01 13:01:50,270 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.63\n",
      "2017-11-01 13:01:50,271 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 2.391992707764102 --ngram 2 --passes 4 --loss_function squared --power_t 0.24457175893082073 --bit_precision 25 \n",
      "2017-11-01 13:02:42,342 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:02:42,939 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 2.391992707764102 --ngram 2 --passes 4 --loss_function squared --power_t 0.24457175893082073 --bit_precision 25 \n",
      "2017-11-01 13:02:42,940 INFO     [root/vw-hyperopt:284]: loss value: 0.365639\n",
      "2017-11-01 13:02:42,942 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:52.672166\n",
      "2017-11-01 13:02:43,079 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005100 seconds\n",
      "2017-11-01 13:02:43,081 INFO     [hyperopt.tpe/tpe:842]: TPE using 63/63 trials with best loss 0.214948\n",
      "2017-11-01 13:02:43,100 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.64\n",
      "2017-11-01 13:02:43,101 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 3.8762245904078494 --l2 9.03163796379374e-05 --l1 3.5607356563719475e-08 --ngram 2 --passes 7 --loss_function squared --power_t 0.4618258683322539 --bit_precision 25 \n",
      "2017-11-01 13:04:16,880 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:04:17,424 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 3.8762245904078494 --l2 9.03163796379374e-05 --l1 3.5607356563719475e-08 --ngram 2 --passes 7 --loss_function squared --power_t 0.4618258683322539 --bit_precision 25 \n",
      "2017-11-01 13:04:17,426 INFO     [root/vw-hyperopt:284]: loss value: 0.266910\n",
      "2017-11-01 13:04:17,428 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:34.327710\n",
      "2017-11-01 13:04:17,558 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005515 seconds\n",
      "2017-11-01 13:04:17,560 INFO     [hyperopt.tpe/tpe:842]: TPE using 64/64 trials with best loss 0.214948\n",
      "2017-11-01 13:04:17,601 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.65\n",
      "2017-11-01 13:04:17,603 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.9167049358674177 --l2 0.026451689298250435 --ngram 2 --passes 1 --loss_function squared --power_t 0.03962790586951648 --bit_precision 25 \n",
      "2017-11-01 13:04:36,885 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:04:37,350 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.9167049358674177 --l2 0.026451689298250435 --ngram 2 --passes 1 --loss_function squared --power_t 0.03962790586951648 --bit_precision 25 \n",
      "2017-11-01 13:04:37,352 INFO     [root/vw-hyperopt:284]: loss value: 0.345165\n",
      "2017-11-01 13:04:37,353 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:00:19.752240\n",
      "2017-11-01 13:04:37,471 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005460 seconds\n",
      "2017-11-01 13:04:37,473 INFO     [hyperopt.tpe/tpe:842]: TPE using 65/65 trials with best loss 0.214948\n",
      "2017-11-01 13:04:37,491 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.66\n",
      "2017-11-01 13:04:37,492 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.3692875459524928 --l2 0.004648516576045691 --ngram 2 --passes 10 --loss_function squared --power_t 0.5201980343452391 --bit_precision 25 \n",
      "2017-11-01 13:06:46,772 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:06:47,363 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.3692875459524928 --l2 0.004648516576045691 --ngram 2 --passes 10 --loss_function squared --power_t 0.5201980343452391 --bit_precision 25 \n",
      "2017-11-01 13:06:47,367 INFO     [root/vw-hyperopt:284]: loss value: 0.236297\n",
      "2017-11-01 13:06:47,370 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:09.878707\n",
      "2017-11-01 13:06:47,519 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005293 seconds\n",
      "2017-11-01 13:06:47,521 INFO     [hyperopt.tpe/tpe:842]: TPE using 66/66 trials with best loss 0.214948\n",
      "2017-11-01 13:06:47,545 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.67\n",
      "2017-11-01 13:06:47,546 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.6280456486363087 --l2 0.002294725341424364 --ngram 2 --passes 8 --loss_function squared --power_t 0.7486122895292996 --bit_precision 25 \n",
      "2017-11-01 13:08:31,516 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:08:32,141 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.6280456486363087 --l2 0.002294725341424364 --ngram 2 --passes 8 --loss_function squared --power_t 0.7486122895292996 --bit_precision 25 \n",
      "2017-11-01 13:08:32,142 INFO     [root/vw-hyperopt:284]: loss value: 0.229972\n",
      "2017-11-01 13:08:32,146 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:44.601084\n",
      "2017-11-01 13:08:32,281 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.007976 seconds\n",
      "2017-11-01 13:08:32,282 INFO     [hyperopt.tpe/tpe:842]: TPE using 67/67 trials with best loss 0.214948\n",
      "2017-11-01 13:08:32,319 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.68\n",
      "2017-11-01 13:08:32,321 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 1.3597771560322678 --l2 0.01673873646486071 --ngram 2 --passes 10 --loss_function squared --power_t 0.6599346512660507 --bit_precision 25 \n",
      "2017-11-01 13:10:51,941 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:10:52,517 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 1.3597771560322678 --l2 0.01673873646486071 --ngram 2 --passes 10 --loss_function squared --power_t 0.6599346512660507 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 13:10:52,519 INFO     [root/vw-hyperopt:284]: loss value: 0.225654\n",
      "2017-11-01 13:10:52,523 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:20.203779\n",
      "2017-11-01 13:10:52,689 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.012958 seconds\n",
      "2017-11-01 13:10:52,691 INFO     [hyperopt.tpe/tpe:842]: TPE using 68/68 trials with best loss 0.214948\n",
      "2017-11-01 13:10:52,713 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.69\n",
      "2017-11-01 13:10:52,714 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.23433510113427503 --l2 0.008445302169332765 --ngram 2 --passes 9 --loss_function squared --power_t 0.6095679241648939 --bit_precision 25 \n",
      "2017-11-01 13:12:50,729 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:12:51,318 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.23433510113427503 --l2 0.008445302169332765 --ngram 2 --passes 9 --loss_function squared --power_t 0.6095679241648939 --bit_precision 25 \n",
      "2017-11-01 13:12:51,320 INFO     [root/vw-hyperopt:284]: loss value: 0.212817\n",
      "2017-11-01 13:12:51,322 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:58.609374\n",
      "2017-11-01 13:12:51,469 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006371 seconds\n",
      "2017-11-01 13:12:51,471 INFO     [hyperopt.tpe/tpe:842]: TPE using 69/69 trials with best loss 0.212817\n",
      "2017-11-01 13:12:51,492 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.70\n",
      "2017-11-01 13:12:51,493 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.5471590612625978 --l2 0.00902951267248553 --ngram 2 --passes 9 --loss_function squared --power_t 0.6165585370504322 --bit_precision 25 \n",
      "2017-11-01 13:14:52,830 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:14:53,426 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.5471590612625978 --l2 0.00902951267248553 --ngram 2 --passes 9 --loss_function squared --power_t 0.6165585370504322 --bit_precision 25 \n",
      "2017-11-01 13:14:53,428 INFO     [root/vw-hyperopt:284]: loss value: 0.211683\n",
      "2017-11-01 13:14:53,431 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:01.938810\n",
      "2017-11-01 13:14:53,570 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006888 seconds\n",
      "2017-11-01 13:14:53,571 INFO     [hyperopt.tpe/tpe:842]: TPE using 70/70 trials with best loss 0.211683\n",
      "2017-11-01 13:14:53,592 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.71\n",
      "2017-11-01 13:14:53,593 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.5479169386376822 --l2 0.008004513953242812 --ngram 2 --passes 9 --loss_function squared --power_t 0.6070770410870611 --bit_precision 25 \n",
      "2017-11-01 13:16:53,128 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:16:53,732 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.5479169386376822 --l2 0.008004513953242812 --ngram 2 --passes 9 --loss_function squared --power_t 0.6070770410870611 --bit_precision 25 \n",
      "2017-11-01 13:16:53,734 INFO     [root/vw-hyperopt:284]: loss value: 0.213359\n",
      "2017-11-01 13:16:53,738 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:00.145700\n",
      "2017-11-01 13:16:53,869 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005717 seconds\n",
      "2017-11-01 13:16:53,871 INFO     [hyperopt.tpe/tpe:842]: TPE using 71/71 trials with best loss 0.211683\n",
      "2017-11-01 13:16:53,916 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.72\n",
      "2017-11-01 13:16:53,918 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.24645721105111368 --l2 0.006424758211601414 --ngram 2 --passes 9 --loss_function squared --power_t 0.6117966875628454 --bit_precision 25 \n",
      "2017-11-01 13:18:55,352 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:18:55,991 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.24645721105111368 --l2 0.006424758211601414 --ngram 2 --passes 9 --loss_function squared --power_t 0.6117966875628454 --bit_precision 25 \n",
      "2017-11-01 13:18:55,993 INFO     [root/vw-hyperopt:284]: loss value: 0.216863\n",
      "2017-11-01 13:18:55,997 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:02.080163\n",
      "2017-11-01 13:18:56,131 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006804 seconds\n",
      "2017-11-01 13:18:56,134 INFO     [hyperopt.tpe/tpe:842]: TPE using 72/72 trials with best loss 0.211683\n",
      "2017-11-01 13:18:56,157 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.73\n",
      "2017-11-01 13:18:56,159 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.10236068960583573 --l2 0.0016427596127230625 --ngram 2 --passes 8 --loss_function squared --power_t 0.7921350544178739 --bit_precision 25 \n",
      "2017-11-01 13:20:43,739 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:20:44,322 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.10236068960583573 --l2 0.0016427596127230625 --ngram 2 --passes 8 --loss_function squared --power_t 0.7921350544178739 --bit_precision 25 \n",
      "2017-11-01 13:20:44,324 INFO     [root/vw-hyperopt:284]: loss value: 0.243460\n",
      "2017-11-01 13:20:44,326 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:48.168935\n",
      "2017-11-01 13:20:44,481 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.010418 seconds\n",
      "2017-11-01 13:20:44,483 INFO     [hyperopt.tpe/tpe:842]: TPE using 73/73 trials with best loss 0.211683\n",
      "2017-11-01 13:20:44,505 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.74\n",
      "2017-11-01 13:20:44,507 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.518890723844263 --l2 0.0003513741544093691 --ngram 2 --passes 9 --loss_function squared --power_t 0.7113980138704898 --bit_precision 25 \n",
      "2017-11-01 13:22:41,754 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:22:42,322 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.518890723844263 --l2 0.0003513741544093691 --ngram 2 --passes 9 --loss_function squared --power_t 0.7113980138704898 --bit_precision 25 \n",
      "2017-11-01 13:22:42,325 INFO     [root/vw-hyperopt:284]: loss value: 0.261848\n",
      "2017-11-01 13:22:42,328 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:57.822687\n",
      "2017-11-01 13:22:42,461 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.009509 seconds\n",
      "2017-11-01 13:22:42,463 INFO     [hyperopt.tpe/tpe:842]: TPE using 74/74 trials with best loss 0.211683\n",
      "2017-11-01 13:22:42,483 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.75\n",
      "2017-11-01 13:22:42,485 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.14416322084735944 --l2 0.013076657052956528 --ngram 2 --passes 8 --loss_function squared --power_t 0.5673548201328373 --bit_precision 25 \n",
      "2017-11-01 13:24:32,050 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 13:24:32,646 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.14416322084735944 --l2 0.013076657052956528 --ngram 2 --passes 8 --loss_function squared --power_t 0.5673548201328373 --bit_precision 25 \n",
      "2017-11-01 13:24:32,648 INFO     [root/vw-hyperopt:284]: loss value: 0.211586\n",
      "2017-11-01 13:24:32,655 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:50.172443\n",
      "2017-11-01 13:24:32,767 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005558 seconds\n",
      "2017-11-01 13:24:32,769 INFO     [hyperopt.tpe/tpe:842]: TPE using 75/75 trials with best loss 0.211586\n",
      "2017-11-01 13:24:32,794 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.76\n",
      "2017-11-01 13:24:32,796 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.14951904463800159 --l2 0.013844479188561843 --ngram 2 --passes 8 --loss_function squared --power_t 0.6859903067373448 --bit_precision 25 \n",
      "2017-11-01 13:26:20,582 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:26:21,302 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.14951904463800159 --l2 0.013844479188561843 --ngram 2 --passes 8 --loss_function squared --power_t 0.6859903067373448 --bit_precision 25 \n",
      "2017-11-01 13:26:21,304 INFO     [root/vw-hyperopt:284]: loss value: 0.216715\n",
      "2017-11-01 13:26:21,305 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:48.510984\n",
      "2017-11-01 13:26:21,454 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006382 seconds\n",
      "2017-11-01 13:26:21,456 INFO     [hyperopt.tpe/tpe:842]: TPE using 76/76 trials with best loss 0.211586\n",
      "2017-11-01 13:26:21,488 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.77\n",
      "2017-11-01 13:26:21,490 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.10473032822287506 --l2 0.002751275209601043 --ngram 2 --passes 7 --loss_function squared --power_t 0.5645067944431356 --bit_precision 25 \n",
      "2017-11-01 13:27:56,185 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:27:56,870 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.10473032822287506 --l2 0.002751275209601043 --ngram 2 --passes 7 --loss_function squared --power_t 0.5645067944431356 --bit_precision 25 \n",
      "2017-11-01 13:27:56,872 INFO     [root/vw-hyperopt:284]: loss value: 0.240148\n",
      "2017-11-01 13:27:56,874 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:35.385478\n",
      "2017-11-01 13:27:57,031 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.023640 seconds\n",
      "2017-11-01 13:27:57,033 INFO     [hyperopt.tpe/tpe:842]: TPE using 77/77 trials with best loss 0.211586\n",
      "2017-11-01 13:27:57,053 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.78\n",
      "2017-11-01 13:27:57,054 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.04056945998174269 --l2 0.000910893244688704 --ngram 2 --passes 8 --loss_function squared --power_t 0.9936803416333644 --bit_precision 25 \n",
      "2017-11-01 13:29:44,167 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:29:44,839 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.04056945998174269 --l2 0.000910893244688704 --ngram 2 --passes 8 --loss_function squared --power_t 0.9936803416333644 --bit_precision 25 \n",
      "2017-11-01 13:29:44,841 INFO     [root/vw-hyperopt:284]: loss value: 0.248606\n",
      "2017-11-01 13:29:44,848 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:47.795754\n",
      "2017-11-01 13:29:44,968 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.009423 seconds\n",
      "2017-11-01 13:29:44,970 INFO     [hyperopt.tpe/tpe:842]: TPE using 78/78 trials with best loss 0.211586\n",
      "2017-11-01 13:29:44,997 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.79\n",
      "2017-11-01 13:29:44,999 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.20289916299733632 --ngram 2 --passes 10 --loss_function squared --power_t 0.828775277646276 --bit_precision 25 \n",
      "2017-11-01 13:31:40,675 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:31:41,412 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.20289916299733632 --ngram 2 --passes 10 --loss_function squared --power_t 0.828775277646276 --bit_precision 25 \n",
      "2017-11-01 13:31:41,417 INFO     [root/vw-hyperopt:284]: loss value: 0.258635\n",
      "2017-11-01 13:31:41,419 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:56.422498\n",
      "2017-11-01 13:31:41,541 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.008600 seconds\n",
      "2017-11-01 13:31:41,543 INFO     [hyperopt.tpe/tpe:842]: TPE using 79/79 trials with best loss 0.211586\n",
      "2017-11-01 13:31:41,572 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.80\n",
      "2017-11-01 13:31:41,574 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.06265577632676292 --l2 6.153740530712802e-05 --ngram 2 --passes 10 --loss_function squared --power_t 0.6332010342421721 --bit_precision 25 \n",
      "2017-11-01 13:33:55,916 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:33:56,953 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.06265577632676292 --l2 6.153740530712802e-05 --ngram 2 --passes 10 --loss_function squared --power_t 0.6332010342421721 --bit_precision 25 \n",
      "2017-11-01 13:33:56,958 INFO     [root/vw-hyperopt:284]: loss value: 0.280347\n",
      "2017-11-01 13:33:56,964 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:15.392404\n",
      "2017-11-01 13:33:57,122 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006074 seconds\n",
      "2017-11-01 13:33:57,126 INFO     [hyperopt.tpe/tpe:842]: TPE using 80/80 trials with best loss 0.211586\n",
      "2017-11-01 13:33:57,152 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.81\n",
      "2017-11-01 13:33:57,154 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.2583170492257697 --l2 2.1965206082258962e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.4851592259442419 --bit_precision 25 \n",
      "2017-11-01 13:35:32,621 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:35:33,613 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.2583170492257697 --l2 2.1965206082258962e-05 --ngram 2 --passes 7 --loss_function squared --power_t 0.4851592259442419 --bit_precision 25 \n",
      "2017-11-01 13:35:33,634 INFO     [root/vw-hyperopt:284]: loss value: 0.290276\n",
      "2017-11-01 13:35:33,640 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:36.487412\n",
      "2017-11-01 13:35:33,790 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.005548 seconds\n",
      "2017-11-01 13:35:33,792 INFO     [hyperopt.tpe/tpe:842]: TPE using 81/81 trials with best loss 0.211586\n",
      "2017-11-01 13:35:33,855 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.82\n",
      "2017-11-01 13:35:33,862 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.1723072922630768 --l2 0.0001911226306365608 --ngram 2 --passes 9 --loss_function squared --power_t 0.9031101877408576 --bit_precision 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-01 13:37:32,956 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:37:33,620 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.1723072922630768 --l2 0.0001911226306365608 --ngram 2 --passes 9 --loss_function squared --power_t 0.9031101877408576 --bit_precision 25 \n",
      "2017-11-01 13:37:33,624 INFO     [root/vw-hyperopt:284]: loss value: 0.257816\n",
      "2017-11-01 13:37:33,629 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:59.774089\n",
      "2017-11-01 13:37:33,755 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006001 seconds\n",
      "2017-11-01 13:37:33,757 INFO     [hyperopt.tpe/tpe:842]: TPE using 82/82 trials with best loss 0.211586\n",
      "2017-11-01 13:37:33,781 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.83\n",
      "2017-11-01 13:37:33,783 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.08635146405081875 --ngram 2 --passes 7 --loss_function squared --power_t 0.5204033056668929 --bit_precision 25 \n",
      "2017-11-01 13:39:04,069 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:39:04,715 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.08635146405081875 --ngram 2 --passes 7 --loss_function squared --power_t 0.5204033056668929 --bit_precision 25 \n",
      "2017-11-01 13:39:04,717 INFO     [root/vw-hyperopt:284]: loss value: 0.296558\n",
      "2017-11-01 13:39:04,719 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:01:30.938373\n",
      "2017-11-01 13:39:04,833 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.006081 seconds\n",
      "2017-11-01 13:39:04,835 INFO     [hyperopt.tpe/tpe:842]: TPE using 83/83 trials with best loss 0.211586\n",
      "2017-11-01 13:39:04,856 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.84\n",
      "2017-11-01 13:39:04,858 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.3126975508778548 --l2 0.000608657550260241 --ngram 2 --passes 9 --loss_function squared --power_t 0.7740931688849082 --bit_precision 25 \n",
      "2017-11-01 13:41:13,003 INFO     [root/vw-hyperopt:236]: executing the following command (validation): vw -t -d ../../data/arktur_medium//part_valid.vw -i ./current.model -p ./holdout.pred --holdout_off -c\n",
      "2017-11-01 13:41:13,594 INFO     [root/vw-hyperopt:283]: parameter suffix: -l 0.3126975508778548 --l2 0.000608657550260241 --ngram 2 --passes 9 --loss_function squared --power_t 0.7740931688849082 --bit_precision 25 \n",
      "2017-11-01 13:41:13,596 INFO     [root/vw-hyperopt:284]: loss value: 0.261582\n",
      "2017-11-01 13:41:13,597 INFO     [root/vw-hyperopt:301]: evaluation time for this step: 0:02:08.741360\n",
      "2017-11-01 13:41:13,738 INFO     [hyperopt.tpe/tpe:814]: tpe_transform took 0.012733 seconds\n",
      "2017-11-01 13:41:13,740 INFO     [hyperopt.tpe/tpe:842]: TPE using 84/84 trials with best loss 0.211586\n",
      "2017-11-01 13:41:13,767 INFO     [root/vw-hyperopt:293]: \n",
      "\n",
      "Starting trial no.85\n",
      "2017-11-01 13:41:13,769 INFO     [root/vw-hyperopt:231]: executing the following command (training): vw -d ../../data/arktur_medium//part_train.vw -f ./current.model --holdout_off -c  -l 0.13123043055964997 --l2 0.011323371420032766 --ngram 2 --passes 8 --loss_function squared --power_t 0.4351752978519615 --bit_precision 25 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/vowpal_wabbit/utl/vw-hyperopt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/vowpal_wabbit/utl/vw-hyperopt.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m                        searcher=args.searcher, is_regression=args.regression)\n\u001b[1;32m    372\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_y_true_holdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperopt_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vowpal_wabbit/utl/vw-hyperopt.py\u001b[0m in \u001b[0;36mhyperopt_search\u001b[0;34m(self, parallel)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting hypersearch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the best hyperopt parameters: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vowpal_wabbit/utl/vw-hyperopt.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nStarting trial no.%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hyperparam_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_vw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_vw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_metric_vw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vowpal_wabbit/utl/vw-hyperopt.py\u001b[0m in \u001b[0;36mfit_vw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_vw_train_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executing the following command (training): %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_vw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1656\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run /vowpal_wabbit/utl/vw-hyperopt.py \\\n",
    "  --train $PATH_TO_DATA/$OUT_PART_TRAIN \\\n",
    "  --holdout $PATH_TO_DATA/$OUT_PART_VALID \\\n",
    "  --max_evals 200 \\\n",
    "  --outer_loss_function squared \\\n",
    "  --vw_space '--algorithms=sgd --ngram=2 --bit_precision=25 --l2=1e-8..1e-1~LO --l1=1e-8..1e-1~LO -l=0.01..10~L --power_t=0.01..1 --passes=1..10~I --loss_function=squared' --plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-grams for all namespaces.\n",
      "ignoring namespaces beginning with: a \n",
      "only testing\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = \n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!vw -i $PATH_TO_DATA/model.vw -t --invert_hash $PATH_TO_DATA/invert_hash.vw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
